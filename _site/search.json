[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs\n\n\n\n\n\n\n\nintel\n\n\ndgpu\n\n\nPVC1100\n\n\narc\n\n\nPyTorch\n\n\nbigdl-llm\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\n8 min\n\n\n\n\n\n\n\n\nStable Diffusion inference on Intel Arc GPUs\n\n\n\n\n\n\n\nintel\n\n\ndgpu\n\n\ngraphics\n\n\narc\n\n\npytorch\n\n\ntensorflow\n\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nConfigure Intel Arc A370M Xe-HPG discrete GPU on Linux\n\n\n\n\n\n\n\nintel\n\n\ndgpu\n\n\ngraphics\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n15 min\n\n\n\n\n\n\n\n\nFlashcard Rust: What type is this variable again?!\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 22, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nFlashcard Rust: Colon colon angle bracket or turbofish!\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nJan 17, 2021\n\n\n2 min\n\n\n\n\n\n\n\n\nFlashcard Rust: Result Type in Rust and how to handle it\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nJan 14, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nSimple methods to try first:\n\n\n\n\n\n\n\nml\n\n\n\n\nCatastrophic forgetting in Neural Nets\n\n\n\n\n\n\nJul 15, 2020\n\n\n12 min\n\n\n\n\n\n\n\n\nTemporal Difference or TD Learning\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nLearning methods in RL\n\n\n\n\n\n\nJul 14, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nTemporal Difference or TD Learning\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nLearning methods in RL\n\n\n\n\n\n\nJul 9, 2020\n\n\n6 min\n\n\n\n\n\n\n\n\nReinforcement Learning (RL)\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nReinforment Learning for the unintiated\n\n\n\n\n\n\nJul 9, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\nPython multiprocessing\n\n\n\n\n\n\n\ncoding\n\n\n\n\nMultiprocessing using ray.\n\n\n\n\n\n\nMay 23, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\nTiming python functions\n\n\n\n\n\n\n\ncoding\n\n\n\n\nA simple timing decorator for python\n\n\n\n\n\n\nMar 23, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\nPython concurrency lessons\n\n\n\n\n\n\n\ncoding\n\n\n\n\nwhen to use threads and processes.\n\n\n\n\n\n\nMar 22, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nFlashcard Rust: Variables\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 16, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nFlashcard Rust: Cargo\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 9, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\nFlashcard Rust: First steps\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 7, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\nMonkeys, Juggling and some Golang\n\n\n\n\n\n\n\ncoding\n\n\n\n\nA simple intro to concurrency in Go\n\n\n\n\n\n\nMar 31, 2017\n\n\n6 min\n\n\n\n\n\n\n\n\nBorg\n\n\n\n\n\n\n\ncoding\n\n\n\n\nAn interesting design pattern - Borg\n\n\n\n\n\n\nMar 28, 2017\n\n\n2 min\n\n\n\n\n\n\n\n\nMessage Authentication and PRFs\n\n\n\n\n\n\n\ncrypto\n\n\n\n\nPseudo random functions and message authentication.\n\n\n\n\n\n\nFeb 12, 2017\n\n\n4 min\n\n\n\n\n\n\n\n\nAn opinonated checklist on crypto\n\n\n\n\n\n\n\ncrypto\n\n\n\n\nA checklist to have when thinking of implementing crypto\n\n\n\n\n\n\nJan 26, 2017\n\n\n5 min\n\n\n\n\n\n\n\n\nGraduated\n\n\n\n\n\n\n\nlife\n\n\n\n\nyup, that’s done!.\n\n\n\n\n\n\nJan 3, 2016\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-03-16-vars.html",
    "href": "posts/2020-03-16-vars.html",
    "title": "Flashcard Rust: Variables",
    "section": "",
    "text": "In this post, let’s look at how variables are considered in Rust.\n\n\nBy default all variables in Rust are immutable, what that means is, once we assign a value to a variable, we cannot reassign another value. The keyword let is used to create variables and bind values to them in Rust.\nfn main() {\n  let name = \"unrahul\";\n  println!(\"name is {}\", name);\nOutput of the above program is:\nname is unrahul\nThere you go, what we have done is; we have created a string literal name in Rust of type &str. Now, if we try to reassign the same variable name in the same scope as main:\nfn main() {\n    let name = \"unrahul\";    // created a variable `name` and assigned a value to it\n    println!(\"name is {}\", name);\n    name = \"rahul\";    // trying to assign a new value to the variable `name`  \n    println!(\"new name is {}\", name);\n}\nThe compiler will throw an error:\nerror[E0384]: cannot assign twice to immutable variable `name`\n --&gt; src/main.rs:4:5\n  |\n2 |     let name = \"unrahul\";\n  |         ----\n  |         |\n  |         first assignment to `name`\n  |         help: make this binding mutable: `mut name`\n3 |     println!(\"name is {}\", name);\n4 |     name = \"rahul\";\n  |     ^^^^^^^^^^^^^^ cannot assign twice to immutable variable\n\nerror: aborting due to previous error\nFor more information about this error, try `rustc --explain E0384`.\nerror: could not compile `playground`.\n{% include info.html text=“try rustc –explain E0384 to see what the error is all about” %}\nThis is freakin’ cool, now that variables cannot be reassigned, in a program, we do not have to worry if the types are going to change or if the state of a variable will change unintentionally.\n\n\n\nIn some situtations, we want variables to be reassigned like in a loop, or ehmm if we want to change our name to a new one.., for those situations, in Rust we can do:\nfn main() {\n    let mut name = \"unrahul\";    // just change the varible to a `mutable` variable\n    println!(\"name is {}\", name);\n    name = \"rahul\";\n    println!(\"new name is {}\", name);\n}\nAn the output is:\nname is unrahul\nnew name is rahul\n\n\n\nShadowing in simple terms mean, a variable already declared can be redeclared with in a inner block with the same name.\nFor example, we can do this in Rust:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    {\n        let name = \"rahul\";    // here the variable is in inner scope (see the braces?), and this is shadowing the external declartion of `name`\n        println!(\"new name is {}\", name);\n    }\n}\nCompiling and running this, we get:\nname is unrahul\nnew name is rahul\nOkay, I lied a bit, shadows need not be in inner or seperate block, this below is perfectly valid:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    let name = \"rahul\";    // This is also considered as shadowing\n    println!(\"new name is {}\", name);\n}\nAnd the output of this is:\nname is unrahul\nnew name is rahul\n\n\n\nNow that we have seen immutable and mutable variables, and also shadowing, may be primitive types next?.."
  },
  {
    "objectID": "posts/2020-03-16-vars.html#immutable-by-default",
    "href": "posts/2020-03-16-vars.html#immutable-by-default",
    "title": "Flashcard Rust: Variables",
    "section": "",
    "text": "By default all variables in Rust are immutable, what that means is, once we assign a value to a variable, we cannot reassign another value. The keyword let is used to create variables and bind values to them in Rust.\nfn main() {\n  let name = \"unrahul\";\n  println!(\"name is {}\", name);\nOutput of the above program is:\nname is unrahul\nThere you go, what we have done is; we have created a string literal name in Rust of type &str. Now, if we try to reassign the same variable name in the same scope as main:\nfn main() {\n    let name = \"unrahul\";    // created a variable `name` and assigned a value to it\n    println!(\"name is {}\", name);\n    name = \"rahul\";    // trying to assign a new value to the variable `name`  \n    println!(\"new name is {}\", name);\n}\nThe compiler will throw an error:\nerror[E0384]: cannot assign twice to immutable variable `name`\n --&gt; src/main.rs:4:5\n  |\n2 |     let name = \"unrahul\";\n  |         ----\n  |         |\n  |         first assignment to `name`\n  |         help: make this binding mutable: `mut name`\n3 |     println!(\"name is {}\", name);\n4 |     name = \"rahul\";\n  |     ^^^^^^^^^^^^^^ cannot assign twice to immutable variable\n\nerror: aborting due to previous error\nFor more information about this error, try `rustc --explain E0384`.\nerror: could not compile `playground`.\n{% include info.html text=“try rustc –explain E0384 to see what the error is all about” %}\nThis is freakin’ cool, now that variables cannot be reassigned, in a program, we do not have to worry if the types are going to change or if the state of a variable will change unintentionally."
  },
  {
    "objectID": "posts/2020-03-16-vars.html#can-a-variable-ever-be-a-mutable",
    "href": "posts/2020-03-16-vars.html#can-a-variable-ever-be-a-mutable",
    "title": "Flashcard Rust: Variables",
    "section": "",
    "text": "In some situtations, we want variables to be reassigned like in a loop, or ehmm if we want to change our name to a new one.., for those situations, in Rust we can do:\nfn main() {\n    let mut name = \"unrahul\";    // just change the varible to a `mutable` variable\n    println!(\"name is {}\", name);\n    name = \"rahul\";\n    println!(\"new name is {}\", name);\n}\nAn the output is:\nname is unrahul\nnew name is rahul"
  },
  {
    "objectID": "posts/2020-03-16-vars.html#shadowing",
    "href": "posts/2020-03-16-vars.html#shadowing",
    "title": "Flashcard Rust: Variables",
    "section": "",
    "text": "Shadowing in simple terms mean, a variable already declared can be redeclared with in a inner block with the same name.\nFor example, we can do this in Rust:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    {\n        let name = \"rahul\";    // here the variable is in inner scope (see the braces?), and this is shadowing the external declartion of `name`\n        println!(\"new name is {}\", name);\n    }\n}\nCompiling and running this, we get:\nname is unrahul\nnew name is rahul\nOkay, I lied a bit, shadows need not be in inner or seperate block, this below is perfectly valid:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    let name = \"rahul\";    // This is also considered as shadowing\n    println!(\"new name is {}\", name);\n}\nAnd the output of this is:\nname is unrahul\nnew name is rahul"
  },
  {
    "objectID": "posts/2020-03-16-vars.html#the-end",
    "href": "posts/2020-03-16-vars.html#the-end",
    "title": "Flashcard Rust: Variables",
    "section": "",
    "text": "Now that we have seen immutable and mutable variables, and also shadowing, may be primitive types next?.."
  },
  {
    "objectID": "posts/2016-01-03-graduated.html",
    "href": "posts/2016-01-03-graduated.html",
    "title": "Graduated",
    "section": "",
    "text": "Graduated\nFinally, after two years of going to school, late-night working on projects when dinner meant calling Dominos at 2:00 AM, I have graduated with a Masters in Computer Engineering. The final semester had been very kind to me; I had a lot of opportunities to work with some cool folks on different aspects of the cloud and Machine Learning.\nTill a month back I had deadlines and submissions, now I am relatively free, I know I should enjoy this moment but it is really hard to focus when you don’t have anything much to do."
  },
  {
    "objectID": "posts/2020-05-23-ray.html",
    "href": "posts/2020-05-23-ray.html",
    "title": "Python multiprocessing",
    "section": "",
    "text": "The ray library for distributed computing has been around for a while. It was a few years back when I first noticed it. This was when I had started doing some Deep Reinforcement Learning at work and needed to do distributed policy explorations. Since then, the library has expanded it’s role in many ways and is a true distributed framework to scale compute intensive workloads. Today I want to try and use it for a non machine learning workload and see how different it would be from using thee multiprocessing library that comes part of the Python standard library.\nI was doing some webscraping and parsing and needed to parallelize some part of my code, specifically I wanted to run 2 functions parallely."
  },
  {
    "objectID": "posts/2020-05-23-ray.html#the-end",
    "href": "posts/2020-05-23-ray.html#the-end",
    "title": "Python multiprocessing",
    "section": "The end",
    "text": "The end\nFor distributed Machine Learning training I used Horovod exlusively, the one thing with Horovod is that it needs MPI and at times, it’s a pain to debug MPI, I would like to tryout Ray and see if it is easier to use in place of Horovod. My initial thoughts are all positive and it looks really great.\n\n\n\nDickinson-Lorenz Typesetting and Distributing Machines"
  },
  {
    "objectID": "posts/2017-03-28-borg.html",
    "href": "posts/2017-03-28-borg.html",
    "title": "Borg",
    "section": "",
    "text": "So I was reading up on some Python and found this interesting pattern by Alex Martelli. It’s call the Borg pattern, which I think is very cool and a bit philosphical.\nclass Borg(object):\n    _shared_data = {}\n    def __init__(self):\n        self.__dict__ = self._shared_data\n\nclass Singleton(Borg):\n    def __init__(self, v):\n        super(Singleton, self).__init__()\n        self.value = v\n    def __str__(self):\n        return str(self.value)\n\n# lets create some instances\n&gt;&gt; one = Singelton(\"first\")\n&gt;&gt; print(one)\n&gt;&gt; 'first'\n&gt;&gt; two = Singelton(\"second\")\n&gt;&gt; print(two)\n&gt;&gt; 'second'\n&gt;&gt; print(one)\n&gt;&gt; 'second'  # wow! thats cool ryt?!\n\n\nAll Singleton objects have the same share the same state - it is made possbile by the all powerful __dict__ attribute. It is a dictionary that contains all attributes and its values. We first assign __dict__ to refer to the dictionary refered to by _shared_data. This makes sure that every time an object for Singelton is created the __dict__ attribute of the Borg object is referring to the same instance variable _shared_data. Thus, when inheriting the class, the child class’s __dict__ is overwritten by parent class’s __dict__.\n\n\n\nAs a closing note, about the philosphy of Borg, think about why we need a singleton? At least one of the reasons why we need one is to maintain a common state among different objects (some would say, that is the only reason). Borg does that for us, one state for all objects, for we are the Borg, we are all the same:)\nThe Borg pattern is so cool because it is so simple."
  },
  {
    "objectID": "posts/2017-03-28-borg.html#whats-going-on-here",
    "href": "posts/2017-03-28-borg.html#whats-going-on-here",
    "title": "Borg",
    "section": "",
    "text": "All Singleton objects have the same share the same state - it is made possbile by the all powerful __dict__ attribute. It is a dictionary that contains all attributes and its values. We first assign __dict__ to refer to the dictionary refered to by _shared_data. This makes sure that every time an object for Singelton is created the __dict__ attribute of the Borg object is referring to the same instance variable _shared_data. Thus, when inheriting the class, the child class’s __dict__ is overwritten by parent class’s __dict__."
  },
  {
    "objectID": "posts/2017-03-28-borg.html#the-end",
    "href": "posts/2017-03-28-borg.html#the-end",
    "title": "Borg",
    "section": "",
    "text": "As a closing note, about the philosphy of Borg, think about why we need a singleton? At least one of the reasons why we need one is to maintain a common state among different objects (some would say, that is the only reason). Borg does that for us, one state for all objects, for we are the Borg, we are all the same:)\nThe Borg pattern is so cool because it is so simple."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html",
    "href": "posts/2020-07-14-learning-methods.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Model free learning can be done using variations of temporal difference learning or Monte Carlo methods."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#temporal-difference-or-td-learning",
    "href": "posts/2020-07-14-learning-methods.html#temporal-difference-or-td-learning",
    "title": "Technical ramblings",
    "section": "Temporal Difference or TD Learning",
    "text": "Temporal Difference or TD Learning\nFrom each step learn something that would make enable us to improve the estimated value for the next step. Consider this, three scenarios, in which the third scenario depends on the second and or the first. If that is the case, then knowing the states in the scenario can help us in better predicting the states in scenario three. We can improve the prediction in the third scenario if there is any change in states for either of the other scenarios, rather than waiting for the third one to finish and then realizing our prediction was close or way off. Consider, you are going somewhere and you expect to go through 2 cities. You estimate that you would reach the destination in 3 hours, as you know or estimate that you will need 1 hour each to cover the 2 other cities. Now, if it’s your lucky day and traffic is low in the first city, thus you could pass through it in 30 minutes instead of 60. Thus, you can estimate that you will reach the final destination 30 minutes early. While passing through the second city, you have car trouble and it takes an hour to fix it and start again. Thus, you now predict you would reach 30 minutes past the estimated time at your destination (Provided, you don’t face any further uncertainties). This continuous improvement of estimate is the main principle behind temporal difference or TD learning. We can say, TD learning is an on-line learning (as we don’t need to wait for the entire episode to finish before updating our estimates). It bootstraps on the estimated value of other states to estimate value of the state in concern.\nTypes : TD(0) and TD(lambda)"
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#q-learning",
    "href": "posts/2020-07-14-learning-methods.html#q-learning",
    "title": "Technical ramblings",
    "section": "Q learning",
    "text": "Q learning\nIt is a variation of TD(0) learning, where we incrementally estimate the Q value for a state based on immediate rewards and the Q value for the next state. The variation is that, to estimate the Q value for the next state, we add the immediate reward with the Q value for the next state that maximizes the value (Q value for the state for the action that gives the maximum value). Also, unlike TD(0) learning, Q learning is an off-policy learning algorithm. Thus the estimated Q value at instance k is, the Q value at k for the state and action at time t plus the difference between estimated Q value using the immediate reward and discounted Q value for the next state for the action that gives the maximum value and the Q value of the current state. This delta between estimated Q value for the next state and Q value for the current state is weighted by a factor called the learning rate, which is between [0, 1]. The weighting factor or learning rate alpha can be decreased based on each iteration or as in many scenarios, use a small fixed value. The rate basically determines by how much we update the Q value. Now, if that sounds complex, trust me, it’s not, I am just not that good at explaining I guess. Just search for the algorithm on line and you will get it instantly.\nHow is it done?\nWell, basically, the agent at time, t, in state, s, does an action, a, and moves to the next state and thus receives a reward, r. Now at time, t+1, the agent knows that it is in state, s, and knows the reward it obtained from the previous state. It uses this information along with the Q value for the optimum action for this state to get a better estimate of the Q value of the state, s. For this update to work, we need to have some starting point of Q values, in practice this is assumed to be zero or set randomly and at each iteration, k, the Q value is updated a little bit based on the learning rate to be a little closer to reality. In time, after many iterations of learning, the Q values for each states will reflect the real values that can be obtained. It has been proved (don’t ask me how, I haven’t checked out the proof) that if we do this iterative update an infinite time, we will eventually get the right Q values, irrespective on the initial Q values, the actions we took in each state etc."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#sarsa-learning",
    "href": "posts/2020-07-14-learning-methods.html#sarsa-learning",
    "title": "Technical ramblings",
    "section": "SARSA learning",
    "text": "SARSA learning\nFirst of all, what a creative naming, it must have taken them a long time to come up with this name. So, why is it called SARSA?, It’s because, the learning algorithm uses the present state, S, the action taken, A, the reward obtained, R, the next state, S, and the action taken, A, in this next state, while following a policy. SARSA stands for State-Action-Reward-State-Action!. While in traditional Q value function, the objective was to estimate the optimum policy doing exploration of a random policy, in SARSA, we start with a policy and tries to estimate the Q value of starting at a state, doing an action in state and following a policy, that is not changed in the course of learning. Thus it is an on-policy learning algorithm, as we don’t change the policy that has been chosen for a particular iteration. The idea is that if we are able to try all the states and all the possible actions infinitely many times, then this will eventually converge to the optimum policy itself. For a particular iteration, it computationally less demanding that Q learning, but overall, it may need more time to converge. This learning is used when the state transition probabilities might not be fixed, there can be changes in the probabilities of switching from one state to the other. The only change in the algorithm compared to Q learning is, it doesn’t take the action that maximizes the utility when calculating the next state value, but obeys by the policy and takes the stipulated action. Here, too if you don’t understand it, please check the algorithm once on line and it will be clear."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#actor-critic-algorithms",
    "href": "posts/2020-07-14-learning-methods.html#actor-critic-algorithms",
    "title": "Technical ramblings",
    "section": "Actor Critic Algorithms",
    "text": "Actor Critic Algorithms\nThis class of learning algorithms has two parts, the Actor, which is a policy function and the Critic, which is a value function that is used to obtain the value for a state transition following an action. After an action has been selected, the Critic estimates the value for the state and calculates value for the state (immediate reward plus value for the next state). This is used to evaluate the action taken, which is the difference between the calculated value (using Bellman equation) and the estimated value. This difference or delta is used to improve the probability for the action in that state. The improvement factor delta is weighted by a learning rate, beta. A variation of this algorithm called the A3C algorithm is one of the fastest RL algorithms out there."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#the-end",
    "href": "posts/2020-07-14-learning-methods.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html",
    "href": "posts/2020-03-23-tiemy.html",
    "title": "Timing python functions",
    "section": "",
    "text": "I have been writing some performant code in Python (oxymoron?), and I wanted a simple decorator to time funtions essentially to measure running time.\nThere a plenty of modules in Python that can help with this, but most were complicated for me to understand what exactly they were doing and had way more functionaltiy than I required.\nSo I wrote a tiny package that can help with this. The package is named tiemy and is available in pypi, it only has one single decorator, timer and I would like to keep it to a minimum.\nYou just have to use the decorator @timer on any Python callable and tiemy runs the function for 3 times and displays the mean and standard deviation of the run time.\nAt the core tiemy uses perf_counter from time module to measure running time.\n\n\nThe module is fairly easy to install, you just need python and pip:\npip install tiemy\nThat’s it!\n\n\n\nConsider a prime number sieve as below is saved in a file prime.py\nfrom tiemy import timer\n\n@timer\ndef sundaram(N):\n    \"\"\"sundaram's sieve to find non primes.\"\"\"\n    primes = list(range(3, N, 2))\n    half = N // 2\n    init = 4\n    for step in range(3, N, 2):\n        for i in range(init, half, step):\n            primes[i-1] = 0\n        init += 2 * (step + 1)\n        if init &gt; half:\n            return [2] + list(filter(0, numbers))\n\n\nif __name__ == \"__main__\":\n    sundaram(100000)\nThe output would be something like:\npython prime.py\ntiming func:: sundaram\nmean:     0.0090, std:     0.0003 sec.\nThe above method to find prime numbers is called sundaram sieve 1, we will look at it more in detail in a later post :).\n\n\n\nTiemy, pronunced as tie-mee, is a simple timing decorator that just does one thing, measure running time.\n\n\n\nJup used to spend time"
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html#installing-tiemy",
    "href": "posts/2020-03-23-tiemy.html#installing-tiemy",
    "title": "Timing python functions",
    "section": "",
    "text": "The module is fairly easy to install, you just need python and pip:\npip install tiemy\nThat’s it!"
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html#how-to-use-tiemy",
    "href": "posts/2020-03-23-tiemy.html#how-to-use-tiemy",
    "title": "Timing python functions",
    "section": "",
    "text": "Consider a prime number sieve as below is saved in a file prime.py\nfrom tiemy import timer\n\n@timer\ndef sundaram(N):\n    \"\"\"sundaram's sieve to find non primes.\"\"\"\n    primes = list(range(3, N, 2))\n    half = N // 2\n    init = 4\n    for step in range(3, N, 2):\n        for i in range(init, half, step):\n            primes[i-1] = 0\n        init += 2 * (step + 1)\n        if init &gt; half:\n            return [2] + list(filter(0, numbers))\n\n\nif __name__ == \"__main__\":\n    sundaram(100000)\nThe output would be something like:\npython prime.py\ntiming func:: sundaram\nmean:     0.0090, std:     0.0003 sec.\nThe above method to find prime numbers is called sundaram sieve 1, we will look at it more in detail in a later post :)."
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html#the-end",
    "href": "posts/2020-03-23-tiemy.html#the-end",
    "title": "Timing python functions",
    "section": "",
    "text": "Tiemy, pronunced as tie-mee, is a simple timing decorator that just does one thing, measure running time.\n\n\n\nJup used to spend time"
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html#footnotes",
    "href": "posts/2020-03-23-tiemy.html#footnotes",
    "title": "Timing python functions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nsieve of sundaram↩︎"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html",
    "href": "posts/2017-01-26-crypto-checklist.html",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Please dont roll your own crypto!!\nOhkay, so you are not heeding the warning ha, let’s see how we can minimize the damage then..\nThis is sort of a checklist of things to verify in crypto implementations on first glance. Most of these are guidelines from Owasp 1 and a few from other sources. Whenever I study a crypto implementaton these are things I first check some of these guidelines. They are obvious, even mundane and some may be different depending upon the circumstances.\nFor example, guidelines for hash lengths, key lengths etc. will depend on how and where the crypto is implemented and cannot be considered as a hard and fast rules.\n\n\n\nApplication level - Closest to source, most desired, ensure usage of established algorithms suchs a openSSL, libreSSL, and use approved APIs, may need more memory/processing bandwidth.\nProtocol level - The next level where the protocol ensures encryption, eg: HTTPS using SSL, when mutual authentication is required, care should be taken as there are two session keys for each side (client –&gt; server and server –&gt; client)\nNetwork level - The next level, protects both protocol and application level with no need for any level of encryption at that level. Typicall example is IPSec VPN, can be used to create a secure tunnel to connect to a network etc.\n\n\n\n\n\nSymmetric keys should be at least 256 bit in length, possibly 512 for critical applications\nHash length - 128 is okay for most applications, 256 is the way to go if a method authentication is being used\nAsymmetric keys 1024 bit is okay for normal applications, 1536 or 2048 for critical applications, for the paraniod 4036\n\n\n\n\n\nCheck who all have access to the keys\nCheck if the keys can be exported/imported without a password\nCheck if keys are stored in code - this can be bad in two ways, one the key can be exploited by anyone having access to source code. Second, if key rotation is needed or if keys become pulbic it would be difficult to change the key.\n\n\n\n\n\nMake sure the access is the least level of required access and not further\nMake sure keys are not exportable (marked as non exportable) when signing requests are generated.\nOnce imported to keystore, destroy the file (certificate) that was imported, stored in the file system\nLog any change to keys\nUse secure passphrases for keys stored\nStop storing keys in code files/binaries\nGive minimum access to applications that use keys to do something\nAlways ask for a passphrase when user interaction is available\n\n\n\n\n\nAssess the levels of trust we can have/we need to have on data transmitted\nAlways encrypt sensitive data before transmission\nEncrypt as close to source as possible\nAll paths should be covered with same level of encryption\nIdentify if any temporary files/garbage collected data is written when encrypting/decrypting if so see who all has access to it\nPick an adequate level of encryption - Algorithm and key strength according to accessment done of levels of trust\n\n\n\n\n\nEnsure session tokens are truly random eg use /dev/urandom to create UUIDs etc\nNever use static UUID where security is needed\nDo not use sequential values to maintain session as this can be easily spoofed\nCheck from where keys are generated for session maintainance (Are they coming from random source?)\n\n\n\n\n\nMessage authentication using md5 - Md5 mac when used to authenticate message using a shared secret is vulnerable to length extension attacks.\n\nThese attack works as follows:\nConsider there is a shared secret a user and server is having\nUser authenticates data sent using the scheme:\n\n\"DATA\"+md5(shared_secret+\"DATA\")\nThis was a common form of MAC a few years back.\nIt has been found that it is trivial to create a valid MAC (Message Authentication Code) based on md5 by simply extending the hash. For example,\n\"DATA\"+\"CRAFTED_DATA\"+md5(shared_secret+\"DATA\"+\"CRAFTED_DATA\")\nWill be accepted by the server as successfully authenticated in some cases, as we can see, the attacker do not need to know the shared_secret but merely can extend the hash. 2\n{% include alert.html text=“Thus do not use md5 as a mac” %}\n\nUsing equality operator with HMAC for message Authentication - Vulnerable to timing attacks, this is because it takes slightly longer to match correct characters, than for wrong characters when equality operator is used to compare.\n\nAn attacker can simply bruteforce the hmac with something like:\nConsider the hmac is bxyz5\n\nThe attacker can then send a string aaaaa to zzzzz first, when the average response time is considered the attack with\nstring baaaa would take a slightly longer time that a---- , thus the attacker now possess the first character of the mac,\nthis can be repeated to get the entire hmac, thus this simple mistake of using equality operator for comparison can lead \nto a vulnerability if the attacker is motivated enough.\nTo prevent timing attacks do not use equality operator but XOR each byte as:\na XOR a = 0\nSum up for all the bytes and if the result is 0 then, the hmac is identical and we have verified the authenticity of the message.\nIn python use something like:\nhmac.hexcompare(a, b)\nTo reduce the chance of timing attacks, this function can only be used if both a and b are ASCII coded.\n\n\n\nHere are some more refereneces to get you started, good luck!\n\nBad crypto practices\nCryptographic right answers\nCrypto coding rules\nMatasano crypto challenges\nConfidentiality is not authentication"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#types",
    "href": "posts/2017-01-26-crypto-checklist.html#types",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Application level - Closest to source, most desired, ensure usage of established algorithms suchs a openSSL, libreSSL, and use approved APIs, may need more memory/processing bandwidth.\nProtocol level - The next level where the protocol ensures encryption, eg: HTTPS using SSL, when mutual authentication is required, care should be taken as there are two session keys for each side (client –&gt; server and server –&gt; client)\nNetwork level - The next level, protects both protocol and application level with no need for any level of encryption at that level. Typicall example is IPSec VPN, can be used to create a secure tunnel to connect to a network etc."
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#keys",
    "href": "posts/2017-01-26-crypto-checklist.html#keys",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Symmetric keys should be at least 256 bit in length, possibly 512 for critical applications\nHash length - 128 is okay for most applications, 256 is the way to go if a method authentication is being used\nAsymmetric keys 1024 bit is okay for normal applications, 1536 or 2048 for critical applications, for the paraniod 4036"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#key-storage",
    "href": "posts/2017-01-26-crypto-checklist.html#key-storage",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Check who all have access to the keys\nCheck if the keys can be exported/imported without a password\nCheck if keys are stored in code - this can be bad in two ways, one the key can be exploited by anyone having access to source code. Second, if key rotation is needed or if keys become pulbic it would be difficult to change the key."
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#how-to-protect-one-self",
    "href": "posts/2017-01-26-crypto-checklist.html#how-to-protect-one-self",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Make sure the access is the least level of required access and not further\nMake sure keys are not exportable (marked as non exportable) when signing requests are generated.\nOnce imported to keystore, destroy the file (certificate) that was imported, stored in the file system\nLog any change to keys\nUse secure passphrases for keys stored\nStop storing keys in code files/binaries\nGive minimum access to applications that use keys to do something\nAlways ask for a passphrase when user interaction is available"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#transmission-of-data",
    "href": "posts/2017-01-26-crypto-checklist.html#transmission-of-data",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Assess the levels of trust we can have/we need to have on data transmitted\nAlways encrypt sensitive data before transmission\nEncrypt as close to source as possible\nAll paths should be covered with same level of encryption\nIdentify if any temporary files/garbage collected data is written when encrypting/decrypting if so see who all has access to it\nPick an adequate level of encryption - Algorithm and key strength according to accessment done of levels of trust"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#session-tokens",
    "href": "posts/2017-01-26-crypto-checklist.html#session-tokens",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Ensure session tokens are truly random eg use /dev/urandom to create UUIDs etc\nNever use static UUID where security is needed\nDo not use sequential values to maintain session as this can be easily spoofed\nCheck from where keys are generated for session maintainance (Are they coming from random source?)"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#dont-do-this",
    "href": "posts/2017-01-26-crypto-checklist.html#dont-do-this",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Message authentication using md5 - Md5 mac when used to authenticate message using a shared secret is vulnerable to length extension attacks.\n\nThese attack works as follows:\nConsider there is a shared secret a user and server is having\nUser authenticates data sent using the scheme:\n\n\"DATA\"+md5(shared_secret+\"DATA\")\nThis was a common form of MAC a few years back.\nIt has been found that it is trivial to create a valid MAC (Message Authentication Code) based on md5 by simply extending the hash. For example,\n\"DATA\"+\"CRAFTED_DATA\"+md5(shared_secret+\"DATA\"+\"CRAFTED_DATA\")\nWill be accepted by the server as successfully authenticated in some cases, as we can see, the attacker do not need to know the shared_secret but merely can extend the hash. 2\n{% include alert.html text=“Thus do not use md5 as a mac” %}\n\nUsing equality operator with HMAC for message Authentication - Vulnerable to timing attacks, this is because it takes slightly longer to match correct characters, than for wrong characters when equality operator is used to compare.\n\nAn attacker can simply bruteforce the hmac with something like:\nConsider the hmac is bxyz5\n\nThe attacker can then send a string aaaaa to zzzzz first, when the average response time is considered the attack with\nstring baaaa would take a slightly longer time that a---- , thus the attacker now possess the first character of the mac,\nthis can be repeated to get the entire hmac, thus this simple mistake of using equality operator for comparison can lead \nto a vulnerability if the attacker is motivated enough.\nTo prevent timing attacks do not use equality operator but XOR each byte as:\na XOR a = 0\nSum up for all the bytes and if the result is 0 then, the hmac is identical and we have verified the authenticity of the message.\nIn python use something like:\nhmac.hexcompare(a, b)\nTo reduce the chance of timing attacks, this function can only be used if both a and b are ASCII coded."
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#the-end",
    "href": "posts/2017-01-26-crypto-checklist.html#the-end",
    "title": "An opinonated checklist on crypto",
    "section": "",
    "text": "Here are some more refereneces to get you started, good luck!\n\nBad crypto practices\nCryptographic right answers\nCrypto coding rules\nMatasano crypto challenges\nConfidentiality is not authentication"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html#footnotes",
    "href": "posts/2017-01-26-crypto-checklist.html#footnotes",
    "title": "An opinonated checklist on crypto",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://bit.ly/2W0dZum↩︎\nhttps://bit.ly/2W0dZum↩︎"
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "",
    "text": "The landscape of AI and natural language processing has dramatically shifted with the advent of Large Language models (LLMs). This shift is characterized by advancements like Low-Rank Adaptation (LoRA) and its more advanced iteration, Quantized LoRA (QLoRA), which have transformed the fine-tuning process from a compute-intensive task into an efficient, scalable procedure."
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#the-advent-of-lora-a-paradigm-shift-in-llm-fine-tuning",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#the-advent-of-lora-a-paradigm-shift-in-llm-fine-tuning",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "The Advent of LoRA: A Paradigm Shift in LLM Fine-Tuning",
    "text": "The Advent of LoRA: A Paradigm Shift in LLM Fine-Tuning\nLoRA represents a significant advancement in the fine-tuning of LLMs. By introducing trainable adapter modules between the layers of a large pre-trained model, LoRA focuses on refining a smaller subset of model parameters. These adapters are low-rank matrices, significantly reducing the computational burden and preserving the valuable pre-trained knowledge embedded within LLMs. The key aspects of LoRA include:\n\nLow-Rank matrix structure: Shaped as (r x d), where ‘r’ is a small rank hyperparameter and ‘d’ is the hidden dimension size. This structure ensures fewer trainable parameters.\nFactorization: The adapter matrix is factorized into two smaller matrices, enhancing the model’s function adaptability with fewer parameters.\nScalability and adaptability: LoRA balances the model’s learning capacity and generalizability by scaling adapters with a parameter α and incorporating dropout for regularization.\n\n\n\n\nLeft: Integration of LoRA adapters into the model. Right: Deployment of LoRA adapters with a foundation model as a task-specific model library"
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#quantized-lora-qlora-efficient-fine-tuning-on-intel-hardware",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#quantized-lora-qlora-efficient-fine-tuning-on-intel-hardware",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "Quantized LoRA (QLoRA): Efficient Fine-Tuning on Intel Hardware",
    "text": "Quantized LoRA (QLoRA): Efficient Fine-Tuning on Intel Hardware\nQLoRA advances LoRA by introducing weight quantization, further reducing memory usage. This approach enables the fine-tuning of large models, such as the 70B-parameter Llama 2, on a single GPU , like Intel® Data Center GPU Max Series 1100 with 48 GB VRAM, which was considered impossible previously. QLoRA’s main features include:\n\nMemory efficiency: Through weight quantization, QLoRA substantially reduces the model’s memory footprint, crucial for handling large LLMs.\nOn-the-fly dequantization: It temporarily dequantizes the quantized weights for computations, focusing only on adapter gradients during training.\n\n\nFine-Tuning with QLoRA on Intel Hardware\nThe fine-tuning process starts with setting up the environment and installing the necessary packages, including bigdl-llm for model loading, parameter-efficient fine-tuning (PEFT) for LoRA adapters, Intel® Extension for PyTorch* for training using Intel discrete GPUs, Hugging face Transformers for fine-tuning, and datasets for loading the dataset. We will walk through the high-level process of fine-tuning an LLM to improve its capabilities. As an example, we will generate SQL queries from natural language input, focusing on general QLoRA fine-tuning. For detailed explanations, check out the full notebook that takes you from setting up the required Python* packages, loading the model, fine-tuning, and inferencing the fine-tuned LLM to generate SQL from text, on Intel® Developer Cloud and also here.\n\n\nModel Loading and Configuration for Fine-Tuning\nThe foundation model is loaded in a 4-bit format using bigdl-llm, significantly reducing memory usage. This step enables fine-tuning large models like Llama 2 70 for example,\nfrom bigdl.llm.transformers import AutoModelForCausalLM \n\n# Loading the model in a 4-bit format for efficient memory usage \nmodel = AutoModelForCausalLM.from_pretrained( \n    \"model_id\",  # Replace with your model ID \n    load_in_low_bit=\"nf4\", \n    optimize_model=False, \n    torch_dtype=torch.float16, \n    modules_to_not_convert=[\"lm_head\"], \n) \n\n\nLearning Rate and Stability in Training\nSelecting an optimal learning rate is critical in QLoRA fine-tuning to balance training stability and convergence speed. This decision is vital for effective fine-tuning outcomes as a higher learning rate can lead to instabilities with the training loss abnormally drop zero after few steps.\nfrom transformers import TrainingArguments \n\n# Configuration for training \ntraining_args = TrainingArguments( \n    learning_rate=2e-5,  # Optimal starting point; adjust as needed \n    per_device_train_batch_size=4, \n    max_steps=200, \n    # Additional parameters... \n) \nDuring the fine-tuning process, there is a notable rapid decrease in the loss after just a few steps, which then gradually levels off, reaching a value near 0.6 at approximately 300 steps as seen in the graph below:\n\n\n\nTraining and Validation loss during fine-tuning a CodeLlama-7b model on the b-mc2/sql-create-context dataset."
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#text-to-sql-conversion-prompt-engineering",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#text-to-sql-conversion-prompt-engineering",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "Text-to-SQL Conversion: Prompt Engineering",
    "text": "Text-to-SQL Conversion: Prompt Engineering\nWith the fine-tuned model, we can convert natural language queries into SQL commands, a vital capability in data analytics and business intelligence. To fine-tune the model, we must carefully convert the data into a structured prompt like below to form an instruction dataset with Input, Context and Response fields:\n# Function to generate structured prompts for Text-to-SQL tasks \ndef generate_prompt_sql(input_question, context, output=\"\"):\n    return f\"\"\"You are a powerful text-to-SQL model. Your job is to answer questions about a database. You are given a question and context regarding one or more tables. \nYou must output the SQL query that answers the question.\n### Input:\n{input_question}\n### Context:\n{context}\n### Response:\n{output}\"\"\""
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#diverse-model-options",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#diverse-model-options",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "Diverse Model Options",
    "text": "Diverse Model Options\nThe notebook supports an array of models, each offering unique capabilities for different fine-tuning objectives:\n\nNousResearch/Nous-Hermes-Llama-2–7b\nNousResearch/Llama-2–7b-chat-hf\nNousResearch/Llama-2–13b-hf\nNousResearch/CodeLlama-7b-hf\nPhind/Phind-CodeLlama-34B-v2\nopenlm-research/open_llama_3b_v2\nopenlm-research/open_llama_13b\nHuggingFaceH4/zephyr-7b-beta"
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#model-inference-with-qlora-a-comparative-approach",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#model-inference-with-qlora-a-comparative-approach",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "Model Inference with QLoRA: A Comparative Approach",
    "text": "Model Inference with QLoRA: A Comparative Approach\nThe true test of any fine-tuning process lies in its inference capabilities. In the case of the implementation, the inference stage not only demonstrates the model’s proficiency in task-specific applications but also allows for a comparative analysis between the base and the fine-tuned models. This comparison sheds light on the effectiveness of the LoRA adapters in enhancing the model’s performance for specific tasks.\n\nModel Loading for Inference\nFor inference, the model is loaded in a low-bit format, typically 4-bit, using the bigdl-llm library. This approach drastically reduces the memory footprint, making it suitable to run multiple LLMs with high parameter count on a single resource-optimized device such as the Intel® Data Center Max GPU 1100. The following code snippet illustrates the model loading process for inference:\nfrom bigdl.llm.transformers import AutoModelForCausalLM \n\n# Loading the model for inference \nmodel_for_inference = AutoModelForCausalLM.from_pretrained( \n    \"finetuned_model_path\",  # Path to the fine-tuned model\n    load_in_4bit=True,  # 4 bit loading\n    optimize_model=True,\n    use_cache=True,\n    torch_dtype=torch.float16, \n    modules_to_not_convert=[\"lm_head\"], \n)\n\n\nRunning Inference: Comparing Base vs Fine-Tuned Model\nOnce the model is loaded, we can perform inference to generate SQL queries from natural language inputs. This process can be conducted on both the base model and the fine-tuned model, allowing you to directly compare the outcomes and assess the improvements brought about by fine-tuning with QLoRA:\n# Generating a SQL query from a text prompt \ntext_prompt = generate_sql_prompt(…)\n# Base Model Inference \nbase_model_sql = base_model.generate(text_prompt) \nprint(\"Base Model SQL:\", base_model_sql) \n# Fine-Tuned Model Inference \nfinetuned_model_sql = finetuned_model.generate(text_prompt) \nprint(\"Fine-Tuned Model SQL:\", finetuned_model_sql)\nFollowing a 15-minute session training itself, the fine-tuned model demonstrates enhanced proficiency in generating SQL queries that reflect the given questions more accurately than the base model. With additional training steps, we can anticipate further improvements in the model’s response accuracy:\nFinetuned Model:\n\n\n\nFine-tuned model SQL generation for a given question and context.\n\n\nBase Model:\n\n\n\nBase model SQL generation for a given question and context.\n\n\n\n\nLoRA Adapters: A Library of Task-Specific Enhancements\nOne of the most compelling aspects of LoRA is its ability to act as a library of task-specific enhancements. These adapters can be fine-tuned for distinct tasks and then saved. Depending on the requirement, a specific adapter can be loaded and used with the base model, effectively switching the model’s capabilities to suit different tasks. This adaptability makes LoRA a highly versatile tool in the realm of LLM fine-tuning.\n\n\nCheck out the notebook on Intel Developer Cloud\nI invite AI practitioners and developers to explore the full notebook on the Intel Developer Cloud, where you can experiment with and explore the capabilities of fine-tuning LLMs using QLoRA on Intel hardware with Intel AI software optimizations. Once you log into Intel Developer Cloud, go to the “Training Catalog”. Under “Gen AI Essentials” in the catalog, you can find the LLM fine-tuning notebook and other notebooks.\n\n\n\nLLM Fine-tuning and other notebooks in Gen AI Essentials under the Training Catalog on IDC."
  },
  {
    "objectID": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#repo",
    "href": "posts/2023-12-05-qlora-finetuning-intel-gpu.html#repo",
    "title": "Text-to-SQL Generation Using LLMs Fine-Tuned with QLoRA on Intel GPUs",
    "section": "Repo",
    "text": "Repo\nYou can find the full code and other related notebooks here."
  },
  {
    "objectID": "posts/2020-03-07-ahoy-rust.html",
    "href": "posts/2020-03-07-ahoy-rust.html",
    "title": "Flashcard Rust: First steps",
    "section": "",
    "text": "Rust is one of the coolest systems programming lanugage on the block, let’s dip our feet in it.\n\n\nHopefully, you are using Linux. To install Rust, use:\n&gt; curl https://sh.rustup.rs -sSf | sh\nRustup is a tool that is used to install Rust, the above script downloads and installs rustup first and then installs Rust.\n\n\n\nLet’s create a directory to keep our source files:\n&gt; mkdir ~/rust_sources\n&gt; cd ~/rust_sources\nOur first program, to say ahoy!\n&gt; mkdir ahoy; cd ahoy\n&gt; touch main.rs\nOpen main.rs and add the following line:\nfn main() {\n    println!(\"ahoy ahoy!\");  \n}\nSave the file and run rustc main.rs, thats it! We have compiled our first rust program.\nThere should be a file named main in the same directory main.rs is in, execute it using:\n./main\nahoy ahoy!\nNow that you have written 2 lines of rust, do read\n\n\n\n\n\n\nhttps://imgur.com/LZ6VxZR.png"
  },
  {
    "objectID": "posts/2020-03-07-ahoy-rust.html#setup",
    "href": "posts/2020-03-07-ahoy-rust.html#setup",
    "title": "Flashcard Rust: First steps",
    "section": "",
    "text": "Hopefully, you are using Linux. To install Rust, use:\n&gt; curl https://sh.rustup.rs -sSf | sh\nRustup is a tool that is used to install Rust, the above script downloads and installs rustup first and then installs Rust."
  },
  {
    "objectID": "posts/2020-03-07-ahoy-rust.html#ahoy",
    "href": "posts/2020-03-07-ahoy-rust.html#ahoy",
    "title": "Flashcard Rust: First steps",
    "section": "",
    "text": "Let’s create a directory to keep our source files:\n&gt; mkdir ~/rust_sources\n&gt; cd ~/rust_sources\nOur first program, to say ahoy!\n&gt; mkdir ahoy; cd ahoy\n&gt; touch main.rs\nOpen main.rs and add the following line:\nfn main() {\n    println!(\"ahoy ahoy!\");  \n}\nSave the file and run rustc main.rs, thats it! We have compiled our first rust program.\nThere should be a file named main in the same directory main.rs is in, execute it using:\n./main\nahoy ahoy!\nNow that you have written 2 lines of rust, do read"
  },
  {
    "objectID": "posts/2020-03-07-ahoy-rust.html#the-end",
    "href": "posts/2020-03-07-ahoy-rust.html#the-end",
    "title": "Flashcard Rust: First steps",
    "section": "",
    "text": "https://imgur.com/LZ6VxZR.png"
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html",
    "href": "posts/2017-02-12-prf-and-macs.html",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Bob gets a message from Alice, as Bob doesn’t think a girl like Alice would talk to him, he wants Alice to authenticate or convince him that it is indeed she who is talking.\n\n\nWell one way to do it is, if she has some secret already shared with Bob, and use it to seed a pseudo random number generator (PRG) that they built at a recent rasberry pi conference, both of them attended and use the number to validate it is indeed Alice.\nShe could seed as the input of the PRG, run it for 10 times chaining the seed each time and get the 10th random number. She can casually write down this number on a note and put it in Bob’s bag.\nBob finds the note, supposedly from Alice, in his bag. He follows the instructions (as any boy would), on how to generate the 10th random number from the shared secret and compares it with the random number that was written at the end of the note.\nNow, assuming Bob didn’t screw up generating the random number, he knows for a fact that it is indeed Alice who left the note as only she knew the ‘secret’ that was used to seed the random number. Bob is happy..\nThis is cool and all, but they haven’t really talked till now. If they have to talk with authentication, then both of them will have to remember how many rounds the PRG ran and keep a tab on this as long as they want to talk, thus they have to maintain a ‘state’, that’s not fun!!.\n\n\n\nAlice intiates,\n10th iteration of the random number generator:\nRandom Number - R10\nAlice - A\nBob - B\n\nA --(R10)--&gt; B\nB --(generates R10 and matches it with Alice)\nNext time, when Bob responds with a encoded secret, both of them have to iterate the random number generator for the 11th time.\nB --(R11)--&gt; A\nA --(generates R11 and matches it with Bob's R11)\nAs you can see both Alice and Bob have to remember how many times they have ran the PRG (maintain a state) and that is not fun.\n\n\n\nWell, one thing they can do is to use a ‘special’ family of pseudo random functions (PRF) called the HMAC-HASH function.\nWhoa!! what the heck is a PRF? you ask.\nA PRF or a Pseudo Random Function 1 is a family of functions that if given, there is no method that can be used to distinguish it from another function from the same family, in the same domain. By this I mean, there is no sane way before hell freezes over that you can get any information if you were given outputs of two functions from the same family to tell which one is which.\nSo coming back to HMAC-HASH. HMAC or Hash based Message Authentication Code is a type of PRF, which you can use to authenticate yourself to a person, if both of you have a shared-secret. The HASH here can be MD5 or SHA1 or SHA256 or something like that.\n\n\n\nFor now, there are no known attacks against, HMAC-MD5 2 or HMAC-SHA1, but that doesn’t mean there will never be, so please don’t use it okay? please…\nOkay, all I want to know how Bob can talk to Alice and authenticate that the message is indeed from Bob, how will he do that?\n\n\n\nWell, Bob could use HMAC-SHA2 3 authentication function, hash the message he is sending to Alice using the shared-secret. She would be able to verify if the message is indeed from Bob by hashing it with HMAC-SHA2 using the shared-secret as the key and comparing the hashes, both should be same.\n\n\n\nHope this helps Bob."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#how-will-she-do-it",
    "href": "posts/2017-02-12-prf-and-macs.html#how-will-she-do-it",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Well one way to do it is, if she has some secret already shared with Bob, and use it to seed a pseudo random number generator (PRG) that they built at a recent rasberry pi conference, both of them attended and use the number to validate it is indeed Alice.\nShe could seed as the input of the PRG, run it for 10 times chaining the seed each time and get the 10th random number. She can casually write down this number on a note and put it in Bob’s bag.\nBob finds the note, supposedly from Alice, in his bag. He follows the instructions (as any boy would), on how to generate the 10th random number from the shared secret and compares it with the random number that was written at the end of the note.\nNow, assuming Bob didn’t screw up generating the random number, he knows for a fact that it is indeed Alice who left the note as only she knew the ‘secret’ that was used to seed the random number. Bob is happy..\nThis is cool and all, but they haven’t really talked till now. If they have to talk with authentication, then both of them will have to remember how many rounds the PRG ran and keep a tab on this as long as they want to talk, thus they have to maintain a ‘state’, that’s not fun!!."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#lets-see-an-example",
    "href": "posts/2017-02-12-prf-and-macs.html#lets-see-an-example",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Alice intiates,\n10th iteration of the random number generator:\nRandom Number - R10\nAlice - A\nBob - B\n\nA --(R10)--&gt; B\nB --(generates R10 and matches it with Alice)\nNext time, when Bob responds with a encoded secret, both of them have to iterate the random number generator for the 11th time.\nB --(R11)--&gt; A\nA --(generates R11 and matches it with Bob's R11)\nAs you can see both Alice and Bob have to remember how many times they have ran the PRG (maintain a state) and that is not fun."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#what-can-they-do-to-avoid-maintaining-the-state",
    "href": "posts/2017-02-12-prf-and-macs.html#what-can-they-do-to-avoid-maintaining-the-state",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Well, one thing they can do is to use a ‘special’ family of pseudo random functions (PRF) called the HMAC-HASH function.\nWhoa!! what the heck is a PRF? you ask.\nA PRF or a Pseudo Random Function 1 is a family of functions that if given, there is no method that can be used to distinguish it from another function from the same family, in the same domain. By this I mean, there is no sane way before hell freezes over that you can get any information if you were given outputs of two functions from the same family to tell which one is which.\nSo coming back to HMAC-HASH. HMAC or Hash based Message Authentication Code is a type of PRF, which you can use to authenticate yourself to a person, if both of you have a shared-secret. The HASH here can be MD5 or SHA1 or SHA256 or something like that."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#is-this-secure",
    "href": "posts/2017-02-12-prf-and-macs.html#is-this-secure",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "For now, there are no known attacks against, HMAC-MD5 2 or HMAC-SHA1, but that doesn’t mean there will never be, so please don’t use it okay? please…\nOkay, all I want to know how Bob can talk to Alice and authenticate that the message is indeed from Bob, how will he do that?"
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#hmm-ahh-impatience",
    "href": "posts/2017-02-12-prf-and-macs.html#hmm-ahh-impatience",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Well, Bob could use HMAC-SHA2 3 authentication function, hash the message he is sending to Alice using the shared-secret. She would be able to verify if the message is indeed from Bob by hashing it with HMAC-SHA2 using the shared-secret as the key and comparing the hashes, both should be same."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#the-end",
    "href": "posts/2017-02-12-prf-and-macs.html#the-end",
    "title": "Message Authentication and PRFs",
    "section": "",
    "text": "Hope this helps Bob."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html#footnotes",
    "href": "posts/2017-02-12-prf-and-macs.html#footnotes",
    "title": "Message Authentication and PRFs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttp://crypto.stackexchange.com/questions/31343/what-is-the-purpose-of-pseudorandom-function-families-prfs↩︎\nhttp://crypto.stackexchange.com/questions/9336/is-hmac-md5-considered-secure-for-authenticating-encrypted-data↩︎\nhttp://security.stackexchange.com/questions/33123/hotp-with-as-hmac-hashing-algoritme-a-hash-from-the-sha-2-family↩︎"
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html",
    "href": "posts/2020-03-22-pyjuggle.html",
    "title": "Python concurrency lessons",
    "section": "",
    "text": "Now that everyone is talking about and using async/await and event loops everywhere, I thought I would go to the basics (well not to the fundamentals, but practical basics) and write a small program to download a bunch of text files and process them.\n\n\nFetch n ebooks from project gutenberg site and do some elemental processing on the content.\nSome of the code snippets:\n\ndef download(url: str) -&gt; str:\n    \"\"\"download and decode content from a url.\"\"\"\n    page = requests.get(url)\n    name = url.split(\"/\")[-1]\n    return page.content.decode(\"utf-8\"), f\"texts/{name}\"\n\ndef save(content: str, fname: str = None):\n    \"\"\"save content to file with name.\"\"\"\n    with open(fname, \"w\") as fh:\n        fh.write(content)\n\ndef process(string: list) -&gt; list:\n    \"\"\"cpu bound tasks\"\"\"\n    ...\n    ...\n    return string\n\ndef main():\n    urls = [\n        \"https://www.gutenberg.org/cache/epub/376/pg376.txt\",\n        \"https://www.gutenberg.org/files/84/84-0.txt\",\n        \"https://www.gutenberg.org/cache/epub/844/pg844.txt\",\n    ]\n\n    # call download\n\n    # call save\n\n    #call string process\nThe full source code is on github at pyjuggle.\n\n\n\nLet’s get this straight, concurrency and parallelism are not the same. Concurrency means, running multiple programs as though they are running at the same time, where as parallelism is running the programs truly in parallel in multiple cores of your machine.\nThe interesting aspect of Python (Ruby and few others) is that there is something called the global interpreter lock or GIL. This is part of the cpython implementation, where only one Python object can have access to the interpreter at one time. This means that, we cannot run two threads of execution parallely in Python but that all threads of execution are time sliced. You can read more on GIL here.\nThreads come handy though when there is a lot of IO involved or when there are subprocess calls, where the interpreter is handing-off to work to another subsystem, and a new thread of execution can start, while the previous thread waits for a response.\nWhen we need to real multi-core parallel tasks in Python, mulitprocessing library comes to our favour. It essentially spins up multiple Python interpreters and works on orthogal exlusive chunks of data, this method is good for CPU bound tasks.\n\n\n\nAlthough, there are exceptions, I would say when IO or subprocess calls are involved or when the interpreter is giving of control to another sub-system use threads.\nThe module of choice for me for concurrency in python is concurrent.futures.\nTo use a thread pool, we can do this:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n\n    # call download method concurrently\n    with tpe(max_workers=3) as exe:\n        results = exe.map(download, urls)\nHere, maximum of 4 threads will be spun up, each time sliced, and will start downloading the urls. One thing to note here is that, depending upon the amount of data, here the urls threaded code may not show significant performance improvement when compared to one without threading, so it is always good to time sequential and threaded code before you decide.\n{% include info.html text=“use threads when the task is IO bound” %}\nThe same approach can be used to save the files, create a threadpool and save the downloaded content:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n    ...\n    ...\n    # call save method concurrently\n    with tpe(max_workers=3) as exe:\n        exe.map(save, results)\n    ...\n    ...\nFinally we come across, a CPU bound task, which has to tokenize the saved files, here if we use threading, although there could be some speedup than serial execution, it is advicable to use multiprocessing, in some cases, threading could actually slow down the program if the program is mostly CPU bound.\nAn example of using a multiprocess executor is:\nfrom concurrent.futures import ProcessPoolExecutor as ppe\n\n    # call process method parallely\n    with ppe(max_workers=4) as exe:\n        exe.map(process, open(\"./all.txt\").readlines())\n{% include info.html text=“use processes when the task is CPU bound” %}\nHere we are mapping 4 process methods with chunks of data from the file, one thing to note here is that the amount of work distributed to the 4 processes is not controlled by the programmer and may not be equal.\n\n\n\nThis was an interesting experience, I wanted to write about this for few years now, finally got to it. I am pretty sure I have got some things wrong, if you find any such techincal inacurracies ping me on twitter or on github.\nMay be next time I will write, when to use async-await."
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html#the-program",
    "href": "posts/2020-03-22-pyjuggle.html#the-program",
    "title": "Python concurrency lessons",
    "section": "",
    "text": "Fetch n ebooks from project gutenberg site and do some elemental processing on the content.\nSome of the code snippets:\n\ndef download(url: str) -&gt; str:\n    \"\"\"download and decode content from a url.\"\"\"\n    page = requests.get(url)\n    name = url.split(\"/\")[-1]\n    return page.content.decode(\"utf-8\"), f\"texts/{name}\"\n\ndef save(content: str, fname: str = None):\n    \"\"\"save content to file with name.\"\"\"\n    with open(fname, \"w\") as fh:\n        fh.write(content)\n\ndef process(string: list) -&gt; list:\n    \"\"\"cpu bound tasks\"\"\"\n    ...\n    ...\n    return string\n\ndef main():\n    urls = [\n        \"https://www.gutenberg.org/cache/epub/376/pg376.txt\",\n        \"https://www.gutenberg.org/files/84/84-0.txt\",\n        \"https://www.gutenberg.org/cache/epub/844/pg844.txt\",\n    ]\n\n    # call download\n\n    # call save\n\n    #call string process\nThe full source code is on github at pyjuggle."
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html#python-and-concurrency",
    "href": "posts/2020-03-22-pyjuggle.html#python-and-concurrency",
    "title": "Python concurrency lessons",
    "section": "",
    "text": "Let’s get this straight, concurrency and parallelism are not the same. Concurrency means, running multiple programs as though they are running at the same time, where as parallelism is running the programs truly in parallel in multiple cores of your machine.\nThe interesting aspect of Python (Ruby and few others) is that there is something called the global interpreter lock or GIL. This is part of the cpython implementation, where only one Python object can have access to the interpreter at one time. This means that, we cannot run two threads of execution parallely in Python but that all threads of execution are time sliced. You can read more on GIL here.\nThreads come handy though when there is a lot of IO involved or when there are subprocess calls, where the interpreter is handing-off to work to another subsystem, and a new thread of execution can start, while the previous thread waits for a response.\nWhen we need to real multi-core parallel tasks in Python, mulitprocessing library comes to our favour. It essentially spins up multiple Python interpreters and works on orthogal exlusive chunks of data, this method is good for CPU bound tasks."
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html#when-to-use-what",
    "href": "posts/2020-03-22-pyjuggle.html#when-to-use-what",
    "title": "Python concurrency lessons",
    "section": "",
    "text": "Although, there are exceptions, I would say when IO or subprocess calls are involved or when the interpreter is giving of control to another sub-system use threads.\nThe module of choice for me for concurrency in python is concurrent.futures.\nTo use a thread pool, we can do this:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n\n    # call download method concurrently\n    with tpe(max_workers=3) as exe:\n        results = exe.map(download, urls)\nHere, maximum of 4 threads will be spun up, each time sliced, and will start downloading the urls. One thing to note here is that, depending upon the amount of data, here the urls threaded code may not show significant performance improvement when compared to one without threading, so it is always good to time sequential and threaded code before you decide.\n{% include info.html text=“use threads when the task is IO bound” %}\nThe same approach can be used to save the files, create a threadpool and save the downloaded content:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n    ...\n    ...\n    # call save method concurrently\n    with tpe(max_workers=3) as exe:\n        exe.map(save, results)\n    ...\n    ...\nFinally we come across, a CPU bound task, which has to tokenize the saved files, here if we use threading, although there could be some speedup than serial execution, it is advicable to use multiprocessing, in some cases, threading could actually slow down the program if the program is mostly CPU bound.\nAn example of using a multiprocess executor is:\nfrom concurrent.futures import ProcessPoolExecutor as ppe\n\n    # call process method parallely\n    with ppe(max_workers=4) as exe:\n        exe.map(process, open(\"./all.txt\").readlines())\n{% include info.html text=“use processes when the task is CPU bound” %}\nHere we are mapping 4 process methods with chunks of data from the file, one thing to note here is that the amount of work distributed to the 4 processes is not controlled by the programmer and may not be equal."
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html#the-end",
    "href": "posts/2020-03-22-pyjuggle.html#the-end",
    "title": "Python concurrency lessons",
    "section": "",
    "text": "This was an interesting experience, I wanted to write about this for few years now, finally got to it. I am pretty sure I have got some things wrong, if you find any such techincal inacurracies ping me on twitter or on github.\nMay be next time I will write, when to use async-await."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html",
    "href": "posts/2020-03-09-cargo.html",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "Cargo is the one ring to rule them all, the one build tool that:\n\nbuilds our rust project\ndownloads dependencies\nbuild dependencies\nhelp release optimized code\n\nLet’s see how cargo helps us in packaging and releasing our code, get it? cargo.. packaging… ah ohkay, not the right time for terrible jokes I guess..\n\n\nCargo should be installed as part of rust installation, to ensure cargo is installed run\n&gt; cargo --version\ncargo 1.41.0\nyou should be able to see something like above… to create a wireframe for a new project hworld run:\n&gt; cargo new hworld\nThis creates a directory hworld, with few files and one directory:\n&gt;\n|-- Cargo.toml\n|-- src\n    |-- main.rs\nCargo goes one step ahead and adds versioning to the project, git is used by default, so if you do a ls -la you can see few more files, but for now let’s ignore that and focus on the main files.\n\nCargo.toml - This is called the manifest file of our Rust project and is the file where we configure stuff like name, version of the project, Rust edition, the, author's, email address, and also there is a section to include the dependencies required by the project.\n\nA sample Cargo.toml looks like:\n[package]\nname = \"hworld\"\nversion = \"0.1.0\"\nauthors = [\"rahulunair &lt;rahulunair@gmail.com&gt;\"]\nedition = \"2018\"\n\n[dependencies]\ntime = \"0.1.12\"\nThere are a lot more options that can be included in the manifest file, for an extensive list, see. Here I have specified one dependency time with version 0.1.12, we wont use it for anything now, but just for kicks eh?\n\n./src/main.rs - It is a placeholder to add our code, cargo has populated it with:\n\nfn main() {\n    println!(\"Hello, world!\");\n}\n\n\n\nIt’s just as simple as running the below command from inside hworld directory:\n&gt; cargo build\nIf you are running it for the first time, you would see something like:\nUpdating crates.io index\n  Downloaded time v0.1.42\n  Downloaded libc v0.2.67\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 49.31s\nThat’s it, we have built our first program using cargo.\n\n\n\nAfter cargo has built the project successfully, it creates a directory ./target/debug/ to save the binary, we can just cd to ./target/debug and run:\n&gt; ./hworld\nHello, world!\nYay! we have run our first Rust program, that was created using Cargo.\n\n\n\nRelease version ideally should be an optimized binary, to obtain such a binary, it is as simple as running:\n&gt; cargo build --release\nRunning the above command on my system from hworld directory, Cargo prints:\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished release [optimized] target(s) in 8.42s\nAnd, lo and behold there is an optimized binary hworld in hworld/target/release/, in later posts, we can see the difference between debug and release versions of binaries generated by Cargo.\nThe eligance of Cargo is that, everything is named as we would expect, no gotcha’s .. I really like that about Cargo.\n\n\n\nAs a closing note, let’s recap Cargo mainly does two things for us,\n\nbuilds our project using - cargo build or cargo build –release\nbuilds and runs our project using - cargo run\n\nNow, there is a third and final command I use, to check for compilation issues fast, without really building the binary, which can be slow at times.\nThe command to check if everything is alright is:\n&gt; cargo check\nOn my system it gives me this:\n    Checking libc v0.2.67\n    Checking time v0.1.42\n    Checking hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 3.61s\nThat’s it, so we saw what cargo can do, it is a tool that can be used when we are building a Rust project, but if it is a simple file, with no dependencies we can still use rustc as we saw earlier in Flashcard Rust."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html#using-cargo-to-create-rust-projects",
    "href": "posts/2020-03-09-cargo.html#using-cargo-to-create-rust-projects",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "Cargo should be installed as part of rust installation, to ensure cargo is installed run\n&gt; cargo --version\ncargo 1.41.0\nyou should be able to see something like above… to create a wireframe for a new project hworld run:\n&gt; cargo new hworld\nThis creates a directory hworld, with few files and one directory:\n&gt;\n|-- Cargo.toml\n|-- src\n    |-- main.rs\nCargo goes one step ahead and adds versioning to the project, git is used by default, so if you do a ls -la you can see few more files, but for now let’s ignore that and focus on the main files.\n\nCargo.toml - This is called the manifest file of our Rust project and is the file where we configure stuff like name, version of the project, Rust edition, the, author's, email address, and also there is a section to include the dependencies required by the project.\n\nA sample Cargo.toml looks like:\n[package]\nname = \"hworld\"\nversion = \"0.1.0\"\nauthors = [\"rahulunair &lt;rahulunair@gmail.com&gt;\"]\nedition = \"2018\"\n\n[dependencies]\ntime = \"0.1.12\"\nThere are a lot more options that can be included in the manifest file, for an extensive list, see. Here I have specified one dependency time with version 0.1.12, we wont use it for anything now, but just for kicks eh?\n\n./src/main.rs - It is a placeholder to add our code, cargo has populated it with:\n\nfn main() {\n    println!(\"Hello, world!\");\n}"
  },
  {
    "objectID": "posts/2020-03-09-cargo.html#building-the-project-with-cargo",
    "href": "posts/2020-03-09-cargo.html#building-the-project-with-cargo",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "It’s just as simple as running the below command from inside hworld directory:\n&gt; cargo build\nIf you are running it for the first time, you would see something like:\nUpdating crates.io index\n  Downloaded time v0.1.42\n  Downloaded libc v0.2.67\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 49.31s\nThat’s it, we have built our first program using cargo."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html#running-the-program",
    "href": "posts/2020-03-09-cargo.html#running-the-program",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "After cargo has built the project successfully, it creates a directory ./target/debug/ to save the binary, we can just cd to ./target/debug and run:\n&gt; ./hworld\nHello, world!\nYay! we have run our first Rust program, that was created using Cargo."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html#releasing-our-code",
    "href": "posts/2020-03-09-cargo.html#releasing-our-code",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "Release version ideally should be an optimized binary, to obtain such a binary, it is as simple as running:\n&gt; cargo build --release\nRunning the above command on my system from hworld directory, Cargo prints:\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished release [optimized] target(s) in 8.42s\nAnd, lo and behold there is an optimized binary hworld in hworld/target/release/, in later posts, we can see the difference between debug and release versions of binaries generated by Cargo.\nThe eligance of Cargo is that, everything is named as we would expect, no gotcha’s .. I really like that about Cargo."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html#the-end",
    "href": "posts/2020-03-09-cargo.html#the-end",
    "title": "Flashcard Rust: Cargo",
    "section": "",
    "text": "As a closing note, let’s recap Cargo mainly does two things for us,\n\nbuilds our project using - cargo build or cargo build –release\nbuilds and runs our project using - cargo run\n\nNow, there is a third and final command I use, to check for compilation issues fast, without really building the binary, which can be slow at times.\nThe command to check if everything is alright is:\n&gt; cargo check\nOn my system it gives me this:\n    Checking libc v0.2.67\n    Checking time v0.1.42\n    Checking hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 3.61s\nThat’s it, so we saw what cargo can do, it is a tool that can be used when we are building a Rust project, but if it is a simple file, with no dependencies we can still use rustc as we saw earlier in Flashcard Rust."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html",
    "href": "posts/2022-08-12-arc-dgpu-linux.html",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "",
    "text": "These instructions are for Ubuntu 22.04 LTS-based Oses. I am using Pop!_OS 22.04 LTS derivative of Ubuntu 22.04."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#configure-linux-kernel",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#configure-linux-kernel",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Configure Linux kernel",
    "text": "Configure Linux kernel\n\nInstall the latest available Linux kernel. For example, I have installed kernel 6.0\n\nInstall a recent version of the kernel. The easiest way to install a different kernel is by using Mainline.\nCoding → add-apt-repository ppa:cappelikan/ppa\nCoding → apt update\nCoding → apt install mainline\nAs seen in the screenshot below, I am using Kernel 6.0.0:\n\n\n\nMainline kernel installer\n\n\nAfter installing the kernel, please restart the machine.\n\nAs Arc dGPUs support are still experimental in the kernel, you will have to force the dGPU to be detected. This can be done using the kernel force_probe parameter i915.force_probe=&lt;device_id&gt; for the Intel i915 HD graphics driver. This will force probe the driver for new Intel graphics devices that are recognized by the kernel but not adequately supported. Hopefully, you wouldn’t have to do that with a newer version of the kernel. I am using a Yoga 7i (16” Intel) with Intel Arc Graphics which has the Intel® Arc™ A370M discrete Graphics card (dgpu) along with an integrated Intel® UHD Graphics. The device id for this dgpu is 5693, and you can find the device id of the card either by looking at i915 logs using sudo dmesg | grep -i i915 or from Intel’s gpu hardware table.\n\nPop!_OS 22.04 LTS uses systemd to manage kernel boot params, to force i915 driver to enable the dgpu use kernelstub tool:\nCoding → kernelstub -a \"i915.force_probe=5693\"\nAfter this restart the machine and check i915 logs using dmesg to see if the graphics card has been detected:\nCoding → dmesg | grep -i i915\nYou should see an output like this:\nCoding →  dmesg | grep -i i915\n[    0.000000] Command line: initrd=\\EFI\\Pop_OS-97fe6a26-7d8a-4120-89db-8f2130b644b7\\initrd.img root=UUID=97fe6a26-7d8a-4120-89db-8f2130b644b7 ro quiet loglevel=0 systemd.show_status=false splash i915.force_probe=5693\n[    0.047217] Kernel command line: initrd=\\EFI\\Pop_OS-97fe6a26-7d8a-4120-89db-8f2130b644b7\\initrd.img root=UUID=97fe6a26-7d8a-4120-89db-8f2130b644b7 ro quiet loglevel=0 systemd.show_status=false splash i915.force_probe=5693\n[    1.775328] i915 0000:00:02.0: [drm] VT-d active for gfx access\n[    1.775383] i915 0000:00:02.0: vgaarb: deactivate vga console\n[    1.775416] i915 0000:00:02.0: [drm] Using Transparent Hugepages\n[    1.775979] i915 0000:00:02.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=io+mem\n[    1.777234] i915 0000:00:02.0: [drm] Finished loading DMC firmware i915/adlp_dmc_ver2_16.bin (v2.16)\n[    1.912397] i915 0000:00:02.0: [drm] GuC firmware i915/adlp_guc_70.1.1.bin version 70.1\n[    1.912399] i915 0000:00:02.0: [drm] HuC firmware i915/tgl_huc_7.9.3.bin version 7.9\n[    1.926153] i915 0000:00:02.0: [drm] HuC authenticated\n[    1.926448] i915 0000:00:02.0: [drm] GuC submission enabled\n[    1.926449] i915 0000:00:02.0: [drm] GuC SLPC enabled\n[    1.927353] i915 0000:00:02.0: [drm] GuC RC: enabled\n[    1.930736] i915 0000:00:02.0: [drm] Protected Xe Path (PXP) protected content support initialized\n[    3.769893] [drm] Initialized i915 1.6.0 20201103 for 0000:00:02.0 on minor 0\n[    3.775461] i915 0000:03:00.0: enabling device (0000 -&gt; 0002)\n[    3.775491] i915 0000:03:00.0: [drm] Incompatible option enable_guc=3 - HuC is not supported!\n[    3.776346] i915 0000:03:00.0: [drm] VT-d active for gfx access\n[    3.776472] i915 0000:03:00.0: [drm] Local memory IO size: 0x00000003fa000000\n[    3.776476] i915 0000:03:00.0: [drm] Local memory available: 0x00000003fa000000\n[    3.787361] fbcon: i915drmfb (fb0) is primary device\n[    3.787366] i915 0000:00:02.0: [drm] fb0: i915drmfb frame buffer device\n[    3.796583] i915 0000:03:00.0: [drm] Finished loading DMC firmware i915/dg2_dmc_ver2_06.bin (v2.6)\n[    5.326692] i915 0000:03:00.0: [drm] failed to retrieve link info, disabling eDP\n[    5.428126] i915 0000:03:00.0: [drm] GuC firmware i915/dg2_guc_70.1.2.bin version 70.1\n[    5.442889] i915 0000:03:00.0: [drm] GuC submission enabled\n[    5.442892] i915 0000:03:00.0: [drm] GuC SLPC enabled\n[    5.443378] i915 0000:03:00.0: [drm] GuC RC: enabled\nThe dg2_guc is loaded as seen in the above line: [ 5.428126] i915 0000:03:00.0: [drm] GuC firmware i915/dg2_guc_70.1.2.bin version 70.1. Both the igpu and dgpu is detected and also the firmware is loaded. If drm verification fails and GuC is not loaded, install the latest linux firmware from this link:\nYou can install the latest firmware files by cloning the repo and moving it to /lib/firmware (warning: this is a brute-force approach):\nCoding →  git clone https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/\nCoding →  cd linux-firmware && yes | cp -r * /lib/firmware\nFor me, the firmware that came with Pop!_OS 22.04 LTS worked, i didn’t have to install the latest ones."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#setup-open-source-mesa-3d-graphics-libraries-for-opengl-and-vulkan",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#setup-open-source-mesa-3d-graphics-libraries-for-opengl-and-vulkan",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Setup Open source Mesa 3d Graphics libraries for OpenGL and Vulkan",
    "text": "Setup Open source Mesa 3d Graphics libraries for OpenGL and Vulkan\n\nInstall drivers\n\nNow, if you want media and graphics support beyond compute, install bleeding edge Mesa libraries from oiabf ppa that provides open graphics drivers; you can do it by:\nCoding → add-apt-repository ppa:oibaf/graphics-drivers\nCoding → apt-update\nCoding → apt-upgrade\n\nCoding → dpkg -l | grep -i mesa\nii  libegl-mesa0:amd64                                                      22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the EGL API -- Mesa vendor library\nii  libgl1-mesa-dri:amd64                                                   22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the OpenGL API -- DRI modules\nii  libglapi-mesa:amd64                                                     22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the GL API -- shared library\nii  libglu1-mesa:amd64                                                      9.0.2-1                                                           amd64        Mesa OpenGL utility library (GLU)\nii  lib\n-mesa0:amd64                                                      22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the OpenGL API -- GLX vendor library\nii  mesa-utils                                                              8.4.0-1ubuntu1                                                    amd64        Miscellaneous Mesa utilities -- symlinks\nii  mesa-utils-bin:amd64                                                    8.4.0-1ubuntu1                                                    amd64        Miscellaneous Mesa utilities -- native applications\nii  mesa-va-drivers:amd64                                                   22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa VA-API video acceleration drivers\nii  mesa-vdpau-drivers:amd64                                                22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa VDPAU video acceleration drivers\nii  mesa-vulkan-drivers:amd64                                               22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa Vulkan graphics drivers\nFair warning, be careful as these drivers are bleeding edge and, by design, may not be stable. After installing the Mesa drivers, you should be able to run glx and Vulkan benchmarks.\n\nCheck if opengl detects the dgpu using glxinfo\n\nInstall glxinfo from apt repo mesa-utils\nTo use the dGPU, set the env variable DRI_PRIME=1, PRIME is a technology in Linux that uses open source graphics drivers to use switchable graphics and install glxinfo from apt repo mesa-utils.\nHere is the output of glxinfo without setting DRI_PRIME environment variable:\nCoding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Graphics (ADL GT2) (0x46a6)\nAfter setting the environment variable for PRIME:\nCoding → export DRI_PRIME=1\nCoding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Arc(tm) A370M Graphics (DG2) (0x5693)\nAs you can the dgpu is recognized by Mesa OpenGL. yay!\n\nLet’s try to run a benchmark on the Arc gpu to see how it performs. I am using glmark2, which can be installed on Ubuntu-based OSes easily using:\n\nCoding → apt-get install glmark2\nCoding → export DRI_PRIME=1\nCoding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Arc(tm) A370M Graphics (DG2) (0x5693)\nCoding → glmark2\n\n\n\nglmark2 running on Arc 370m dGPU\n\n\n\nNow, let’s install the intel compute drivers. Goto. Get the latest release and install using the deb packages for OpenCL, level zero, igc etc.\n\nAs of this writing, the latest compute driver release version is 22.39.24347:\nCoding → cd /tmp && mkdir compute_drivers\nCoding → cd compute_drivers\nCoding → wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.12149.1/intel-igc-core_1.0.12149.1_amd64.deb\nCoding → wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.12149.1/intel-igc-opencl_1.0.12149.1_amd64.deb\nCoding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/intel-level-zero-gpu_1.3.24347_amd64.deb\nCoding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/intel-opencl-icd_22.39.24347_amd64.deb\nCoding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/libigdgmm12_22.2.0_amd64.deb\nCoding → dpkg -i *.deb\nNow that the compute drivers are installed let’s see if OpenCL can detect the dgpu. Install clinfo from apt and check using:\nCoding → clinfo | grep \"0x5690\"\n  Device Name                                     Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\nWe can see that the dgpu has been detected."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#install-oneapi-basekit-and-device-discovery-using-sycl",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#install-oneapi-basekit-and-device-discovery-using-sycl",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Install oneAPI basekit and device discovery using sycl",
    "text": "Install oneAPI basekit and device discovery using sycl\n\nFinally, install the oneapi basekit to use the dpcpp runtime. I used 2022.2.0 version of oneapi basekit.\n\nPlease refer to oneAPI installation guide to install using the apt package manager.\nSource the oneAPI environment using:\n→ source /opt/intel/oneapi/setvars.sh\n~ → dpcpp -v\nIntel(R) oneAPI DPC++/C++ Compiler 2022.2.0 (2022.2.0.20220730)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /opt/intel/oneapi/compiler/2022.2.0/linux/bin-llvm\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/11\nSelected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/11\nCandidate multilib: .;@m64\nSelected multilib: .;@m64\n\nDevice discovery using sycl-ls see if sycl can detect the dgpu:\n\nIntel dgpus like the A370m are represented as SYCL devices. sycl-ls is a tool that is part of the oneAPI basekit that can show all the detected devices and all the SYCL backends support by the runtime. Once the oneapi basekit has been installed, source the environment using:\nCoding → source /opt/intel/oneapi/setvars.sh \nDevice discovery using syclto see if sycl can detect the dgpu:\nCoding → sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2022.14.7.0.30_160000]\n[opencl:cpu:1] Intel(R) OpenCL, 12th Gen Intel(R) Core(TM) i7-12700H 3.0 [2022.14.7.0.30_160000]\n[opencl:gpu:2] Intel(R) OpenCL HD Graphics, Intel(R) Graphics [0x5693] 3.0 [22.40.024349]\n[opencl:gpu:3] Intel(R) OpenCL HD Graphics, Intel(R) Graphics [0x46a6] 3.0 [22.40.024349]\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x5693] 1.3 [1.3.24349]\n[ext_oneapi_level_zero:gpu:1] Intel(R) Level-Zero, Intel(R) Graphics [0x46a6] 1.3 [1.3.24349]\nTo get a more verbose output, use, sycl-ls --verbose:\nCoding → sycl-ls --verbose | grep -i name\n    Name     : Intel(R) FPGA Emulation Platform for OpenCL(TM)\n        Name       : Intel(R) FPGA Emulation Device\n    Name     : Intel(R) OpenCL\n        Name       : 12th Gen Intel(R) Core(TM) i7-12700H\n    Name     : Intel(R) OpenCL HD Graphics\n        Name       : Intel(R) Graphics [0x5693]\n    Name     : Intel(R) OpenCL HD Graphics\n        Name       : Intel(R) Graphics [0x46a6]\n    Name     : Intel(R) Level-Zero\n        Name       : Intel(R) Graphics [0x5693]\n        Name       : Intel(R) Graphics [0x46a6]\n    Name     : SYCL host platform\n        Name       : SYCL host device\nAs seen above 2 GPU devices are detected by the SYCL runtime and are supported using both OpenCL and Level-Zero drivers.\nWe now have configured the machine with all the required software stack to fully utilize the discrete gpu available on the laptop. These instructions can be used to enable any Arc discrete GPUs like the A370m, A770m, A770, A750 etc on Linux."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#intels-system-monitoring-utility-to-monitor-the-dgpu",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#intels-system-monitoring-utility-to-monitor-the-dgpu",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Intel’s System Monitoring Utility to monitor the dgpu",
    "text": "Intel’s System Monitoring Utility to monitor the dgpu\n\nInstall sysmon\n\nsysmon is a tool similar to top for cpu, that is part of Intel’s Platform Tools Interfaces for GPU. sysmon helps in monitoring the Intel gpu parameters like frequency, memory, etc. The tool can be installed using:\nCoding → git clone https://github.com/intel/pti-gpu/\nCoding → cd pti-gpu/tools/sysmon\nCoding → mkdir build && cd build\nCoding → cmake -DCMAKE_BUILD_TYPE=Release .. && make\n\nAfter successfully building sysmon let’s check the dgpu frequency:\n\n./sysmon\n\n\n\nsystem monitor output on my laptop\n\n\nAs seen above both the igpu and dgpu performance can be monitored using the tool."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#acknowledgements",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#acknowledgements",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI wouldn’t have been able to do this without the help of @sanchitintel and @gujingui. Also, Phoronix has been publishing updates on the best way to enable Intel Arc dGPU, detailing the version of the kernel, mesa drivers, etc. That was my start in setting up the software stack."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#something-not-working",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#something-not-working",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Something not working?",
    "text": "Something not working?\nPlease create an issue here to track any issues with these steps."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#citation",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#citation",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Citation",
    "text": "Citation\nIf you are using this information, please cite using the below link:\nUnnikrishnan Nair, R. (2022). dgpu_setup_pytorch (Version 1.0.0) [Computer software]. https://github.com/rahulunair/intel_arc_dgpu_linux"
  },
  {
    "objectID": "posts/2021-01-14-rust-result.html",
    "href": "posts/2021-01-14-rust-result.html",
    "title": "Flashcard Rust: Result Type in Rust and how to handle it",
    "section": "",
    "text": "Rust Result is a type used to return / propogate errors from a function to the caller. It is an Enum with two variants - an Ok(T) and an Err(E). An Ok(T) represents success and error represents failure.\n\n\n\nRust Result Type\n\n\nIn code it looks like:\nenum Result&lt;T, E&gt; {\n  Ok(T),\n  Err(E),\n}\nLet’s use a simple program to see how Result type can be used and handled. The program below has two functions, an is_even and the main function:\n// a simple program that shows how to use Result Type in Rust\nuse std::io::stdin;\n\n// check if a number is even or odd\n// return a Result type (Ok(String) if even, Err(String) if odd)\nfn is_even(n: u32) -&gt; Result&lt;String, String&gt; {\n    let even = n % 2;\n    match even {\n        0 =&gt; Ok(format!(\"{} is even!\", n)),\n        _ =&gt; Err(format!(\"{} is not even!\", n)),\n    }\n}\n\nfn main() {\n    // read input from stdin\n    let mut input = String::new();\n    println!(\"enter an integer: \");\n    stdin().read_line(&mut input).expect(\"enter an integer!\");\n\n    // parse String as u32, returns a Result type\n    let input = input.trim().parse::&lt;u32&gt;().unwrap();\n\n    // is_even function returns a custom Result type\n    let res = is_even(input).unwrap();\n    println!(\"{:?}\", res);\n}\nAs the name suggests, is_even is used to check if a digit is even or odd, it returns a Result Type, both Ok and Err variants of the type returns a String in case the digit is even or failure (Error) if its odd.\nLet’s see how the program works, building and running the program using cargo:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n2\n\"2 is even!\"\n➜  results git:(master) ✗\nThat worked as expected, we gave 2 and it printed on the screen 2 is even.\nNow if we give an odd number, let’s see what happens:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n1\nthread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: \"1 is not even!\"', src/main.rs:24:30\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nThe program panicked with an Error value, \"1 is not even!\". That is not great, error handling to say the least.\nThe main function has a few other things going on as well, the line:\nstdin().read_line(&mut input).expect(\"enter an integer\");\ntries to read a line of input from standard input as a String owned by the variable input, if it fails, the program panics and outputs, “enter an integer”.\nThe line:\nlet input = input.trim().parse::&lt;u32&gt;().expect(\"error in parsing input\");\ntrims the input string and tries to parse the value as an unsigned integer, it uses the same variable input to assign its result, this is called shadowing in Rust, if you are not familar with it, please see\nNow that you understand the program, let’s talk about the ways to handle a Result type. There are three ways in general to handle a Result type:\n\nunwrap - This is the simplest case, here it essentially means, we don’t care about the error, and tells the program to try to get the success (Ok) value and if the call results in a failure (Err), panic. This is okay in the case we are writing simple scripts, or knows for sure that there should be an Ok value, or if we, you know are lazy.\n? - It is a short hand notation in Rust, which basically tries to unwrap a value if it’s a success (Ok) or if it’s a failure returns an Err. As it can have two possible variants, and errors have to be handled some way, ? can only be used inside functions that returns a Result Type.\n\nWe could use it in our program, but we will have to change the signature of our main to:\nfn main() -&gt; Result(&lt;(), std:io:Error&gt; {\n\n    //same code till `let res = ` as original program\n    let res = is_even(input)?; // change unwrap to ?\n    Ok(())\n}\nThis can be read as if failure, main will exit with an error code, if not main returns nothing a ().\n\nFinally, we come to the most exhaustive way to handle Result type, here, we use a match express for both success (Ok) and failure (Err), to capture and handle all possible scenarios gracefully.\n\nTo use it in our original program, remove is_even(input).unwrap(); line and add:\nfn main() {\n\n    //same code till `let res = ` as original program\n    match is_even(input) {\n        Ok(val) =&gt; println!(\"{:?}, val),\n    Err(err) =&gt; println!(\"{:?}, err),\n    }\n}\nDoing this, we can avoid our program from panicking and now when we run the program, it doesn’t panic:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n1\n\"1 is not even!\"\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n2\n\"2 is even!\"\n➜  results git:(master) ✗\nyay!, now isn’t this much better than before :) .\nSo to wrap up, Result type in Rust is an Enum used to handle success and failure scenarios in functions. It can be handled in three ways, using unwrap, using a ? or using a match expression.\n\n\nFor more details, please read:\nRust docs on Result type Unwrap and Expect in Rust Unrwap and Expect\n\n\n\nIn this post, we got a basic idea of what Results are and how to handle them. Next time, we can see how to handle errors in a bit more detail."
  },
  {
    "objectID": "posts/2021-01-14-rust-result.html#references",
    "href": "posts/2021-01-14-rust-result.html#references",
    "title": "Flashcard Rust: Result Type in Rust and how to handle it",
    "section": "",
    "text": "For more details, please read:\nRust docs on Result type Unwrap and Expect in Rust Unrwap and Expect"
  },
  {
    "objectID": "posts/2021-01-14-rust-result.html#the-end",
    "href": "posts/2021-01-14-rust-result.html#the-end",
    "title": "Flashcard Rust: Result Type in Rust and how to handle it",
    "section": "",
    "text": "In this post, we got a basic idea of what Results are and how to handle them. Next time, we can see how to handle errors in a bit more detail."
  },
  {
    "objectID": "posts/2017-03-31-juggling.html",
    "href": "posts/2017-03-31-juggling.html",
    "title": "Monkeys, Juggling and some Golang",
    "section": "",
    "text": "Today let’s do some basic monkey work, rather lets do some juggling. I am always fascinated by people being able to juggle multiple balls. It’s something amazing, at least to me. They even have a World Juggling day, can you believe it?! So, why are we talking about monkeys, juggling and what not… Well, to me golang goroutines is sort of juggling. Where the juggler is able to do one thing at one time and, is able to keep track of multiple balls or whatever is being juggled. Well, you might have a different opinion and you are right, if the opinion is something like we can truely do tasks parallely if there are multiple CPUs, akin to two monkeys handling 2 seperate balls. hmm… yeah, so this might be the most over simplified and in some ways incorrect expalnation of concurrency and parallelism. For people who are curious to know the difference, there are awesome videos out there, particularly one comes to mind, the one Rob Pike gave. Check it out, you might like it.\nI initially thought of writing about different ways in which we can do concurrent jobs in both golang and python. Well, now that I am thinking let me give an example of how to use go routines.\nGo routines are basically light threads handled by the go scheduler to do concurrent jobs. There are primitives such as mutexes, synchonozation etc. to prevent conditions like data lock, unwanted write, multiple threads trying to read and write a resource at the sametime etc. more on that later.\n\n\nSo let’s see how to find the value of e in three different ways. I think if you ask me what is your favorite number, I might say it is e, never ending, rather transcendental constant with value 2.7182818… . Funny thing is that this number comes up in many places across the universe, it was made famous by Bernoulli when he came across it, while studying compound interest..go figure out..&gt;&gt;\n\n\n\nThere are lot of ways to find e, we can use the Binomial expansion, Newton’s method, Brother’s method (formualated in 2004 I think) etc.\npackage main\n\n    import (\n        \"fmt\"\n        \"math\"\n        \"sync\"\n    )\n\n    // dumbFact is a recursive function to print factorial\n    func dumbFact(num float64) float64 {\n        if num &lt;= 1 {\n            return 1\n        }\n        return num * dumbFact(num-1)\n    }\n\n    // Brother method to find e using approximation\n    // for n=0 to limit\n    //    (2n+2)/(2n+1)!\n    func Brother(limit float64, wg *sync.WaitGroup) {\n        var n float64\n        var e float64\n        // `defer` keyword will run the expression after the\n        //function returns\n        defer wg.Done()\n        for n &lt;= limit {\n            e = e + (2*n+2)/dumbFact(2*n+1)\n            n = n + 1\n        }\n        fmt.Printf(\"Brother's method for `e` yields: %20.15f\\n\", e)\n    }\n\n    // Binomial method to find `e`\n    func Binomial(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        e := math.Pow((1 + 1/limit), limit)\n        fmt.Printf(\"Binomial solution to `e` is : %20.15f\\n\", e)\n    }\n\n    // Newton method - approximation when n is big\n    func Newton(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        var n float64\n        var e float64\n        for n &lt;= limit {\n            e = e + 1/dumbFact(n)\n            n = n + 1\n        }\n        fmt.Printf(\"Newton's method gives the value as: %20.15f\\n\", e)\n    }\n\n    func main() {\n        fmt.Println(\"A simple go routine example\")\n        // A waitgroup is just used to wait until the\n        //routine completes\n        var wg sync.WaitGroup\n        wg.Add(3)\n        go Brother(10000, &wg)\n        go Binomial(10000, &wg)\n        go Newton(10000, &wg)\n        wg.Wait()\n        fmt.Println(\"Done!\")\n    }\nYou can see the program in action here.\n\n\n\nThe juggling happens in lines 58, 59 and 60 of the program. Where, the scheduler will concurrently work on the three routines, there is nothing complicated in the program, it uses the basic factorial function, uses something called the WaitGroup and runs three go routines.\nSo why am I using a WaitGroup, you ask. Well, WaitGroup is just to prevent go from falling through and terminating the programming before the routines are done. It tells go to wait till all the routines have finished running. We could waited for a simple input from the user at the end after line 61 and made the program explicitly wait, which would have given the routines time to run as well. I arbitrarily chose this WaitGroup option.\n\n\n\nEssentially, go routines are simple at first, and that is important. As in any concurrent programming paradigms, there are complications and real world pains, but the entry level bar is low, which is in my opinion neat. This helps one learn the concepts of concurrency eventhough you might not use Go. Next time we shall see how to do something similar to this in python, or will we ?"
  },
  {
    "objectID": "posts/2017-03-31-juggling.html#magical-number-e",
    "href": "posts/2017-03-31-juggling.html#magical-number-e",
    "title": "Monkeys, Juggling and some Golang",
    "section": "",
    "text": "So let’s see how to find the value of e in three different ways. I think if you ask me what is your favorite number, I might say it is e, never ending, rather transcendental constant with value 2.7182818… . Funny thing is that this number comes up in many places across the universe, it was made famous by Bernoulli when he came across it, while studying compound interest..go figure out..&gt;&gt;"
  },
  {
    "objectID": "posts/2017-03-31-juggling.html#an-example-program-to-find-value-of-e",
    "href": "posts/2017-03-31-juggling.html#an-example-program-to-find-value-of-e",
    "title": "Monkeys, Juggling and some Golang",
    "section": "",
    "text": "There are lot of ways to find e, we can use the Binomial expansion, Newton’s method, Brother’s method (formualated in 2004 I think) etc.\npackage main\n\n    import (\n        \"fmt\"\n        \"math\"\n        \"sync\"\n    )\n\n    // dumbFact is a recursive function to print factorial\n    func dumbFact(num float64) float64 {\n        if num &lt;= 1 {\n            return 1\n        }\n        return num * dumbFact(num-1)\n    }\n\n    // Brother method to find e using approximation\n    // for n=0 to limit\n    //    (2n+2)/(2n+1)!\n    func Brother(limit float64, wg *sync.WaitGroup) {\n        var n float64\n        var e float64\n        // `defer` keyword will run the expression after the\n        //function returns\n        defer wg.Done()\n        for n &lt;= limit {\n            e = e + (2*n+2)/dumbFact(2*n+1)\n            n = n + 1\n        }\n        fmt.Printf(\"Brother's method for `e` yields: %20.15f\\n\", e)\n    }\n\n    // Binomial method to find `e`\n    func Binomial(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        e := math.Pow((1 + 1/limit), limit)\n        fmt.Printf(\"Binomial solution to `e` is : %20.15f\\n\", e)\n    }\n\n    // Newton method - approximation when n is big\n    func Newton(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        var n float64\n        var e float64\n        for n &lt;= limit {\n            e = e + 1/dumbFact(n)\n            n = n + 1\n        }\n        fmt.Printf(\"Newton's method gives the value as: %20.15f\\n\", e)\n    }\n\n    func main() {\n        fmt.Println(\"A simple go routine example\")\n        // A waitgroup is just used to wait until the\n        //routine completes\n        var wg sync.WaitGroup\n        wg.Add(3)\n        go Brother(10000, &wg)\n        go Binomial(10000, &wg)\n        go Newton(10000, &wg)\n        wg.Wait()\n        fmt.Println(\"Done!\")\n    }\nYou can see the program in action here."
  },
  {
    "objectID": "posts/2017-03-31-juggling.html#whats-happening-here",
    "href": "posts/2017-03-31-juggling.html#whats-happening-here",
    "title": "Monkeys, Juggling and some Golang",
    "section": "",
    "text": "The juggling happens in lines 58, 59 and 60 of the program. Where, the scheduler will concurrently work on the three routines, there is nothing complicated in the program, it uses the basic factorial function, uses something called the WaitGroup and runs three go routines.\nSo why am I using a WaitGroup, you ask. Well, WaitGroup is just to prevent go from falling through and terminating the programming before the routines are done. It tells go to wait till all the routines have finished running. We could waited for a simple input from the user at the end after line 61 and made the program explicitly wait, which would have given the routines time to run as well. I arbitrarily chose this WaitGroup option."
  },
  {
    "objectID": "posts/2017-03-31-juggling.html#the-end",
    "href": "posts/2017-03-31-juggling.html#the-end",
    "title": "Monkeys, Juggling and some Golang",
    "section": "",
    "text": "Essentially, go routines are simple at first, and that is important. As in any concurrent programming paradigms, there are complications and real world pains, but the entry level bar is low, which is in my opinion neat. This helps one learn the concepts of concurrency eventhough you might not use Go. Next time we shall see how to do something similar to this in python, or will we ?"
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html",
    "href": "posts/2020-07-09-learning-methods.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Model free learning can be done using variations of temporal difference learning or Monte Carlo methods."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#temporal-difference-or-td-learning",
    "href": "posts/2020-07-09-learning-methods.html#temporal-difference-or-td-learning",
    "title": "Technical ramblings",
    "section": "Temporal Difference or TD Learning",
    "text": "Temporal Difference or TD Learning\nFrom each step learn something that would enable us to improve the estimated value for the next step. Consider this, three scenarios, in which the third scenario depends on the second and or the first. If that is the case, then knowing the states in the scenario can help us in better predicting the states in scenario three. We can improve the prediction in the third scenario if there is any change in states for either of the other scenarios, rather than waiting for the third one to finish and then realizing our prediction was close or way off.\nConsider, you are going somewhere and you expect to go through 2 cities. You estimate that you would reach the destination in 3 hours, as you know or estimate that you will need 1 hour each to cover the 2 other cities. Now, if it’s your lucky day and traffic is low in the first city, you could pass through it in 30 minutes instead of 60. Thus, you can estimate that you will reach the final destination 30 minutes early. While passing through the second city, you have car trouble and it takes an hour to fix it and start again. Thus, you now predict you would reach 30 minutes past the estimated time at your destination (Provided, you don’t face any further uncertainties). This continuous improvement of estimate is the main principle behind temporal difference or TD learning.\nWe can say, TD learning is an on-line learning (as we don’t need to wait for the entire episode to finish before updating our estimates). It bootstraps on the estimated value of other states to estimate value of the state in concern.\nTypes : TD(0) and TD(lambda)"
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#q-learning",
    "href": "posts/2020-07-09-learning-methods.html#q-learning",
    "title": "Technical ramblings",
    "section": "Q learning",
    "text": "Q learning\nIt is a variation of TD(0) learning, where we incrementally estimate the Q value for a state based on immediate rewards and the Q value for the next state. The variation is that, to estimate the Q value for the next state, we add the immediate reward with the Q value for the next state that maximizes the value (Q value for the state for the action that gives the maximum value). Also, unlike TD(0) learning, Q learning is an off-policy learning algorithm. Thus the estimated Q value at instance k is, the Q value at k for the state and action at time t plus the difference between estimated Q value using the immediate reward and discounted Q value for the next state for the action that gives the maximum value and the Q value of the current state. This delta between estimated Q value for the next state and Q value for the current state is weighted by a factor called the learning rate, which is between [0, 1]. The weighting factor or learning rate alpha can be decreased based on each iteration or as in many scenarios, use a small fixed value. The rate basically determines by how much we update the Q value. Now, if that sounds complex, trust me, it’s not, I am just not that good at explaining I guess. Just search for the algorithm online and you will get it instantly.\nHow is it done?\nWell, basically, the agent at time, t, in state, s, does an action, a, and moves to the next state and thus receives a reward, r. Now at time, t+1, the agent knows that it is in state, s, and knows the reward it obtained from the previous state. It uses this information along with the Q value for the optimum action for this state to get a better estimate of the Q value of the state, s. For this update to work, we need to have some starting point of Q values, in practice this is assumed to be zero or set randomly and at each iteration, k, the Q value is updated a little bit based on the learning rate to be a little closer to reality. In time, after many iterations of learning, the Q values for each states will reflect the real values that can be obtained. It has been proved (don’t ask me how, I haven’t checked out the proof) that if we do this iterative update an infinite time, we will eventually get the right Q values, irrespective on the initial Q values, the actions we took in each state etc."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#sarsa-learning",
    "href": "posts/2020-07-09-learning-methods.html#sarsa-learning",
    "title": "Technical ramblings",
    "section": "SARSA learning",
    "text": "SARSA learning\nFirst of all, what a creative naming, it must have taken them a long time to come up with this name. So, why is it called SARSA?, It’s because, the learning algorithm uses the present state, S, the action taken, A, the reward obtained, R, the next state, S, and the action taken, A, in this next state, while following a policy. SARSA stands for State-Action-Reward-State-Action!. While in traditional Q value function, the objective was to estimate the optimum policy doing exploration of a random policy, in SARSA, we start with a policy and tries to estimate the Q value of starting at a state, doing an action in state and following a policy, that is not changed in the course of learning. Thus it is an on-policy learning algorithm, as we don’t change the policy that has been chosen for a particular iteration. The idea is that if we are able to try all the states and all the possible actions infinitely many times, then this will eventually converge to the optimum policy itself. For a particular iteration, it is computationally less demanding that Q learning, but overall, it may need more time to converge. This learning is used when the state transition probabilities might not be fixed, there can be changes in the probabilities of switching from one state to the other. The only change in the algorithm compared to Q learning is, it doesn’t take the action that maximizes the utility when calculating the next state value, but obeys by the policy and takes the stipulated action. Here, too if you don’t understand it, please check the algorithm once on line and it will be clear."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#actor-critic-algorithms",
    "href": "posts/2020-07-09-learning-methods.html#actor-critic-algorithms",
    "title": "Technical ramblings",
    "section": "Actor Critic Algorithms",
    "text": "Actor Critic Algorithms\nThis class of learning algorithms has two parts, the Actor, which is a policy function and the Critic, which is a value function that is used to obtain the value for a state transition following an action. After an action has been selected, the Critic estimates the value for the state and calculates value for the state (immediate reward plus value for the next state). This is used to evaluate the action taken, which is the difference between the calculated value (using Bellman equation) and the estimated value. This difference or delta is used to improve the probability for the action in that state. The improvement factor delta is weighted by a learning rate, beta. A variation of this algorithm called the A3C algorithm is one of the fastest RL algorithms out there."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#the-end",
    "href": "posts/2020-07-09-learning-methods.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFun and study well combined."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html",
    "href": "posts/2020-07-09-rl-intro.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Learning by doing, where the quality of learning is determined by how much reward one can get at the end of an episode."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html#reinforcement-learning-rl",
    "href": "posts/2020-07-09-rl-intro.html#reinforcement-learning-rl",
    "title": "Technical ramblings",
    "section": "",
    "text": "Learning by doing, where the quality of learning is determined by how much reward one can get at the end of an episode."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html#some-terms-that-people-drop-when-talking-about-rl",
    "href": "posts/2020-07-09-rl-intro.html#some-terms-that-people-drop-when-talking-about-rl",
    "title": "Technical ramblings",
    "section": "Some terms that people drop when talking about RL",
    "text": "Some terms that people drop when talking about RL\n\nMDP, Models and the rest:\n\nMarkov Decision Process or MDP\nMDP - A function that is a tuple of (S, A, R, T) , where S - State, A - Action, R - Reward for that A in S, T - Transition probability to go to a next state when action A is taken in state S.\n\n\nPolicy pi:S x A -&gt; [0, 1]\nThat is, for every state there is a mapping from state to action which has a probability. Sum of all possible action probabilities in a state is 1.\n\n\nTypes\nModel free - We don’t know the transition probabilities or rewards before hand. Model based - Transition and rewards are know, thus just follow the path using value iteration to get the optimum policy.\n\n\nGoal\nTo gather rewards.\n\n\nOptimality\nWhat is the goal?\nSimple, maximize the reward.\nThree models of optimality (criteria).\n\nFinite horizon - Expectation of reward from time = 0 to horizon.\nDiscounted, infinite horizon - Expectation of discounted rewards from time = 0 to infinity.\nAverage reward - Expectation of rewards from time t to horizon when limit of horizon tends to infinity.\n\n\nNote: Expectation is the average of results when some operation/task is repeated many times, while average is the average of something that is done once.\n\n\n\nHow the goal is achieved?\nThis depends on what the optimality condition is, is it convergence, speed of convergence etc.\n\n\nValue functions\nLinks optimal criteria to policies.\n\n\nTwo types of functions\nState value function : The value of a state s under policy pi.\nThis function helps us to know that is the value of being in a state s and then from there following the policy pi.\nState, Action function : The value of state s in which we take the action a.\nThis is similar to the state value function, and this one is called the Q value function, it the value of a state s in which we take a specific action a and then afterwards follow a policy pi. The cool thing about these functions are recursive in nature, that is, the solution or value of a state s can be known if we know that is the solution or value of the state s+1 is. This is called the Bellman Equation. Bellman Equation - The Expected return value of a state can be defined in terms of immediate reward for that state and value of possible next states, weighted by their transition probabilities.\n\n\nAchieving the best policy.\n\nOptimal Value functions\nThe value of taking the best or the optimal policy for a state is equal to the return we get by taking the best action for that state. Once we know the optimal value for a state, we can just greedily take the best action for the state and follow through the optimal value function, taking the best action in each state. This thus becomes our optimal policy. This is called the greedy policy, as we are inclined to take the best action available thus far to maximize the expected returns. This is all cool, only problem here is that we need to know the Transition probabilities of going from one state to another. This can be a problem in a model free environment, as we don’t have a clue what is the probability of going from one state to another before exhausting all possible transitions. Thus comes the optimal state, value functions to the rescue.\n\n\nOptimal (State, Value) functions\nThese functions, known as Q functions, makes weighted summation over different alternatives. Let’s think about it, for finding the optimal value for an MDP, we need to know which action is the best and how often the best action is chosen. What if we don’t know that, instead we just weigh different actions taken from the state and assigns a number value to each state for all possible actions. If we have a table like this, we could take the actions for each state that gives us the best value for that state and move on to the next state, following this same process again. In the end, we would get the optimum value for the state from where we started. If we had kept a tab on the actions we took from the starting state, then we have the list of actions that enabled us to get the best possible value, which is the optimal action.\nRelationships between the Q value, Value and Policy is a follows:\n\nOptimum Value is equal to Q values for each state when the action that gives maximum value for a state is taken.\nOptimum policy is nothing but the action that resulted in getting the optimum values in the first place."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html#the-end",
    "href": "posts/2020-07-09-rl-intro.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html",
    "href": "posts/2021-03-22-rust-type.html",
    "title": "Flashcard Rust: What type is this variable again?!",
    "section": "",
    "text": "This is going to be a very short post on a tiny function that can help us in figuring out what is the type of a variable.\nThis is helpful to me personally when learning rust and after some hours of coding, I want to know what is the type of a variable quickly, which is not easy always.I need to thank @timClicks for showing this in one of his streams on youtube. If you are learning Rust and is looking for a book, let me please recommend Rust in Action by Tim, it is an excellent in-depth introduction to Rust.\n\n\nuse std::any::type_name;\n\n// what is the type of the passed argument\nfn what_type&lt;T&gt;(_: &T) {\n    println!(\"type is: {}\", type_name::&lt;T&gt;());\n\n}\ntype_name function from the std crate returns the name of a type as a string slice.\nBefore knowing about this function available from the std crate(reminder to self, to read the standard library docs!), i tended to use the ide introspection features to confirm what the type of a variable was at times or use a () as type and the compiler complains to me that the type is something else, which is what i wanted to known in the first place, but know I have a shiny new function that I can use, lets see how it works.\n\n\n\nLet’s take a simple example,\nfn main() {\n    let number = 3232;\n    let name = \"Rahul\".to_string();\n    let list_of_nums = vec![1, 2, ];\n    what_type(&number);\n    what_type(&name);\n    what_type(&list_of_nums);\n}\nThat is it, and it will print the type of the variable passed into the what_type function as below:\ntype is: i32\ntype is: alloc::string::String\ntype is: alloc::vec::Vec&lt;i32&gt;\n\n\n\ntype_name\n\n\n\nLike I said a short post about a really useful(hopefully) function in Rust."
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html#what-is-it",
    "href": "posts/2021-03-22-rust-type.html#what-is-it",
    "title": "Flashcard Rust: What type is this variable again?!",
    "section": "",
    "text": "use std::any::type_name;\n\n// what is the type of the passed argument\nfn what_type&lt;T&gt;(_: &T) {\n    println!(\"type is: {}\", type_name::&lt;T&gt;());\n\n}\ntype_name function from the std crate returns the name of a type as a string slice.\nBefore knowing about this function available from the std crate(reminder to self, to read the standard library docs!), i tended to use the ide introspection features to confirm what the type of a variable was at times or use a () as type and the compiler complains to me that the type is something else, which is what i wanted to known in the first place, but know I have a shiny new function that I can use, lets see how it works."
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html#how-is-it-used",
    "href": "posts/2021-03-22-rust-type.html#how-is-it-used",
    "title": "Flashcard Rust: What type is this variable again?!",
    "section": "",
    "text": "Let’s take a simple example,\nfn main() {\n    let number = 3232;\n    let name = \"Rahul\".to_string();\n    let list_of_nums = vec![1, 2, ];\n    what_type(&number);\n    what_type(&name);\n    what_type(&list_of_nums);\n}\nThat is it, and it will print the type of the variable passed into the what_type function as below:\ntype is: i32\ntype is: alloc::string::String\ntype is: alloc::vec::Vec&lt;i32&gt;"
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html#references",
    "href": "posts/2021-03-22-rust-type.html#references",
    "title": "Flashcard Rust: What type is this variable again?!",
    "section": "",
    "text": "type_name"
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html#end",
    "href": "posts/2021-03-22-rust-type.html#end",
    "title": "Flashcard Rust: What type is this variable again?!",
    "section": "",
    "text": "Like I said a short post about a really useful(hopefully) function in Rust."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html",
    "href": "posts/2020-07-15-ml-forget.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "In relation to learning new tasks, the phenomenon of a model forgetting how to perform a previously learned task when trained on a new task is called catastrophic forgetting (CF). Catastrophic forgetting can be at fault in online training of similar tasks as well. Ideally for a deep enough network to learn all tasks it has to be presented with all the training data at once. This is often not quite possible because of reasons like memory constraints, security, non-sustainable solution and serious limitations in the online learning environment. This is especially important in reinforcement learning because catastrophic inference is most visible in sequential online tasks. RL methods we use by definition is a sequential online learning algorithm. Here online means the agent has to adapt to the environment in real time."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#simple-methods-to-try-first",
    "href": "posts/2020-07-15-ml-forget.html#simple-methods-to-try-first",
    "title": "Technical ramblings",
    "section": "Simple methods to try first:",
    "text": "Simple methods to try first:\n\nRegularize with dropout and maxout\nStore weights for each environment\nTake average of these weights and initialize when learning a new environment\nAdd adaptive learning rate for weights, slow learning rate for weights for common skills for each environment and fast learning rate for new environments"
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#ideas-to-further-investigate",
    "href": "posts/2020-07-15-ml-forget.html#ideas-to-further-investigate",
    "title": "Technical ramblings",
    "section": "Ideas to further investigate:",
    "text": "Ideas to further investigate:\n\nPathNet - For learning similar tasks\nElastic Weight Consolidation - Good for dissimilar tasks\nH-DRLN - Hierarchal Deep Life Long RL – - A framework based that combines DQN and lifelong learning techniques\n\nBefore we dive in, lets see why does a neural network ever have ‘Catastrphic Forgetting’ problem?\nCatastrophic Forgetting is not only limited to neural nets; if that was the case, we could have easily replaced it some other learning algorithm. Researchers who studied this problem were of the opinion that the underlying cause is the generalization that a neural net does. The ability of a neural net to distribute and share representation of the relationship between input and an output label helps it to generalize, which is one of the most important properties of a neural net. Thus, each time a new data point (x, y) comes up to be trained, the neural net tries to distribute and share the representation by adjusting the weights of the neural net. This leads to the neural net forgetting earlier representations.\nWhat are some of the possible solutions?\nMany methods that detailed below can be explained off as a particular type of regularization. The techniques given below can be considered as a bag of tricks that could be used to limit catastrophic forgetting. As mentioned above, most of these solutions can be explained as a type of regularization and freezing of learned structures. To remember everything is to not change any of the weights or the activation traces of a task. Some methods directly tackle this by freezing weights, others by freezing activation traces of the entire network from input to output."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#regularization-dropout-and-activation-functions",
    "href": "posts/2020-07-15-ml-forget.html#regularization-dropout-and-activation-functions",
    "title": "Technical ramblings",
    "section": "Regularization, dropout and activation functions",
    "text": "Regularization, dropout and activation functions\n\nRegularization\nJoint Many-Task model - Successive regularization at each epoch\nDropout with Maxout\n\nRegularization is traditionally used to prevent overfitting; this can be done in many ways. A standard way to do this is to add a norm component to the loss function that is proportional to the square of the weights of a neural network (L2 norm). L2 regularization has the effect of the network preferring to learn smaller weights; as more significant the weight, larger would be the loss value. How this prevents forgetting can be attributed to the fact that, when the weights are small (regularized), small input values will not affect the loss function drastically. This, in turn means, the gradient of the loss would be a smaller value, thus when the weights which are adjusted based on this gradient value will not change much.\nDropout is used as a regularizing technique. Here how it regularizes can be explained by considering dropout as training a set of neural networks (each time dropping out a set of hidden units), then averaging the result of an ensemble of nets at the end [dropout]. This makes the model robust to losses by not assuming that a set of neurons (thus information) will always be present. Thus, when a new task is learned, the combined effect of a robust and regularized network can to some degree minimize forgetting.\nDropout has been empirically proven to help in adapting to new task while remembering old tasks. It has been suggested to use maxout as the activation function when using dropout as a technique to minimize forgetting."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#weight-freezingslow-updating",
    "href": "posts/2020-07-15-ml-forget.html#weight-freezingslow-updating",
    "title": "Technical ramblings",
    "section": "Weight freezing/slow updating",
    "text": "Weight freezing/slow updating\n\nUsing fast weights to de-blur old memories - Hinton\nElastic Weight Consolidation - Deepmind\n\nHere the idea is to not frequently update parameters of a Neural Net when learning new tasks. The larger the activation weights between two nodes for a task, the less chance that these weights should be affected. Hinton worked on this problem in the 80s and designed a network with weights of different rate of plasticity [Fast weights]. A similar approach was chosen by Kirkpatrick and the Deepmind team with the Elastic Weight Consolidation (EWC) technique. Here a constraint is added to the loss function that controls which weights can be updated and which cannot. When a new task is being learned, strong activation paths are not updated, and weights that didn’t contribute much to a previous task is updated first.\nThe insight that led to EWC is that, in a deep neural network, there can be many different configurations of weights that will give us a similar rate of error for a task. The goal with EWC is to find a set of weights from the parameter space that has low error rate for both the new task and the old one. In this approach the authors consider gradient descent in a Bayesian perspective. Whereas stochastic gradient descent tries to estimate a single parameter (weight), they tried to estimate a parameter for the entire distribution of data (Bayesian estimate), now this is not tractable, so what they do is that they use a trick called Laplace approximation and they call their approach EWC. By learning a distribution of parameters for each task, they were able to sample a set of parameters (weights) that worked for both tasks. [EWC]."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#ensemble-methods",
    "href": "posts/2020-07-15-ml-forget.html#ensemble-methods",
    "title": "Technical ramblings",
    "section": "Ensemble methods",
    "text": "Ensemble methods\nProgressive Neural Networks - Perfect memory PathNet - Deepmind\nEnsemble methods attempt to train multiple networks and combine them to get the result, essentially training separate classifiers for each new task.\nProgressive neural networks progressively extend its structure in proportion to new tasks. It starts with an initial neural net and to learn a new task; new lateral connections are formed in parallel to the initial one. This helps in few ways, first when a new task has to be learned the previous network weights are frozen thus eliminating catastrophic forgetting and as lateral connections between each layer are formed the new network shares knowledge from the previous network. The only disadvantage with a progressive network is with the growth of new tasks, the network weights also grow.\nPathNet is a network of networks. It has a fixed set of networks in which each layer has a collection of parallel mini neural network modules. Optimal paths are discovered using a genetic algorithm or reinforcement learning from the fixed size neural network for each task. After training and identification of a path, they are frozen. Thus the network does not forget. The cool thing about PathNet is that the base network is fixed and it does not grow further than the initial architecture. It is possible for the learned representations to be reused as well. PathNet is considered the best model for this type of learning."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#memory-methods-rehearsal",
    "href": "posts/2020-07-15-ml-forget.html#memory-methods-rehearsal",
    "title": "Technical ramblings",
    "section": "Memory Methods – Rehearsal",
    "text": "Memory Methods – Rehearsal\n\nEpisodic Memory Approach\nEpisodic Generative Approach\nDual memory models\nGradient Episodic Memory\n\nThese involve both static memory and generative memory methods. Static memory methods involve using a big experience replay type of memory to store previous session data, and during training of each new task, data from memory is randomly sampled and mixed with the new training data. The generative model can be a type of auto-encoder or a generative adversarial network (GAN), where the statistics of the data is stored and replayed while training for new tasks.\nDual memory models use a system of short-term memory (STM) modules and a long-term memory (LTM) module. Both of the memories have a combination of a generator and a learner. The STM which is a collection of task-specific networks, also, has a hash-table that keeps a tab on how many tasks the agent has learned and indexes each of the task-specific networks. The LTM uses a collection of the raw data sample from all the previous tasks to train its generative memory. The STM is used for fast training, and LTM is used to reinforce the STM.\nIn Gradient Episodic Memory, when learning in a new environment, weight update depends on a meeting a metric. Here for each task, a sample of the states (X-y) are stored in memory. For each update, the gradient is compared with the gradient of all the previous tasks, if the gradient of the current update does not contribute to the overall gradients of all the previous tasks, then an approximation that does contribute is chosen as the update. This reduces drastic changes to weights when learning new tasks, hence prevents catastrophic forgetting.\nNow let me give some details on approaches specific to reinforcement learning."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#life-long-reinforcement-learning",
    "href": "posts/2020-07-15-ml-forget.html#life-long-reinforcement-learning",
    "title": "Technical ramblings",
    "section": "Life Long Reinforcement Learning",
    "text": "Life Long Reinforcement Learning\nA lifelong reinforcement learning agent should have 2 main abilities: Efficiently retain knowledge base of learned policy in an environment (highly regularized sparse data structure). Efficient transfer - Should have the ability to transfer knowledge from the previous environment to the new one.\nIn life-long reinforcement learning, instead of learning each task, the objective is for the agent to distill learning from different environments, use shared knowledge and learn environment specific information as well. In the following section task and environment mean the same thing."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#life-long-rl-through-multiple-environments",
    "href": "posts/2020-07-15-ml-forget.html#life-long-rl-through-multiple-environments",
    "title": "Technical ramblings",
    "section": "Life Long RL through multiple environments",
    "text": "Life Long RL through multiple environments\nThese learning methods are based on introducing bias into learning. Here, bias refers to knowledge about the environment. So a lifelong learning agent is initialized with something called the initial bias which is the weights of the previous environment and then updated with learning bias for the new environment. Now, this is in some ways similar to what our DQN agent is doing where we initialize the net with the weights trained from the initial simulator and then use this to bootstrap for the real environment. This can be extended, and initial bias could be the average of weights for all the previous environments. Also, adaptive learning rates are used for weights. The weights are updated in proportion to the amount by which they varied for the previous environments. If they did not vary by a threshold value, the learning rate for those weights are set to be very low, and if the weights beyond a threshold, then that means that these weights are environment dependent, thus the learning rate for these weights for the new environment is set to a higher one.\nMost of the recent lifelong learning schemes used in RL is based on an algorithm known as ELLA (Efficient Lifelong Learning Algorithm). The principle behind ELLA is that we assume each task is sequentially submitted to the agent, and each learnable parameter is considered as a linear combination of a latent basis component (a component common to all tasks) and a task-specific component. Thus, the objective would be to minimize predictive loss over all tasks while encouraging significance of the latent shared component. This is done by introducing a sort of inductive bias (inductive of shared task information) to the loss function."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#policy-gradient---ella",
    "href": "posts/2020-07-15-ml-forget.html#policy-gradient---ella",
    "title": "Technical ramblings",
    "section": "Policy Gradient - ELLA",
    "text": "Policy Gradient - ELLA\nThe goal of policy gradient using efficient lifelong learning algorithm is to find optimum policies parameterized by a set of weights for each environment. We use the same approach as in general ELLA where the parameters (weights) for each task is considered to be a linear combination of parameters of the common latent model between the tasks and task-specific weights. Here the latent structure is stored in memory for future learning, and the PG algorithm in each of its iteration uses this knowledge base to update both latent weights and task-specific weights."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#hierarchical-deep-reinforcement-lifelong-learning-network-h-drln",
    "href": "posts/2020-07-15-ml-forget.html#hierarchical-deep-reinforcement-lifelong-learning-network-h-drln",
    "title": "Technical ramblings",
    "section": "Hierarchical Deep Reinforcement Lifelong Learning Network (H-DRLN)",
    "text": "Hierarchical Deep Reinforcement Lifelong Learning Network (H-DRLN)\nTo solve complex tasks, rather than just knowing what action to take in a state, an agent has to learn skills; these would include things like picking up an object, moving from a point to another point, etc. Reinforcement learning was extended using the options framework to do exactly this. [options framework]. To use reusable skills in a lifelong manner, an algorithm should be able to learn a skill (Eg: how to move left or right); enable an agent to determine which of these skills should be used/reused and cache reusable skills.\nH-DRLN has something called a Deep Skill Network which stores independent DQNs that have special skills stored, for example, there could be 2 DQNs as in our case one for passive control and another for active control. Along with the Deep Skill Network, the agent has a generic output layer that outputs either simple actions or skills from the Deep Skill Network depending upon the state sampled. This agent can be considered as a DQN with actions being temporally extended to solve tasks. For training the agent, a modification is made to the experience reply, and a new memory called Skill Experience Reply (S-ER) is used.\nWith this, we come to the end of the survey on Catastrophic Forgetting and lifelong learning approaches. A few other methods like explicit sparse coding, policy distillation, and curriculum learning have not been mentioned here as many of the techniques discussed could be considered as variations of these algorithms."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#terms",
    "href": "posts/2020-07-15-ml-forget.html#terms",
    "title": "Technical ramblings",
    "section": "Terms",
    "text": "Terms\nPlasticity - The propensity of a weight to be affected by a change. A weight with high plasticity can be modified easily than a weight with lower plasticity. Auto-encoder - A type of neural networks used to learn representation of data, mainly for dimensionality reduction GAN - GAN or Generative Adversarial Networks uses a competing pair of networks to learn the representation of data and can be used to generate data points from the learned distribution."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#references",
    "href": "posts/2020-07-15-ml-forget.html#references",
    "title": "Technical ramblings",
    "section": "References",
    "text": "References\n[Dropout] : https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf [Empirical study] : https://arxiv.org/pdf/1312.6211.pdf [Fast weights]: https://www.cs.bham.ac.uk/~jxb/PUBS/COGSCI05.pdf [EWC]: https://arxiv.org/pdf/1612.00796.pdf [Lifelong ML]: https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf [Options framework]: http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf [Multi task learning PG]: http://proceedings.mlr.press/v32/ammar14.pdf [Gradient Episodic Memory]: https://arxiv.org/pdf/1706.08840.pdf [PathNet]: https://arxiv.org/pdf/1701.08734.pdf [Deep Generative Memory]: https://arxiv.org/pdf/1710.10368.pdf [Progressive Neural Net]: 1606.04671 Progressive Neural Networks"
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#the-end",
    "href": "posts/2020-07-15-ml-forget.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nThats all for now folks on Catastrophic Forgetting in Learning systems using connectionist methods.\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2022-09-06-arc-dgpu-stable-diffusion.html",
    "href": "posts/2022-09-06-arc-dgpu-stable-diffusion.html",
    "title": "Stable Diffusion inference on Intel Arc GPUs",
    "section": "",
    "text": "Now that we have our Arc discrete GPU setup on Linux, let’s try to run Stable Diffusion model using it."
  },
  {
    "objectID": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#a-quick-recap-updated-steps-to-set-up-arc-on-linux",
    "href": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#a-quick-recap-updated-steps-to-set-up-arc-on-linux",
    "title": "Stable Diffusion inference on Intel Arc GPUs",
    "section": "A quick recap / updated steps to set up Arc on Linux",
    "text": "A quick recap / updated steps to set up Arc on Linux\nIntel has now published documentation on how to set up Arc on Linux. I tried it today, it worked beautifully.\n\nSteps to configure Arc\n\nInstall the 5.7 OEM kernel\nInstall kernel mode drivers, gpu firmware\nInstall usermod drivers for compute, 3d graphics and media\nAdd user to render group\nInstall oneAPI 2022.3 (latest as of this writeup)"
  },
  {
    "objectID": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#stable-diffusion",
    "href": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#stable-diffusion",
    "title": "Stable Diffusion inference on Intel Arc GPUs",
    "section": "Stable Diffusion",
    "text": "Stable Diffusion\nStable Diffusion is a fully open-source (thank you Stability.ai) deep learning text to image and image to image model. For more information on the model, checkout the wikipedia entry for the same.\n\nPyTorch\nTo use PyTorch on Intel GPUs, we need to install, the Intel extensions for PyTorch or ipex. Let’s get the latest release for pyTorch and ipex.\n\nCreate a conda environment with Python 3.9 and install both of the wheels.\n\n~ → conda create -n ipex python=3.9 -y\n~ → conda activate ipex\n~ → pip install ~/Downloads/*.whl\nLet’s see how to run the model using PyTorch first,\n\nInstall diffusers library and dependencies\n\n~ → pip install diffusers ftfy transformers Pillow\n\nRun stable diffusion\n\nWe will use a model from 🤗 maintained by runwayml, runwayml/stable-diffusion-v1-5. To use the model, you will have to generate a User access token for the 🤗 model hub. Once generated we can easily download the model using diffusers API. Now that we have installed all the required packages and have the user token, lets try it out:\nimport intel_extension_for_pytorch\nimport torch\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id=\"runwayml/stable-diffusion-v1-5\"\nprompt = \"vivid red hot air ballons over paris in the evening\"\npipe = StableDiffusionPipeline.from_pretrained(\n    model_id,\n    torch_dtype=torch.float16,  # this can be torch.float32 as well\n    revision=\"fp16\",\n    use_auth_token=\"&lt;the token you generated&gt;\")\npipe = pipe.to(\"xpu\")\nimage = pipe(prompt).images[0]\nimage.save(f\"{prompt[:5]}.png\")\nExecuting this, we get the result:\nIn [8]: image = pipe(prompt).images[0]\n   ...: \n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:35&lt;00:00,  1.43it/s]\nIn [9]: image = pipe(prompt).images[0]\n100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 51/51 [00:09&lt;00:00,  5.20it/s]\n\nAs you can see the first time you run the model, it takes about 35 seconds, subsequent runs take about 10 seconds, you can expect this number to double when using fp32.\n\n\nTensorFlow\nMoving on to TensorFlow, we have this awesome repo from divamgupta\n\nInstall stable_diffusion_tensorflow package and dependencies\n\n~ → pip install git+https://github.com/divamgupta/stable-diffusion-tensorflow ftfy pillow tqdm regex tensorflow-addons\n\nRun stable diffusion\n\nRunning the TensorFlow model is straightforward as there are no user tokens or anything like that required.\nimport intel_extension_for_tensorflow\nimport tensorflow\nfrom stable_diffusion_tf.stable_diffusion import StableDiffusion\nfrom PIL import Image\n\nprompt = \"vivid red hot air ballons over paris in the evening\"\ngenerator = StableDiffusion(\n    img_height=512,\n    img_width=512,\n    jit_compile=False,\n)\n\nimg = generator.generate(\n    prompt,\n    num_steps=50,\n    unconditional_guidance_scale=7.5,\n    temperature=1,\n    batch_size=1,\n)\nImage.fromarray(img[0]).save(\"sd_tf_fp32.png\")\nExecuting this, we get the result:\n2022-11-06 23:00:51.948547: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type XPU is enabled.\n  0   1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [01:00&lt;00:00,  1.21s/it]\n2022-11-06 23:01:55.103111: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type XPU is enabled.\n  0   1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:29&lt;00:00,  1.67it/s]\n\nAs you can see the first time you run the model, it takes about 60 seconds, subsequent runs take about 30 seconds. One thing to note here is that, for the TensorFlow version we used FP32 and not FP16 as in the case of pyTorch."
  },
  {
    "objectID": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#repo",
    "href": "posts/2022-09-06-arc-dgpu-stable-diffusion.html#repo",
    "title": "Stable Diffusion inference on Intel Arc GPUs",
    "section": "Repo",
    "text": "Repo\nYou can find the full code and other related materials here."
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html",
    "href": "posts/2021-01-17-rust-turbofish.html",
    "title": "Flashcard Rust: Colon colon angle bracket or turbofish!",
    "section": "",
    "text": "This confused me a lot when i started learning rust and the heading colon colon angle braket is what I searched for first when I saw this syntax, but the official name of this syntax within the Rust community is turbofish.\n\n\nIt is to specify a concrete type, for a function, a struct, a method or an enum. The syntax of turbofish looks like:\n::&lt;T&gt;\nIt looks kinda like a fish I guess, the name turbofish, check steve klabnik’s official explanation here for more details.\n\n\n\nLet’s take a simple example, to parse a string into float,\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nThis program will not compile as parse function is too generic, and the rust compiler being helpful as always tell us to give a time annotation to the variable pi_float:\n Compiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nerror[E0282]: type annotations needed\n --&gt; src/main.rs:3:9\n  |\n3 |     let pi_float = pi_string.parse().unwrap();\n  |         ^^^^^^^^ consider giving `pi_float` a type\nThis can be solved in two ways, either we use a turbofish notation for parse function or use type annotation, let’s see how we can use turbofish to solve this.\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse::&lt;f32&gt;().unwrap();\n    println!(\"{}\", pi_float);\n}\nWe tell the compile, which concrete type is being parsed using parse, and the code compiles:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415\nLastly, let’s try the compiler suggested fix, that is to use type annotation:\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float : f32 = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nYup, that works too!, as shown below:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415\n\n\n\nWhere to put turbofish What is Rust’s turbofish\n\n\n\nThis was a quick introduction to the turbofish notation and how it is used."
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html#what-is-it",
    "href": "posts/2021-01-17-rust-turbofish.html#what-is-it",
    "title": "Flashcard Rust: Colon colon angle bracket or turbofish!",
    "section": "",
    "text": "It is to specify a concrete type, for a function, a struct, a method or an enum. The syntax of turbofish looks like:\n::&lt;T&gt;\nIt looks kinda like a fish I guess, the name turbofish, check steve klabnik’s official explanation here for more details."
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html#how-is-it-used",
    "href": "posts/2021-01-17-rust-turbofish.html#how-is-it-used",
    "title": "Flashcard Rust: Colon colon angle bracket or turbofish!",
    "section": "",
    "text": "Let’s take a simple example, to parse a string into float,\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nThis program will not compile as parse function is too generic, and the rust compiler being helpful as always tell us to give a time annotation to the variable pi_float:\n Compiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nerror[E0282]: type annotations needed\n --&gt; src/main.rs:3:9\n  |\n3 |     let pi_float = pi_string.parse().unwrap();\n  |         ^^^^^^^^ consider giving `pi_float` a type\nThis can be solved in two ways, either we use a turbofish notation for parse function or use type annotation, let’s see how we can use turbofish to solve this.\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse::&lt;f32&gt;().unwrap();\n    println!(\"{}\", pi_float);\n}\nWe tell the compile, which concrete type is being parsed using parse, and the code compiles:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415\nLastly, let’s try the compiler suggested fix, that is to use type annotation:\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float : f32 = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nYup, that works too!, as shown below:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415"
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html#references",
    "href": "posts/2021-01-17-rust-turbofish.html#references",
    "title": "Flashcard Rust: Colon colon angle bracket or turbofish!",
    "section": "",
    "text": "Where to put turbofish What is Rust’s turbofish"
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html#end",
    "href": "posts/2021-01-17-rust-turbofish.html#end",
    "title": "Flashcard Rust: Colon colon angle bracket or turbofish!",
    "section": "",
    "text": "This was a quick introduction to the turbofish notation and how it is used."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi!, I am unrahul, welcome to my blog; here, you can read and comment on my ramblings and learnings on tech and not-so-tech topics.\nIf you are curious as to what the image seen here in the left is, it is a stable diffusion generated one when I used the prompt:\n“dali designing a cyberpunk version of a dream he had about a futuristic microprocessor”."
  }
]
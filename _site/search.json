[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "intel\n\n\ndgpu\n\n\ngraphics\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 22, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nJan 17, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nJan 14, 2021\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nml\n\n\n\n\nCatastrophic forgetting in Neural Nets\n\n\n\n\n\n\nJul 15, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nLearning methods in RL\n\n\n\n\n\n\nJul 14, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nLearning methods in RL\n\n\n\n\n\n\nJul 9, 2020\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprose\n\n\nml\n\n\n\n\nReinforment Learning for the unintiated\n\n\n\n\n\n\nJul 9, 2020\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\n\n\nMultiprocessing using ray.\n\n\n\n\n\n\nMay 23, 2020\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\n\n\nA simple timing decorator for python\n\n\n\n\n\n\nMar 23, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\n\n\nwhen to use threads and processes.\n\n\n\n\n\n\nMar 22, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 16, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 9, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\nrust\n\n\n\n\nA minimal introduction to Rust.\n\n\n\n\n\n\nMar 7, 2020\n\n\n0 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\n\n\nA simple intro to concurrency in Go\n\n\n\n\n\n\nMar 31, 2017\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoding\n\n\n\n\nAn interesting design pattern - Borg\n\n\n\n\n\n\nMar 28, 2017\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrypto\n\n\n\n\nPseudo random functions and message authentication.\n\n\n\n\n\n\nFeb 12, 2017\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncrypto\n\n\n\n\nA checklist to have when thinking of implementing crypto\n\n\n\n\n\n\nJan 26, 2017\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlife\n\n\n\n\nyup, that’s done!.\n\n\n\n\n\n\nJan 3, 2016\n\n\n0 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-01-14-rust-result.html",
    "href": "posts/2021-01-14-rust-result.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Rust Result is a type used to return / propogate errors from a function to the caller. It is an Enum with two variants - an Ok(T) and an Err(E). An Ok(T) represents success and error represents failure.\n\n\n\nRust Result Type\n\n\nIn code it looks like:\nenum Result<T, E> {\n  Ok(T),\n  Err(E),\n}\nLet’s use a simple program to see how Result type can be used and handled. The program below has two functions, an is_even and the main function:\n// a simple program that shows how to use Result Type in Rust\nuse std::io::stdin;\n\n// check if a number is even or odd\n// return a Result type (Ok(String) if even, Err(String) if odd)\nfn is_even(n: u32) -> Result<String, String> {\n    let even = n % 2;\n    match even {\n        0 => Ok(format!(\"{} is even!\", n)),\n        _ => Err(format!(\"{} is not even!\", n)),\n    }\n}\n\nfn main() {\n    // read input from stdin\n    let mut input = String::new();\n    println!(\"enter an integer: \");\n    stdin().read_line(&mut input).expect(\"enter an integer!\");\n\n    // parse String as u32, returns a Result type\n    let input = input.trim().parse::<u32>().unwrap();\n\n    // is_even function returns a custom Result type\n    let res = is_even(input).unwrap();\n    println!(\"{:?}\", res);\n}\nAs the name suggests, is_even is used to check if a digit is even or odd, it returns a Result Type, both Ok and Err variants of the type returns a String in case the digit is even or failure (Error) if its odd.\nLet’s see how the program works, building and running the program using cargo:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n2\n\"2 is even!\"\n➜  results git:(master) ✗\nThat worked as expected, we gave 2 and it printed on the screen 2 is even.\nNow if we give an odd number, let’s see what happens:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n1\nthread 'main' panicked at 'called `Result::unwrap()` on an `Err` value: \"1 is not even!\"', src/main.rs:24:30\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nThe program panicked with an Error value, \"1 is not even!\". That is not great, error handling to say the least.\nThe main function has a few other things going on as well, the line:\nstdin().read_line(&mut input).expect(\"enter an integer\");\ntries to read a line of input from standard input as a String owned by the variable input, if it fails, the program panics and outputs, “enter an integer”.\nThe line:\nlet input = input.trim().parse::<u32>().expect(\"error in parsing input\");\ntrims the input string and tries to parse the value as an unsigned integer, it uses the same variable input to assign its result, this is called shadowing in Rust, if you are not familar with it, please see\nNow that you understand the program, let’s talk about the ways to handle a Result type. There are three ways in general to handle a Result type:\n\nunwrap - This is the simplest case, here it essentially means, we don’t care about the error, and tells the program to try to get the success (Ok) value and if the call results in a failure (Err), panic. This is okay in the case we are writing simple scripts, or knows for sure that there should be an Ok value, or if we, you know are lazy.\n? - It is a short hand notation in Rust, which basically tries to unwrap a value if it’s a success (Ok) or if it’s a failure returns an Err. As it can have two possible variants, and errors have to be handled some way, ? can only be used inside functions that returns a Result Type.\n\nWe could use it in our program, but we will have to change the signature of our main to:\nfn main() -> Result(<(), std:io:Error> {\n\n    //same code till `let res = ` as original program\n    let res = is_even(input)?; // change unwrap to ?\n    Ok(())\n}\nThis can be read as if failure, main will exit with an error code, if not main returns nothing a ().\n\nFinally, we come to the most exhaustive way to handle Result type, here, we use a match express for both success (Ok) and failure (Err), to capture and handle all possible scenarios gracefully.\n\nTo use it in our original program, remove is_even(input).unwrap(); line and add:\nfn main() {\n\n    //same code till `let res = ` as original program\n    match is_even(input) {\n        Ok(val) => println!(\"{:?}, val),\n    Err(err) => println!(\"{:?}, err),\n    }\n}\nDoing this, we can avoid our program from panicking and now when we run the program, it doesn’t panic:\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n1\n\"1 is not even!\"\n➜  results git:(master) ✗ cargo build && cargo run\nenter an integer:\n2\n\"2 is even!\"\n➜  results git:(master) ✗\nyay!, now isn’t this much better than before :) .\nSo to wrap up, Result type in Rust is an Enum used to handle success and failure scenarios in functions. It can be handled in three ways, using unwrap, using a ? or using a match expression.\n\n\nFor more details, please read:\nRust docs on Result type Unwrap and Expect in Rust Unrwap and Expect\n\n\n\nIn this post, we got a basic idea of what Results are and how to handle them. Next time, we can see how to handle errors in a bit more detail."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html",
    "href": "posts/2022-08-12-arc-dgpu-linux.html",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "",
    "text": "These instructions are for Ubuntu 22.04 LTS-based Oses. I am using Pop!_OS 22.04 LTS derivative of Ubuntu 22.04."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#configure-linux-kernel",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#configure-linux-kernel",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Configure Linux kernel",
    "text": "Configure Linux kernel\n\nInstall the latest available Linux kernel. For example, I have installed kernel 6.0\n\nInstall a recent version of the kernel. The easiest way to install a different kernel is by using Mainline.\nroot@runnikri-mobl Coding → add-apt-repository ppa:cappelikan/ppa\nroot@runnikri-mobl Coding → apt update\nroot@runnikri-mobl Coding → apt install mainline\nAs seen in the screenshot below, I am using Kernel 6.0.0:\n\n\n\nMainline kernel installer\n\n\nAfter installing the kernel, please restart the machine.\n\nAs Arc dGPUs support are still experimental in the kernel, you will have to force the dGPU to be detected. This can be done using the kernel force_probe parameter i915.force_probe=<device_id> for the Intel i915 HD graphics driver. This will force probe the driver for new Intel graphics devices that are recognized by the kernel but not adequately supported. Hopefully, you wouldn’t have to do that with a newer version of the kernel. I am using a Yoga 7i (16” Intel) with Intel Arc Graphics which has the Intel® Arc™ A370M discrete Graphics card (dgpu) along with an integrated Intel® UHD Graphics. The device id for this dgpu is 5693, and you can find the device id of the card either by looking at i915 logs using sudo dmesg | grep -i i915 or from Intel’s gpu hardware table.\n\nPop!_OS 22.04 LTS uses systemd to manage kernel boot params, to force i915 driver to enable the dgpu use kernelstub tool:\nroot@runnikri-mobl Coding → kernelstub -a \"i915.force_probe=5693\"\nAfter this restart the machine and check i915 logs using dmesg to see if the graphics card has been detected:\nroot@runnikri-mobl Coding → dmesg | grep -i i915\nYou should see an output like this:\nroot@runnikri-mobl Coding →  dmesg | grep -i i915\n[    0.000000] Command line: initrd=\\EFI\\Pop_OS-97fe6a26-7d8a-4120-89db-8f2130b644b7\\initrd.img root=UUID=97fe6a26-7d8a-4120-89db-8f2130b644b7 ro quiet loglevel=0 systemd.show_status=false splash i915.force_probe=5693\n[    0.047217] Kernel command line: initrd=\\EFI\\Pop_OS-97fe6a26-7d8a-4120-89db-8f2130b644b7\\initrd.img root=UUID=97fe6a26-7d8a-4120-89db-8f2130b644b7 ro quiet loglevel=0 systemd.show_status=false splash i915.force_probe=5693\n[    1.775328] i915 0000:00:02.0: [drm] VT-d active for gfx access\n[    1.775383] i915 0000:00:02.0: vgaarb: deactivate vga console\n[    1.775416] i915 0000:00:02.0: [drm] Using Transparent Hugepages\n[    1.775979] i915 0000:00:02.0: vgaarb: changed VGA decodes: olddecodes=io+mem,decodes=none:owns=io+mem\n[    1.777234] i915 0000:00:02.0: [drm] Finished loading DMC firmware i915/adlp_dmc_ver2_16.bin (v2.16)\n[    1.912397] i915 0000:00:02.0: [drm] GuC firmware i915/adlp_guc_70.1.1.bin version 70.1\n[    1.912399] i915 0000:00:02.0: [drm] HuC firmware i915/tgl_huc_7.9.3.bin version 7.9\n[    1.926153] i915 0000:00:02.0: [drm] HuC authenticated\n[    1.926448] i915 0000:00:02.0: [drm] GuC submission enabled\n[    1.926449] i915 0000:00:02.0: [drm] GuC SLPC enabled\n[    1.927353] i915 0000:00:02.0: [drm] GuC RC: enabled\n[    1.930736] i915 0000:00:02.0: [drm] Protected Xe Path (PXP) protected content support initialized\n[    3.769893] [drm] Initialized i915 1.6.0 20201103 for 0000:00:02.0 on minor 0\n[    3.775461] i915 0000:03:00.0: enabling device (0000 -> 0002)\n[    3.775491] i915 0000:03:00.0: [drm] Incompatible option enable_guc=3 - HuC is not supported!\n[    3.776346] i915 0000:03:00.0: [drm] VT-d active for gfx access\n[    3.776472] i915 0000:03:00.0: [drm] Local memory IO size: 0x00000003fa000000\n[    3.776476] i915 0000:03:00.0: [drm] Local memory available: 0x00000003fa000000\n[    3.787361] fbcon: i915drmfb (fb0) is primary device\n[    3.787366] i915 0000:00:02.0: [drm] fb0: i915drmfb frame buffer device\n[    3.796583] i915 0000:03:00.0: [drm] Finished loading DMC firmware i915/dg2_dmc_ver2_06.bin (v2.6)\n[    5.326692] i915 0000:03:00.0: [drm] failed to retrieve link info, disabling eDP\n[    5.428126] i915 0000:03:00.0: [drm] GuC firmware i915/dg2_guc_70.1.2.bin version 70.1\n[    5.442889] i915 0000:03:00.0: [drm] GuC submission enabled\n[    5.442892] i915 0000:03:00.0: [drm] GuC SLPC enabled\n[    5.443378] i915 0000:03:00.0: [drm] GuC RC: enabled\nThe dg2_guc is loaded as seen in the above line: [ 5.428126] i915 0000:03:00.0: [drm] GuC firmware i915/dg2_guc_70.1.2.bin version 70.1. Both the igpu and dgpu is detected and also the firmware is loaded. If drm verification fails and GuC is not loaded, install the latest linux firmware from this link:\nYou can install the latest firmware files by cloning the repo and moving it to /lib/firmware (warning: this is a brute-force approach):\nroot@runnikri-mobl Coding →  git clone https://git.kernel.org/pub/scm/linux/kernel/git/firmware/linux-firmware.git/\nroot@runnikri-mobl Coding →  cd linux-firmware && yes | cp -r * /lib/firmware\nFor me, the firmware that came with Pop!_OS 22.04 LTS worked, i didn’t have to install the latest ones."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#setup-open-source-mesa-3d-graphics-libraries-for-opengl-and-vulkan",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#setup-open-source-mesa-3d-graphics-libraries-for-opengl-and-vulkan",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Setup Open source Mesa 3d Graphics libraries for OpenGL and Vulkan",
    "text": "Setup Open source Mesa 3d Graphics libraries for OpenGL and Vulkan\n\nInstall drivers\n\nNow, if you want media and graphics support beyond compute, install bleeding edge Mesa libraries from oiabf ppa that provides open graphics drivers; you can do it by:\nroot@runnikri-mobl Coding → add-apt-repository ppa:oibaf/graphics-drivers\nroot@runnikri-mobl Coding → apt-update\nroot@runnikri-mobl Coding → apt-upgrade\n\nroot@runnikri-mobl Coding → dpkg -l | grep -i mesa\nii  libegl-mesa0:amd64                                                      22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the EGL API -- Mesa vendor library\nii  libgl1-mesa-dri:amd64                                                   22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the OpenGL API -- DRI modules\nii  libglapi-mesa:amd64                                                     22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the GL API -- shared library\nii  libglu1-mesa:amd64                                                      9.0.2-1                                                           amd64        Mesa OpenGL utility library (GLU)\nii  lib\n-mesa0:amd64                                                      22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        free implementation of the OpenGL API -- GLX vendor library\nii  mesa-utils                                                              8.4.0-1ubuntu1                                                    amd64        Miscellaneous Mesa utilities -- symlinks\nii  mesa-utils-bin:amd64                                                    8.4.0-1ubuntu1                                                    amd64        Miscellaneous Mesa utilities -- native applications\nii  mesa-va-drivers:amd64                                                   22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa VA-API video acceleration drivers\nii  mesa-vdpau-drivers:amd64                                                22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa VDPAU video acceleration drivers\nii  mesa-vulkan-drivers:amd64                                               22.3~git2210120600.ddc5c3~oibaf~j                                 amd64        Mesa Vulkan graphics drivers\nFair warning, be careful as these drivers are bleeding edge and, by design, may not be stable. After installing the Mesa drivers, you should be able to run glx and Vulkan benchmarks.\n\nCheck if opengl detects the dgpu using glxinfo\n\nInstall glxinfo from apt repo mesa-utils\nTo use the dGPU, set the env variable DRI_PRIME=1, PRIME is a technology in Linux that uses open source graphics drivers to use switchable graphics and install glxinfo from apt repo mesa-utils.\nHere is the output of glxinfo without setting DRI_PRIME environment variable:\nroot@runnikri-mobl Coding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Graphics (ADL GT2) (0x46a6)\nAfter setting the environment variable for PRIME:\nroot@runnikri-mobl Coding → export DRI_PRIME=1\nroot@runnikri-mobl Coding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Arc(tm) A370M Graphics (DG2) (0x5693)\nAs you can the dgpu is recognized by Mesa OpenGL. yay!\n\nLet’s try to run a benchmark on the Arc gpu to see how it performs. I am using glmark2, which can be installed on Ubuntu-based OSes easily using:\n\nroot@runnikri-mobl Coding → apt-get install glmark2\nroot@runnikri-mobl Coding → export DRI_PRIME=1\nroot@runnikri-mobl Coding → glxinfo -B | grep -i device\n    Device: Mesa Intel(R) Arc(tm) A370M Graphics (DG2) (0x5693)\nroot@runnikri-mobl Coding → glmark2\n\n\n\nglmark2 running on Arc 370m dGPU\n\n\n\nNow, let’s install the intel compute drivers. Goto. Get the latest release and install using the deb packages for OpenCL, level zero, igc etc.\n\nAs of this writing, the latest compute driver release version is 22.39.24347:\nroot@runnikri-mobl Coding → cd /tmp && mkdir compute_drivers\nroot@runnikri-mobl Coding → cd compute_drivers\nroot@runnikri-mobl Coding → wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.12149.1/intel-igc-core_1.0.12149.1_amd64.deb\nroot@runnikri-mobl Coding → wget https://github.com/intel/intel-graphics-compiler/releases/download/igc-1.0.12149.1/intel-igc-opencl_1.0.12149.1_amd64.deb\nroot@runnikri-mobl Coding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/intel-level-zero-gpu_1.3.24347_amd64.deb\nroot@runnikri-mobl Coding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/intel-opencl-icd_22.39.24347_amd64.deb\nroot@runnikri-mobl Coding → wget https://github.com/intel/compute-runtime/releases/download/22.39.24347/libigdgmm12_22.2.0_amd64.deb\nroot@runnikri-mobl Coding → dpkg -i *.deb\nNow that the compute drivers are installed let’s see if OpenCL can detect the dgpu. Install clinfo from apt and check using:\nroot@runnikri-mobl Coding → clinfo | grep \"0x5690\"\n  Device Name                                     Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\n    Device Name                                   Intel(R) Graphics [0x5693]\nWe can see that the dgpu has been detected."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#install-oneapi-basekit-and-device-discovery-using-sycl",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#install-oneapi-basekit-and-device-discovery-using-sycl",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Install oneAPI basekit and device discovery using sycl",
    "text": "Install oneAPI basekit and device discovery using sycl\n\nFinally, install the oneapi basekit to use the dpcpp runtime. I used 2022.2.0 version of oneapi basekit.\n\n04:37:56  |base|rahul@pop-os ~ → dpcpp -v\nIntel(R) oneAPI DPC++/C++ Compiler 2022.2.0 (2022.2.0.20220730)\nTarget: x86_64-unknown-linux-gnu\nThread model: posix\nInstalledDir: /opt/intel/oneapi/compiler/2022.2.0/linux/bin-llvm\nFound candidate GCC installation: /usr/lib/gcc/x86_64-linux-gnu/11\nSelected GCC installation: /usr/lib/gcc/x86_64-linux-gnu/11\nCandidate multilib: .;@m64\nSelected multilib: .;@m64\n\nDevice discovery using sycl-ls see if sycl can detect the dgpu:\n\nIntel dgpus like the A370m are represented as SYCL devices. sycl-ls is a tool that is part of the oneAPI basekit that can show all the detected devices and all the SYCL backends support by the runtime. Once the oneapi basekit has been installed, source the environment using:\nroot@runnikri-mobl Coding → source /opt/intel/oneapi/setvars.sh \nDevice discovery using syclto see if sycl can detect the dgpu:\nroot@runnikri-mobl Coding → sycl-ls\n[opencl:acc:0] Intel(R) FPGA Emulation Platform for OpenCL(TM), Intel(R) FPGA Emulation Device 1.2 [2022.14.7.0.30_160000]\n[opencl:cpu:1] Intel(R) OpenCL, 12th Gen Intel(R) Core(TM) i7-12700H 3.0 [2022.14.7.0.30_160000]\n[opencl:gpu:2] Intel(R) OpenCL HD Graphics, Intel(R) Graphics [0x5693] 3.0 [22.40.024349]\n[opencl:gpu:3] Intel(R) OpenCL HD Graphics, Intel(R) Graphics [0x46a6] 3.0 [22.40.024349]\n[ext_oneapi_level_zero:gpu:0] Intel(R) Level-Zero, Intel(R) Graphics [0x5693] 1.3 [1.3.24349]\n[ext_oneapi_level_zero:gpu:1] Intel(R) Level-Zero, Intel(R) Graphics [0x46a6] 1.3 [1.3.24349]\nTo get a more verbose output, use, sycl-ls --verbose:\nroot@runnikri-mobl Coding → sycl-ls --verbose | grep -i name\n    Name     : Intel(R) FPGA Emulation Platform for OpenCL(TM)\n        Name       : Intel(R) FPGA Emulation Device\n    Name     : Intel(R) OpenCL\n        Name       : 12th Gen Intel(R) Core(TM) i7-12700H\n    Name     : Intel(R) OpenCL HD Graphics\n        Name       : Intel(R) Graphics [0x5693]\n    Name     : Intel(R) OpenCL HD Graphics\n        Name       : Intel(R) Graphics [0x46a6]\n    Name     : Intel(R) Level-Zero\n        Name       : Intel(R) Graphics [0x5693]\n        Name       : Intel(R) Graphics [0x46a6]\n    Name     : SYCL host platform\n        Name       : SYCL host device\nAs seen above 2 GPU devices are detected by the SYCL runtime and are supported using both OpenCL and Level-Zero drivers.\nWe now have configured the machine with all the required software stack to fully utilize the discrete gpu available on the laptop. These instructions can be used to enable any Arc discrete GPUs like the A370m, A770m, A770, A750 etc on Linux."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#intels-system-monitoring-utility-to-monitor-the-dgpu",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#intels-system-monitoring-utility-to-monitor-the-dgpu",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Intel’s System Monitoring Utility to monitor the dgpu",
    "text": "Intel’s System Monitoring Utility to monitor the dgpu\n\nInstall sysmon\n\nsysmon is a tool similar to top for cpu, that is part of Intel’s Platform Tools Interfaces for GPU. sysmon helps in monitoring the Intel gpu parameters like frequency, memory, etc. The tool can be installed using:\nroot@runnikri-mobl Coding → git clone https://github.com/intel/pti-gpu/\nroot@runnikri-mobl Coding → cd pti-gpu/tools/sysmon\nroot@runnikri-mobl Coding → mkdir build && cd build\nroot@runnikri-mobl Coding → cmake -DCMAKE_BUILD_TYPE=Release .. && make\n\nAfter successfully building sysmon let’s check the dgpu frequency:\n\n./sysmon\n\n\n\nsystem monitor output on my laptop\n\n\nAs seen above both the igpu and dgpu performance can be monitored using the tool."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#acknowledgements",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#acknowledgements",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI wouldn’t have been able to do this without the help of @sanchitintel and @gujingui. Also, Phoronix has been publishing updates on the best way to enable Intel Arc dGPU, detailing the version of the kernel, mesa drivers, etc. That was my start in setting up the software stack."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#something-not-working",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#something-not-working",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Something not working?",
    "text": "Something not working?\nPlease create an issue here to track any issues with these steps."
  },
  {
    "objectID": "posts/2022-08-12-arc-dgpu-linux.html#citation",
    "href": "posts/2022-08-12-arc-dgpu-linux.html#citation",
    "title": "Configure Intel Arc A370M Xe-HPG discrete GPU on Linux",
    "section": "Citation",
    "text": "Citation\nIf you are using this information, please cite using the below link:\nUnnikrishnan Nair, R. (2022). dgpu_setup_pytorch (Version 1.0.0) [Computer software]. https://github.com/rahulunair/intel_arc_dgpu_linux"
  },
  {
    "objectID": "posts/2020-03-22-pyjuggle.html",
    "href": "posts/2020-03-22-pyjuggle.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Now that everyone is talking about and using async/await and event loops everywhere, I thought I would go to the basics (well not to the fundamentals, but practical basics) and write a small program to download a bunch of text files and process them.\n\n\nFetch n ebooks from project gutenberg site and do some elemental processing on the content.\nSome of the code snippets:\n\ndef download(url: str) -> str:\n    \"\"\"download and decode content from a url.\"\"\"\n    page = requests.get(url)\n    name = url.split(\"/\")[-1]\n    return page.content.decode(\"utf-8\"), f\"texts/{name}\"\n\ndef save(content: str, fname: str = None):\n    \"\"\"save content to file with name.\"\"\"\n    with open(fname, \"w\") as fh:\n        fh.write(content)\n\ndef process(string: list) -> list:\n    \"\"\"cpu bound tasks\"\"\"\n    ...\n    ...\n    return string\n\ndef main():\n    urls = [\n        \"https://www.gutenberg.org/cache/epub/376/pg376.txt\",\n        \"https://www.gutenberg.org/files/84/84-0.txt\",\n        \"https://www.gutenberg.org/cache/epub/844/pg844.txt\",\n    ]\n\n    # call download\n\n    # call save\n\n    #call string process\nThe full source code is on github at pyjuggle.\n\n\n\nLet’s get this straight, concurrency and parallelism are not the same. Concurrency means, running multiple programs as though they are running at the same time, where as parallelism is running the programs truly in parallel in multiple cores of your machine.\nThe interesting aspect of Python (Ruby and few others) is that there is something called the global interpreter lock or GIL. This is part of the cpython implementation, where only one Python object can have access to the interpreter at one time. This means that, we cannot run two threads of execution parallely in Python but that all threads of execution are time sliced. You can read more on GIL here.\nThreads come handy though when there is a lot of IO involved or when there are subprocess calls, where the interpreter is handing-off to work to another subsystem, and a new thread of execution can start, while the previous thread waits for a response.\nWhen we need to real multi-core parallel tasks in Python, mulitprocessing library comes to our favour. It essentially spins up multiple Python interpreters and works on orthogal exlusive chunks of data, this method is good for CPU bound tasks.\n\n\n\nAlthough, there are exceptions, I would say when IO or subprocess calls are involved or when the interpreter is giving of control to another sub-system use threads.\nThe module of choice for me for concurrency in python is concurrent.futures.\nTo use a thread pool, we can do this:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n\n    # call download method concurrently\n    with tpe(max_workers=3) as exe:\n        results = exe.map(download, urls)\nHere, maximum of 4 threads will be spun up, each time sliced, and will start downloading the urls. One thing to note here is that, depending upon the amount of data, here the urls threaded code may not show significant performance improvement when compared to one without threading, so it is always good to time sequential and threaded code before you decide.\n{% include info.html text=“use threads when the task is IO bound” %}\nThe same approach can be used to save the files, create a threadpool and save the downloaded content:\nfrom concurrent.futures import ThreadPoolExecutor as tpe\n    ...\n    ...\n    # call save method concurrently\n    with tpe(max_workers=3) as exe:\n        exe.map(save, results)\n    ...\n    ...\nFinally we come across, a CPU bound task, which has to tokenize the saved files, here if we use threading, although there could be some speedup than serial execution, it is advicable to use multiprocessing, in some cases, threading could actually slow down the program if the program is mostly CPU bound.\nAn example of using a multiprocess executor is:\nfrom concurrent.futures import ProcessPoolExecutor as ppe\n\n    # call process method parallely\n    with ppe(max_workers=4) as exe:\n        exe.map(process, open(\"./all.txt\").readlines())\n{% include info.html text=“use processes when the task is CPU bound” %}\nHere we are mapping 4 process methods with chunks of data from the file, one thing to note here is that the amount of work distributed to the 4 processes is not controlled by the programmer and may not be equal.\n\n\n\nThis was an interesting experience, I wanted to write about this for few years now, finally got to it. I am pretty sure I have got some things wrong, if you find any such techincal inacurracies ping me on twitter or on github.\nMay be next time I will write, when to use async-await."
  },
  {
    "objectID": "posts/2020-03-23-tiemy.html",
    "href": "posts/2020-03-23-tiemy.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Footnotes\n\n\nsieve of sundaram↩︎"
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html",
    "href": "posts/2020-07-09-rl-intro.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Learning by doing, where the quality of learning is determined by how much reward one can get at the end of an episode."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html#some-terms-that-people-drop-when-talking-about-rl",
    "href": "posts/2020-07-09-rl-intro.html#some-terms-that-people-drop-when-talking-about-rl",
    "title": "Technical ramblings",
    "section": "Some terms that people drop when talking about RL",
    "text": "Some terms that people drop when talking about RL\n\nMDP, Models and the rest:\n\nMarkov Decision Process or MDP\nMDP - A function that is a tuple of (S, A, R, T) , where S - State, A - Action, R - Reward for that A in S, T - Transition probability to go to a next state when action A is taken in state S.\n\n\nPolicy pi:S x A -> [0, 1]\nThat is, for every state there is a mapping from state to action which has a probability. Sum of all possible action probabilities in a state is 1.\n\n\nTypes\nModel free - We don’t know the transition probabilities or rewards before hand. Model based - Transition and rewards are know, thus just follow the path using value iteration to get the optimum policy.\n\n\nGoal\nTo gather rewards.\n\n\nOptimality\nWhat is the goal?\nSimple, maximize the reward.\nThree models of optimality (criteria).\n\nFinite horizon - Expectation of reward from time = 0 to horizon.\nDiscounted, infinite horizon - Expectation of discounted rewards from time = 0 to infinity.\nAverage reward - Expectation of rewards from time t to horizon when limit of horizon tends to infinity.\n\n\nNote: Expectation is the average of results when some operation/task is repeated many times, while average is the average of something that is done once.\n\n\n\nHow the goal is achieved?\nThis depends on what the optimality condition is, is it convergence, speed of convergence etc.\n\n\nValue functions\nLinks optimal criteria to policies.\n\n\nTwo types of functions\nState value function : The value of a state s under policy pi.\nThis function helps us to know that is the value of being in a state s and then from there following the policy pi.\nState, Action function : The value of state s in which we take the action a.\nThis is similar to the state value function, and this one is called the Q value function, it the value of a state s in which we take a specific action a and then afterwards follow a policy pi. The cool thing about these functions are recursive in nature, that is, the solution or value of a state s can be known if we know that is the solution or value of the state s+1 is. This is called the Bellman Equation. Bellman Equation - The Expected return value of a state can be defined in terms of immediate reward for that state and value of possible next states, weighted by their transition probabilities.\n\n\nAchieving the best policy.\n\nOptimal Value functions\nThe value of taking the best or the optimal policy for a state is equal to the return we get by taking the best action for that state. Once we know the optimal value for a state, we can just greedily take the best action for the state and follow through the optimal value function, taking the best action in each state. This thus becomes our optimal policy. This is called the greedy policy, as we are inclined to take the best action available thus far to maximize the expected returns. This is all cool, only problem here is that we need to know the Transition probabilities of going from one state to another. This can be a problem in a model free environment, as we don’t have a clue what is the probability of going from one state to another before exhausting all possible transitions. Thus comes the optimal state, value functions to the rescue.\n\n\nOptimal (State, Value) functions\nThese functions, known as Q functions, makes weighted summation over different alternatives. Let’s think about it, for finding the optimal value for an MDP, we need to know which action is the best and how often the best action is chosen. What if we don’t know that, instead we just weigh different actions taken from the state and assigns a number value to each state for all possible actions. If we have a table like this, we could take the actions for each state that gives us the best value for that state and move on to the next state, following this same process again. In the end, we would get the optimum value for the state from where we started. If we had kept a tab on the actions we took from the starting state, then we have the list of actions that enabled us to get the best possible value, which is the optimal action.\nRelationships between the Q value, Value and Policy is a follows:\n\nOptimum Value is equal to Q values for each state when the action that gives maximum value for a state is taken.\nOptimum policy is nothing but the action that resulted in getting the optimum values in the first place."
  },
  {
    "objectID": "posts/2020-07-09-rl-intro.html#the-end",
    "href": "posts/2020-07-09-rl-intro.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2021-01-17-rust-turbofish.html",
    "href": "posts/2021-01-17-rust-turbofish.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "This confused me a lot when i started learning rust and the heading colon colon angle braket is what I searched for first when I saw this syntax, but the official name of this syntax within the Rust community is turbofish.\n\n\nIt is to specify a concrete type, for a function, a struct, a method or an enum. The syntax of turbofish looks like:\n::<T>\nIt looks kinda like a fish I guess, the name turbofish, check steve klabnik’s official explanation here for more details.\n\n\n\nLet’s take a simple example, to parse a string into float,\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nThis program will not compile as parse function is too generic, and the rust compiler being helpful as always tell us to give a time annotation to the variable pi_float:\n Compiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nerror[E0282]: type annotations needed\n --> src/main.rs:3:9\n  |\n3 |     let pi_float = pi_string.parse().unwrap();\n  |         ^^^^^^^^ consider giving `pi_float` a type\nThis can be solved in two ways, either we use a turbofish notation for parse function or use type annotation, let’s see how we can use turbofish to solve this.\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float = pi_string.parse::<f32>().unwrap();\n    println!(\"{}\", pi_float);\n}\nWe tell the compile, which concrete type is being parsed using parse, and the code compiles:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415\nLastly, let’s try the compiler suggested fix, that is to use type annotation:\nfn main() {\n    let pi_string = \"3.1415\";\n    let pi_float : f32 = pi_string.parse().unwrap();\n    println!(\"{}\", pi_float);\n}\nYup, that works too!, as shown below:\nCompiling turbofish v0.1.0 (/Users/unrahul/Coding/rust/learn/turbofish)\nFinished dev [unoptimized + debuginfo] target(s) in 0.21s\nRunning `target/debug/turbofish`\n3.1415\n\n\n\nWhere to put turbofish What is Rust’s turbofish\n\n\n\nThis was a quick introduction to the turbofish notation and how it is used."
  },
  {
    "objectID": "posts/2020-03-07-ahoy-rust.html",
    "href": "posts/2020-03-07-ahoy-rust.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Rust is one of the coolest systems programming lanugage on the block, let’s dip our feet in it.\n\n\nHopefully, you are using Linux. To install Rust, use:\n> curl https://sh.rustup.rs -sSf | sh\nRustup is a tool that is used to install Rust, the above script downloads and installs rustup first and then installs Rust.\n\n\n\nLet’s create a directory to keep our source files:\n> mkdir ~/rust_sources\n> cd ~/rust_sources\nOur first program, to say ahoy!\n> mkdir ahoy; cd ahoy\n> touch main.rs\nOpen main.rs and add the following line:\nfn main() {\n    println!(\"ahoy ahoy!\");  \n}\nSave the file and run rustc main.rs, thats it! We have compiled our first rust program.\nThere should be a file named main in the same directory main.rs is in, execute it using:\n./main\nahoy ahoy!\nNow that you have written 2 lines of rust, do read\n\n\n\n\n\n\nhttps://imgur.com/LZ6VxZR.png"
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html",
    "href": "posts/2020-07-15-ml-forget.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "In relation to learning new tasks, the phenomenon of a model forgetting how to perform a previously learned task when trained on a new task is called catastrophic forgetting (CF). Catastrophic forgetting can be at fault in online training of similar tasks as well. Ideally for a deep enough network to learn all tasks it has to be presented with all the training data at once. This is often not quite possible because of reasons like memory constraints, security, non-sustainable solution and serious limitations in the online learning environment. This is especially important in reinforcement learning because catastrophic inference is most visible in sequential online tasks. RL methods we use by definition is a sequential online learning algorithm. Here online means the agent has to adapt to the environment in real time."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#simple-methods-to-try-first",
    "href": "posts/2020-07-15-ml-forget.html#simple-methods-to-try-first",
    "title": "Technical ramblings",
    "section": "Simple methods to try first:",
    "text": "Simple methods to try first:\n\nRegularize with dropout and maxout\nStore weights for each environment\nTake average of these weights and initialize when learning a new environment\nAdd adaptive learning rate for weights, slow learning rate for weights for common skills for each environment and fast learning rate for new environments"
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#ideas-to-further-investigate",
    "href": "posts/2020-07-15-ml-forget.html#ideas-to-further-investigate",
    "title": "Technical ramblings",
    "section": "Ideas to further investigate:",
    "text": "Ideas to further investigate:\n\nPathNet - For learning similar tasks\nElastic Weight Consolidation - Good for dissimilar tasks\nH-DRLN - Hierarchal Deep Life Long RL – - A framework based that combines DQN and lifelong learning techniques\n\nBefore we dive in, lets see why does a neural network ever have ‘Catastrphic Forgetting’ problem?\nCatastrophic Forgetting is not only limited to neural nets; if that was the case, we could have easily replaced it some other learning algorithm. Researchers who studied this problem were of the opinion that the underlying cause is the generalization that a neural net does. The ability of a neural net to distribute and share representation of the relationship between input and an output label helps it to generalize, which is one of the most important properties of a neural net. Thus, each time a new data point (x, y) comes up to be trained, the neural net tries to distribute and share the representation by adjusting the weights of the neural net. This leads to the neural net forgetting earlier representations.\nWhat are some of the possible solutions?\nMany methods that detailed below can be explained off as a particular type of regularization. The techniques given below can be considered as a bag of tricks that could be used to limit catastrophic forgetting. As mentioned above, most of these solutions can be explained as a type of regularization and freezing of learned structures. To remember everything is to not change any of the weights or the activation traces of a task. Some methods directly tackle this by freezing weights, others by freezing activation traces of the entire network from input to output."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#regularization-dropout-and-activation-functions",
    "href": "posts/2020-07-15-ml-forget.html#regularization-dropout-and-activation-functions",
    "title": "Technical ramblings",
    "section": "Regularization, dropout and activation functions",
    "text": "Regularization, dropout and activation functions\n\nRegularization\nJoint Many-Task model - Successive regularization at each epoch\nDropout with Maxout\n\nRegularization is traditionally used to prevent overfitting; this can be done in many ways. A standard way to do this is to add a norm component to the loss function that is proportional to the square of the weights of a neural network (L2 norm). L2 regularization has the effect of the network preferring to learn smaller weights; as more significant the weight, larger would be the loss value. How this prevents forgetting can be attributed to the fact that, when the weights are small (regularized), small input values will not affect the loss function drastically. This, in turn means, the gradient of the loss would be a smaller value, thus when the weights which are adjusted based on this gradient value will not change much.\nDropout is used as a regularizing technique. Here how it regularizes can be explained by considering dropout as training a set of neural networks (each time dropping out a set of hidden units), then averaging the result of an ensemble of nets at the end [dropout]. This makes the model robust to losses by not assuming that a set of neurons (thus information) will always be present. Thus, when a new task is learned, the combined effect of a robust and regularized network can to some degree minimize forgetting.\nDropout has been empirically proven to help in adapting to new task while remembering old tasks. It has been suggested to use maxout as the activation function when using dropout as a technique to minimize forgetting."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#weight-freezingslow-updating",
    "href": "posts/2020-07-15-ml-forget.html#weight-freezingslow-updating",
    "title": "Technical ramblings",
    "section": "Weight freezing/slow updating",
    "text": "Weight freezing/slow updating\n\nUsing fast weights to de-blur old memories - Hinton\nElastic Weight Consolidation - Deepmind\n\nHere the idea is to not frequently update parameters of a Neural Net when learning new tasks. The larger the activation weights between two nodes for a task, the less chance that these weights should be affected. Hinton worked on this problem in the 80s and designed a network with weights of different rate of plasticity [Fast weights]. A similar approach was chosen by Kirkpatrick and the Deepmind team with the Elastic Weight Consolidation (EWC) technique. Here a constraint is added to the loss function that controls which weights can be updated and which cannot. When a new task is being learned, strong activation paths are not updated, and weights that didn’t contribute much to a previous task is updated first.\nThe insight that led to EWC is that, in a deep neural network, there can be many different configurations of weights that will give us a similar rate of error for a task. The goal with EWC is to find a set of weights from the parameter space that has low error rate for both the new task and the old one. In this approach the authors consider gradient descent in a Bayesian perspective. Whereas stochastic gradient descent tries to estimate a single parameter (weight), they tried to estimate a parameter for the entire distribution of data (Bayesian estimate), now this is not tractable, so what they do is that they use a trick called Laplace approximation and they call their approach EWC. By learning a distribution of parameters for each task, they were able to sample a set of parameters (weights) that worked for both tasks. [EWC]."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#ensemble-methods",
    "href": "posts/2020-07-15-ml-forget.html#ensemble-methods",
    "title": "Technical ramblings",
    "section": "Ensemble methods",
    "text": "Ensemble methods\nProgressive Neural Networks - Perfect memory PathNet - Deepmind\nEnsemble methods attempt to train multiple networks and combine them to get the result, essentially training separate classifiers for each new task.\nProgressive neural networks progressively extend its structure in proportion to new tasks. It starts with an initial neural net and to learn a new task; new lateral connections are formed in parallel to the initial one. This helps in few ways, first when a new task has to be learned the previous network weights are frozen thus eliminating catastrophic forgetting and as lateral connections between each layer are formed the new network shares knowledge from the previous network. The only disadvantage with a progressive network is with the growth of new tasks, the network weights also grow.\nPathNet is a network of networks. It has a fixed set of networks in which each layer has a collection of parallel mini neural network modules. Optimal paths are discovered using a genetic algorithm or reinforcement learning from the fixed size neural network for each task. After training and identification of a path, they are frozen. Thus the network does not forget. The cool thing about PathNet is that the base network is fixed and it does not grow further than the initial architecture. It is possible for the learned representations to be reused as well. PathNet is considered the best model for this type of learning."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#memory-methods-rehearsal",
    "href": "posts/2020-07-15-ml-forget.html#memory-methods-rehearsal",
    "title": "Technical ramblings",
    "section": "Memory Methods – Rehearsal",
    "text": "Memory Methods – Rehearsal\n\nEpisodic Memory Approach\nEpisodic Generative Approach\nDual memory models\nGradient Episodic Memory\n\nThese involve both static memory and generative memory methods. Static memory methods involve using a big experience replay type of memory to store previous session data, and during training of each new task, data from memory is randomly sampled and mixed with the new training data. The generative model can be a type of auto-encoder or a generative adversarial network (GAN), where the statistics of the data is stored and replayed while training for new tasks.\nDual memory models use a system of short-term memory (STM) modules and a long-term memory (LTM) module. Both of the memories have a combination of a generator and a learner. The STM which is a collection of task-specific networks, also, has a hash-table that keeps a tab on how many tasks the agent has learned and indexes each of the task-specific networks. The LTM uses a collection of the raw data sample from all the previous tasks to train its generative memory. The STM is used for fast training, and LTM is used to reinforce the STM.\nIn Gradient Episodic Memory, when learning in a new environment, weight update depends on a meeting a metric. Here for each task, a sample of the states (X-y) are stored in memory. For each update, the gradient is compared with the gradient of all the previous tasks, if the gradient of the current update does not contribute to the overall gradients of all the previous tasks, then an approximation that does contribute is chosen as the update. This reduces drastic changes to weights when learning new tasks, hence prevents catastrophic forgetting.\nNow let me give some details on approaches specific to reinforcement learning."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#life-long-reinforcement-learning",
    "href": "posts/2020-07-15-ml-forget.html#life-long-reinforcement-learning",
    "title": "Technical ramblings",
    "section": "Life Long Reinforcement Learning",
    "text": "Life Long Reinforcement Learning\nA lifelong reinforcement learning agent should have 2 main abilities: Efficiently retain knowledge base of learned policy in an environment (highly regularized sparse data structure). Efficient transfer - Should have the ability to transfer knowledge from the previous environment to the new one.\nIn life-long reinforcement learning, instead of learning each task, the objective is for the agent to distill learning from different environments, use shared knowledge and learn environment specific information as well. In the following section task and environment mean the same thing."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#life-long-rl-through-multiple-environments",
    "href": "posts/2020-07-15-ml-forget.html#life-long-rl-through-multiple-environments",
    "title": "Technical ramblings",
    "section": "Life Long RL through multiple environments",
    "text": "Life Long RL through multiple environments\nThese learning methods are based on introducing bias into learning. Here, bias refers to knowledge about the environment. So a lifelong learning agent is initialized with something called the initial bias which is the weights of the previous environment and then updated with learning bias for the new environment. Now, this is in some ways similar to what our DQN agent is doing where we initialize the net with the weights trained from the initial simulator and then use this to bootstrap for the real environment. This can be extended, and initial bias could be the average of weights for all the previous environments. Also, adaptive learning rates are used for weights. The weights are updated in proportion to the amount by which they varied for the previous environments. If they did not vary by a threshold value, the learning rate for those weights are set to be very low, and if the weights beyond a threshold, then that means that these weights are environment dependent, thus the learning rate for these weights for the new environment is set to a higher one.\nMost of the recent lifelong learning schemes used in RL is based on an algorithm known as ELLA (Efficient Lifelong Learning Algorithm). The principle behind ELLA is that we assume each task is sequentially submitted to the agent, and each learnable parameter is considered as a linear combination of a latent basis component (a component common to all tasks) and a task-specific component. Thus, the objective would be to minimize predictive loss over all tasks while encouraging significance of the latent shared component. This is done by introducing a sort of inductive bias (inductive of shared task information) to the loss function."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#policy-gradient---ella",
    "href": "posts/2020-07-15-ml-forget.html#policy-gradient---ella",
    "title": "Technical ramblings",
    "section": "Policy Gradient - ELLA",
    "text": "Policy Gradient - ELLA\nThe goal of policy gradient using efficient lifelong learning algorithm is to find optimum policies parameterized by a set of weights for each environment. We use the same approach as in general ELLA where the parameters (weights) for each task is considered to be a linear combination of parameters of the common latent model between the tasks and task-specific weights. Here the latent structure is stored in memory for future learning, and the PG algorithm in each of its iteration uses this knowledge base to update both latent weights and task-specific weights."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#hierarchical-deep-reinforcement-lifelong-learning-network-h-drln",
    "href": "posts/2020-07-15-ml-forget.html#hierarchical-deep-reinforcement-lifelong-learning-network-h-drln",
    "title": "Technical ramblings",
    "section": "Hierarchical Deep Reinforcement Lifelong Learning Network (H-DRLN)",
    "text": "Hierarchical Deep Reinforcement Lifelong Learning Network (H-DRLN)\nTo solve complex tasks, rather than just knowing what action to take in a state, an agent has to learn skills; these would include things like picking up an object, moving from a point to another point, etc. Reinforcement learning was extended using the options framework to do exactly this. [options framework]. To use reusable skills in a lifelong manner, an algorithm should be able to learn a skill (Eg: how to move left or right); enable an agent to determine which of these skills should be used/reused and cache reusable skills.\nH-DRLN has something called a Deep Skill Network which stores independent DQNs that have special skills stored, for example, there could be 2 DQNs as in our case one for passive control and another for active control. Along with the Deep Skill Network, the agent has a generic output layer that outputs either simple actions or skills from the Deep Skill Network depending upon the state sampled. This agent can be considered as a DQN with actions being temporally extended to solve tasks. For training the agent, a modification is made to the experience reply, and a new memory called Skill Experience Reply (S-ER) is used.\nWith this, we come to the end of the survey on Catastrophic Forgetting and lifelong learning approaches. A few other methods like explicit sparse coding, policy distillation, and curriculum learning have not been mentioned here as many of the techniques discussed could be considered as variations of these algorithms."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#terms",
    "href": "posts/2020-07-15-ml-forget.html#terms",
    "title": "Technical ramblings",
    "section": "Terms",
    "text": "Terms\nPlasticity - The propensity of a weight to be affected by a change. A weight with high plasticity can be modified easily than a weight with lower plasticity. Auto-encoder - A type of neural networks used to learn representation of data, mainly for dimensionality reduction GAN - GAN or Generative Adversarial Networks uses a competing pair of networks to learn the representation of data and can be used to generate data points from the learned distribution."
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#references",
    "href": "posts/2020-07-15-ml-forget.html#references",
    "title": "Technical ramblings",
    "section": "References",
    "text": "References\n[Dropout] : https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf [Empirical study] : https://arxiv.org/pdf/1312.6211.pdf [Fast weights]: https://www.cs.bham.ac.uk/~jxb/PUBS/COGSCI05.pdf [EWC]: https://arxiv.org/pdf/1612.00796.pdf [Lifelong ML]: https://www.cs.uic.edu/~liub/lifelong-machine-learning-draft.pdf [Options framework]: http://www-anw.cs.umass.edu/~barto/courses/cs687/Sutton-Precup-Singh-AIJ99.pdf [Multi task learning PG]: http://proceedings.mlr.press/v32/ammar14.pdf [Gradient Episodic Memory]: https://arxiv.org/pdf/1706.08840.pdf [PathNet]: https://arxiv.org/pdf/1701.08734.pdf [Deep Generative Memory]: https://arxiv.org/pdf/1710.10368.pdf [Progressive Neural Net]: 1606.04671 Progressive Neural Networks"
  },
  {
    "objectID": "posts/2020-07-15-ml-forget.html#the-end",
    "href": "posts/2020-07-15-ml-forget.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nThats all for now folks on Catastrophic Forgetting in Learning systems using connectionist methods.\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2017-03-28-borg.html",
    "href": "posts/2017-03-28-borg.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "So I was reading up on some Python and found this interesting pattern by Alex Martelli. It’s call the Borg pattern, which I think is very cool and a bit philosphical.\nclass Borg(object):\n    _shared_data = {}\n    def __init__(self):\n        self.__dict__ = self._shared_data\n\nclass Singleton(Borg):\n    def __init__(self, v):\n        super(Singleton, self).__init__()\n        self.value = v\n    def __str__(self):\n        return str(self.value)\n\n# lets create some instances\n>> one = Singelton(\"first\")\n>> print(one)\n>> 'first'\n>> two = Singelton(\"second\")\n>> print(two)\n>> 'second'\n>> print(one)\n>> 'second'  # wow! thats cool ryt?!\n\n\nAll Singleton objects have the same share the same state - it is made possbile by the all powerful __dict__ attribute. It is a dictionary that contains all attributes and its values. We first assign __dict__ to refer to the dictionary refered to by _shared_data. This makes sure that every time an object for Singelton is created the __dict__ attribute of the Borg object is referring to the same instance variable _shared_data. Thus, when inheriting the class, the child class’s __dict__ is overwritten by parent class’s __dict__.\n\n\n\nAs a closing note, about the philosphy of Borg, think about why we need a singleton? At least one of the reasons why we need one is to maintain a common state among different objects (some would say, that is the only reason). Borg does that for us, one state for all objects, for we are the Borg, we are all the same:)\nThe Borg pattern is so cool because it is so simple."
  },
  {
    "objectID": "posts/2020-03-09-cargo.html",
    "href": "posts/2020-03-09-cargo.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Cargo is the one ring to rule them all, the one build tool that:\n\nbuilds our rust project\ndownloads dependencies\nbuild dependencies\nhelp release optimized code\n\nLet’s see how cargo helps us in packaging and releasing our code, get it? cargo.. packaging… ah ohkay, not the right time for terrible jokes I guess..\n\n\nCargo should be installed as part of rust installation, to ensure cargo is installed run\n> cargo --version\ncargo 1.41.0\nyou should be able to see something like above… to create a wireframe for a new project hworld run:\n> cargo new hworld\nThis creates a directory hworld, with few files and one directory:\n>\n|-- Cargo.toml\n|-- src\n    |-- main.rs\nCargo goes one step ahead and adds versioning to the project, git is used by default, so if you do a ls -la you can see few more files, but for now let’s ignore that and focus on the main files.\n\nCargo.toml - This is called the manifest file of our Rust project and is the file where we configure stuff like name, version of the project, Rust edition, the, author's, email address, and also there is a section to include the dependencies required by the project.\n\nA sample Cargo.toml looks like:\n[package]\nname = \"hworld\"\nversion = \"0.1.0\"\nauthors = [\"rahulunair <rahulunair@gmail.com>\"]\nedition = \"2018\"\n\n[dependencies]\ntime = \"0.1.12\"\nThere are a lot more options that can be included in the manifest file, for an extensive list, see. Here I have specified one dependency time with version 0.1.12, we wont use it for anything now, but just for kicks eh?\n\n./src/main.rs - It is a placeholder to add our code, cargo has populated it with:\n\nfn main() {\n    println!(\"Hello, world!\");\n}\n\n\n\nIt’s just as simple as running the below command from inside hworld directory:\n> cargo build\nIf you are running it for the first time, you would see something like:\nUpdating crates.io index\n  Downloaded time v0.1.42\n  Downloaded libc v0.2.67\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 49.31s\nThat’s it, we have built our first program using cargo.\n\n\n\nAfter cargo has built the project successfully, it creates a directory ./target/debug/ to save the binary, we can just cd to ./target/debug and run:\n> ./hworld\nHello, world!\nYay! we have run our first Rust program, that was created using Cargo.\n\n\n\nRelease version ideally should be an optimized binary, to obtain such a binary, it is as simple as running:\n> cargo build --release\nRunning the above command on my system from hworld directory, Cargo prints:\n   Compiling libc v0.2.67\n   Compiling time v0.1.42\n   Compiling hworld v0.1.0 (hworld)\n    Finished release [optimized] target(s) in 8.42s\nAnd, lo and behold there is an optimized binary hworld in hworld/target/release/, in later posts, we can see the difference between debug and release versions of binaries generated by Cargo.\nThe eligance of Cargo is that, everything is named as we would expect, no gotcha’s .. I really like that about Cargo.\n\n\n\nAs a closing note, let’s recap Cargo mainly does two things for us,\n\nbuilds our project using - cargo build or cargo build –release\nbuilds and runs our project using - cargo run\n\nNow, there is a third and final command I use, to check for compilation issues fast, without really building the binary, which can be slow at times.\nThe command to check if everything is alright is:\n> cargo check\nOn my system it gives me this:\n    Checking libc v0.2.67\n    Checking time v0.1.42\n    Checking hworld v0.1.0 (hworld)\n    Finished dev [unoptimized + debuginfo] target(s) in 3.61s\nThat’s it, so we saw what cargo can do, it is a tool that can be used when we are building a Rust project, but if it is a simple file, with no dependencies we can still use rustc as we saw earlier in Flashcard Rust."
  },
  {
    "objectID": "posts/2017-03-31-juggling.html",
    "href": "posts/2017-03-31-juggling.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Today let’s do some basic monkey work, rather lets do some juggling. I am always fascinated by people being able to juggle multiple balls. It’s something amazing, at least to me. They even have a World Juggling day, can you believe it?! So, why are we talking about monkeys, juggling and what not… Well, to me golang goroutines is sort of juggling. Where the juggler is able to do one thing at one time and, is able to keep track of multiple balls or whatever is being juggled. Well, you might have a different opinion and you are right, if the opinion is something like we can truely do tasks parallely if there are multiple CPUs, akin to two monkeys handling 2 seperate balls. hmm… yeah, so this might be the most over simplified and in some ways incorrect expalnation of concurrency and parallelism. For people who are curious to know the difference, there are awesome videos out there, particularly one comes to mind, the one Rob Pike gave. Check it out, you might like it.\nI initially thought of writing about different ways in which we can do concurrent jobs in both golang and python. Well, now that I am thinking let me give an example of how to use go routines.\nGo routines are basically light threads handled by the go scheduler to do concurrent jobs. There are primitives such as mutexes, synchonozation etc. to prevent conditions like data lock, unwanted write, multiple threads trying to read and write a resource at the sametime etc. more on that later.\n\n\nSo let’s see how to find the value of e in three different ways. I think if you ask me what is your favorite number, I might say it is e, never ending, rather transcendental constant with value 2.7182818… . Funny thing is that this number comes up in many places across the universe, it was made famous by Bernoulli when he came across it, while studying compound interest..go figure out..>>\n\n\n\nThere are lot of ways to find e, we can use the Binomial expansion, Newton’s method, Brother’s method (formualated in 2004 I think) etc.\npackage main\n\n    import (\n        \"fmt\"\n        \"math\"\n        \"sync\"\n    )\n\n    // dumbFact is a recursive function to print factorial\n    func dumbFact(num float64) float64 {\n        if num <= 1 {\n            return 1\n        }\n        return num * dumbFact(num-1)\n    }\n\n    // Brother method to find e using approximation\n    // for n=0 to limit\n    //    (2n+2)/(2n+1)!\n    func Brother(limit float64, wg *sync.WaitGroup) {\n        var n float64\n        var e float64\n        // `defer` keyword will run the expression after the\n        //function returns\n        defer wg.Done()\n        for n <= limit {\n            e = e + (2*n+2)/dumbFact(2*n+1)\n            n = n + 1\n        }\n        fmt.Printf(\"Brother's method for `e` yields: %20.15f\\n\", e)\n    }\n\n    // Binomial method to find `e`\n    func Binomial(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        e := math.Pow((1 + 1/limit), limit)\n        fmt.Printf(\"Binomial solution to `e` is : %20.15f\\n\", e)\n    }\n\n    // Newton method - approximation when n is big\n    func Newton(limit float64, wg *sync.WaitGroup) {\n        defer wg.Done()\n        var n float64\n        var e float64\n        for n <= limit {\n            e = e + 1/dumbFact(n)\n            n = n + 1\n        }\n        fmt.Printf(\"Newton's method gives the value as: %20.15f\\n\", e)\n    }\n\n    func main() {\n        fmt.Println(\"A simple go routine example\")\n        // A waitgroup is just used to wait until the\n        //routine completes\n        var wg sync.WaitGroup\n        wg.Add(3)\n        go Brother(10000, &wg)\n        go Binomial(10000, &wg)\n        go Newton(10000, &wg)\n        wg.Wait()\n        fmt.Println(\"Done!\")\n    }\nYou can see the program in action here.\n\n\n\nThe juggling happens in lines 58, 59 and 60 of the program. Where, the scheduler will concurrently work on the three routines, there is nothing complicated in the program, it uses the basic factorial function, uses something called the WaitGroup and runs three go routines.\nSo why am I using a WaitGroup, you ask. Well, WaitGroup is just to prevent go from falling through and terminating the programming before the routines are done. It tells go to wait till all the routines have finished running. We could waited for a simple input from the user at the end after line 61 and made the program explicitly wait, which would have given the routines time to run as well. I arbitrarily chose this WaitGroup option.\n\n\n\nEssentially, go routines are simple at first, and that is important. As in any concurrent programming paradigms, there are complications and real world pains, but the entry level bar is low, which is in my opinion neat. This helps one learn the concepts of concurrency eventhough you might not use Go. Next time we shall see how to do something similar to this in python, or will we ?"
  },
  {
    "objectID": "posts/2020-05-23-ray.html",
    "href": "posts/2020-05-23-ray.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "The ray library for distributed computing has been around for a while. It was a few years back when I first noticed it. This was when I had started doing some Deep Reinforcement Learning at work and needed to do distributed policy explorations. Since then, the library has expanded it’s role in many ways and is a true distributed framework to scale compute intensive workloads. Today I want to try and use it for a non machine learning workload and see how different it would be from using thee multiprocessing library that comes part of the Python standard library.\nI was doing some webscraping and parsing and needed to parallelize some part of my code, specifically I wanted to run 2 functions parallely."
  },
  {
    "objectID": "posts/2020-05-23-ray.html#the-end",
    "href": "posts/2020-05-23-ray.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nFor distributed Machine Learning training I used Horovod exlusively, the one thing with Horovod is that it needs MPI and at times, it’s a pain to debug MPI, I would like to tryout Ray and see if it is easier to use in place of Horovod. My initial thoughts are all positive and it looks really great.\n\n\n\nDickinson-Lorenz Typesetting and Distributing Machines"
  },
  {
    "objectID": "posts/2017-01-26-crypto-checklist.html",
    "href": "posts/2017-01-26-crypto-checklist.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Footnotes\n\n\nhttps://bit.ly/2W0dZum↩︎\nhttps://bit.ly/2W0dZum↩︎"
  },
  {
    "objectID": "posts/2021-03-22-rust-type.html",
    "href": "posts/2021-03-22-rust-type.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "This is going to be a very short post on a tiny function that can help us in figuring out what is the type of a variable.\nThis is helpful to me personally when learning rust and after some hours of coding, I want to know what is the type of a variable quickly, which is not easy always.I need to thank @timClicks for showing this in one of his streams on youtube. If you are learning Rust and is looking for a book, let me please recommend Rust in Action by Tim, it is an excellent in-depth introduction to Rust.\n\n\nuse std::any::type_name;\n\n// what is the type of the passed argument\nfn what_type<T>(_: &T) {\n    println!(\"type is: {}\", type_name::<T>());\n\n}\ntype_name function from the std crate returns the name of a type as a string slice.\nBefore knowing about this function available from the std crate(reminder to self, to read the standard library docs!), i tended to use the ide introspection features to confirm what the type of a variable was at times or use a () as type and the compiler complains to me that the type is something else, which is what i wanted to known in the first place, but know I have a shiny new function that I can use, lets see how it works.\n\n\n\nLet’s take a simple example,\nfn main() {\n    let number = 3232;\n    let name = \"Rahul\".to_string();\n    let list_of_nums = vec![1, 2, ];\n    what_type(&number);\n    what_type(&name);\n    what_type(&list_of_nums);\n}\nThat is it, and it will print the type of the variable passed into the what_type function as below:\ntype is: i32\ntype is: alloc::string::String\ntype is: alloc::vec::Vec<i32>\n\n\n\ntype_name\n\n\n\nLike I said a short post about a really useful(hopefully) function in Rust."
  },
  {
    "objectID": "posts/2017-02-12-prf-and-macs.html",
    "href": "posts/2017-02-12-prf-and-macs.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Footnotes\n\n\nhttp://crypto.stackexchange.com/questions/31343/what-is-the-purpose-of-pseudorandom-function-families-prfs↩︎\nhttp://crypto.stackexchange.com/questions/9336/is-hmac-md5-considered-secure-for-authenticating-encrypted-data↩︎\nhttp://security.stackexchange.com/questions/33123/hotp-with-as-hmac-hashing-algoritme-a-hash-from-the-sha-2-family↩︎"
  },
  {
    "objectID": "posts/2016-01-03-graduated.html",
    "href": "posts/2016-01-03-graduated.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Finally, after two years of going to school, late-night working on projects when dinner meant calling Dominos at 2:00 AM, I have graduated with a Masters in Computer Engineering. The final semester had been very kind to me; I had a lot of opportunities to work with some cool folks on different aspects of the cloud and Machine Learning.\nTill a month back I had deadlines and submissions, now I am relatively free, I know I should enjoy this moment but it is really hard to focus when you don’t have anything much to do."
  },
  {
    "objectID": "posts/2020-03-16-vars.html",
    "href": "posts/2020-03-16-vars.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "In this post, let’s look at how variables are considered in Rust.\n\n\nBy default all variables in Rust are immutable, what that means is, once we assign a value to a variable, we cannot reassign another value. The keyword let is used to create variables and bind values to them in Rust.\nfn main() {\n  let name = \"unrahul\";\n  println!(\"name is {}\", name);\nOutput of the above program is:\nname is unrahul\nThere you go, what we have done is; we have created a string literal name in Rust of type &str. Now, if we try to reassign the same variable name in the same scope as main:\nfn main() {\n    let name = \"unrahul\";    // created a variable `name` and assigned a value to it\n    println!(\"name is {}\", name);\n    name = \"rahul\";    // trying to assign a new value to the variable `name`  \n    println!(\"new name is {}\", name);\n}\nThe compiler will throw an error:\nerror[E0384]: cannot assign twice to immutable variable `name`\n --> src/main.rs:4:5\n  |\n2 |     let name = \"unrahul\";\n  |         ----\n  |         |\n  |         first assignment to `name`\n  |         help: make this binding mutable: `mut name`\n3 |     println!(\"name is {}\", name);\n4 |     name = \"rahul\";\n  |     ^^^^^^^^^^^^^^ cannot assign twice to immutable variable\n\nerror: aborting due to previous error\nFor more information about this error, try `rustc --explain E0384`.\nerror: could not compile `playground`.\n{% include info.html text=“try rustc –explain E0384 to see what the error is all about” %}\nThis is freakin’ cool, now that variables cannot be reassigned, in a program, we do not have to worry if the types are going to change or if the state of a variable will change unintentionally.\n\n\n\nIn some situtations, we want variables to be reassigned like in a loop, or ehmm if we want to change our name to a new one.., for those situations, in Rust we can do:\nfn main() {\n    let mut name = \"unrahul\";    // just change the varible to a `mutable` variable\n    println!(\"name is {}\", name);\n    name = \"rahul\";\n    println!(\"new name is {}\", name);\n}\nAn the output is:\nname is unrahul\nnew name is rahul\n\n\n\nShadowing in simple terms mean, a variable already declared can be redeclared with in a inner block with the same name.\nFor example, we can do this in Rust:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    {\n        let name = \"rahul\";    // here the variable is in inner scope (see the braces?), and this is shadowing the external declartion of `name`\n        println!(\"new name is {}\", name);\n    }\n}\nCompiling and running this, we get:\nname is unrahul\nnew name is rahul\nOkay, I lied a bit, shadows need not be in inner or seperate block, this below is perfectly valid:\nfn main() {\n    let name = \"unrahul\";\n    println!(\"name is {}\", name);\n    let name = \"rahul\";    // This is also considered as shadowing\n    println!(\"new name is {}\", name);\n}\nAnd the output of this is:\nname is unrahul\nnew name is rahul\n\n\n\nNow that we have seen immutable and mutable variables, and also shadowing, may be primitive types next?.."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html",
    "href": "posts/2020-07-14-learning-methods.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Model free learning can be done using variations of temporal difference learning or Monte Carlo methods."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#temporal-difference-or-td-learning",
    "href": "posts/2020-07-14-learning-methods.html#temporal-difference-or-td-learning",
    "title": "Technical ramblings",
    "section": "Temporal Difference or TD Learning",
    "text": "Temporal Difference or TD Learning\nFrom each step learn something that would make enable us to improve the estimated value for the next step. Consider this, three scenarios, in which the third scenario depends on the second and or the first. If that is the case, then knowing the states in the scenario can help us in better predicting the states in scenario three. We can improve the prediction in the third scenario if there is any change in states for either of the other scenarios, rather than waiting for the third one to finish and then realizing our prediction was close or way off. Consider, you are going somewhere and you expect to go through 2 cities. You estimate that you would reach the destination in 3 hours, as you know or estimate that you will need 1 hour each to cover the 2 other cities. Now, if it’s your lucky day and traffic is low in the first city, thus you could pass through it in 30 minutes instead of 60. Thus, you can estimate that you will reach the final destination 30 minutes early. While passing through the second city, you have car trouble and it takes an hour to fix it and start again. Thus, you now predict you would reach 30 minutes past the estimated time at your destination (Provided, you don’t face any further uncertainties). This continuous improvement of estimate is the main principle behind temporal difference or TD learning. We can say, TD learning is an on-line learning (as we don’t need to wait for the entire episode to finish before updating our estimates). It bootstraps on the estimated value of other states to estimate value of the state in concern.\nTypes : TD(0) and TD(lambda)"
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#q-learning",
    "href": "posts/2020-07-14-learning-methods.html#q-learning",
    "title": "Technical ramblings",
    "section": "Q learning",
    "text": "Q learning\nIt is a variation of TD(0) learning, where we incrementally estimate the Q value for a state based on immediate rewards and the Q value for the next state. The variation is that, to estimate the Q value for the next state, we add the immediate reward with the Q value for the next state that maximizes the value (Q value for the state for the action that gives the maximum value). Also, unlike TD(0) learning, Q learning is an off-policy learning algorithm. Thus the estimated Q value at instance k is, the Q value at k for the state and action at time t plus the difference between estimated Q value using the immediate reward and discounted Q value for the next state for the action that gives the maximum value and the Q value of the current state. This delta between estimated Q value for the next state and Q value for the current state is weighted by a factor called the learning rate, which is between [0, 1]. The weighting factor or learning rate alpha can be decreased based on each iteration or as in many scenarios, use a small fixed value. The rate basically determines by how much we update the Q value. Now, if that sounds complex, trust me, it’s not, I am just not that good at explaining I guess. Just search for the algorithm on line and you will get it instantly.\nHow is it done?\nWell, basically, the agent at time, t, in state, s, does an action, a, and moves to the next state and thus receives a reward, r. Now at time, t+1, the agent knows that it is in state, s, and knows the reward it obtained from the previous state. It uses this information along with the Q value for the optimum action for this state to get a better estimate of the Q value of the state, s. For this update to work, we need to have some starting point of Q values, in practice this is assumed to be zero or set randomly and at each iteration, k, the Q value is updated a little bit based on the learning rate to be a little closer to reality. In time, after many iterations of learning, the Q values for each states will reflect the real values that can be obtained. It has been proved (don’t ask me how, I haven’t checked out the proof) that if we do this iterative update an infinite time, we will eventually get the right Q values, irrespective on the initial Q values, the actions we took in each state etc."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#sarsa-learning",
    "href": "posts/2020-07-14-learning-methods.html#sarsa-learning",
    "title": "Technical ramblings",
    "section": "SARSA learning",
    "text": "SARSA learning\nFirst of all, what a creative naming, it must have taken them a long time to come up with this name. So, why is it called SARSA?, It’s because, the learning algorithm uses the present state, S, the action taken, A, the reward obtained, R, the next state, S, and the action taken, A, in this next state, while following a policy. SARSA stands for State-Action-Reward-State-Action!. While in traditional Q value function, the objective was to estimate the optimum policy doing exploration of a random policy, in SARSA, we start with a policy and tries to estimate the Q value of starting at a state, doing an action in state and following a policy, that is not changed in the course of learning. Thus it is an on-policy learning algorithm, as we don’t change the policy that has been chosen for a particular iteration. The idea is that if we are able to try all the states and all the possible actions infinitely many times, then this will eventually converge to the optimum policy itself. For a particular iteration, it computationally less demanding that Q learning, but overall, it may need more time to converge. This learning is used when the state transition probabilities might not be fixed, there can be changes in the probabilities of switching from one state to the other. The only change in the algorithm compared to Q learning is, it doesn’t take the action that maximizes the utility when calculating the next state value, but obeys by the policy and takes the stipulated action. Here, too if you don’t understand it, please check the algorithm once on line and it will be clear."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#actor-critic-algorithms",
    "href": "posts/2020-07-14-learning-methods.html#actor-critic-algorithms",
    "title": "Technical ramblings",
    "section": "Actor Critic Algorithms",
    "text": "Actor Critic Algorithms\nThis class of learning algorithms has two parts, the Actor, which is a policy function and the Critic, which is a value function that is used to obtain the value for a state transition following an action. After an action has been selected, the Critic estimates the value for the state and calculates value for the state (immediate reward plus value for the next state). This is used to evaluate the action taken, which is the difference between the calculated value (using Bellman equation) and the estimated value. This difference or delta is used to improve the probability for the action in that state. The improvement factor delta is weighted by a learning rate, beta. A variation of this algorithm called the A3C algorithm is one of the fastest RL algorithms out there."
  },
  {
    "objectID": "posts/2020-07-14-learning-methods.html#the-end",
    "href": "posts/2020-07-14-learning-methods.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFall of the Flying Man, London."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html",
    "href": "posts/2020-07-09-learning-methods.html",
    "title": "Technical ramblings",
    "section": "",
    "text": "Model free learning can be done using variations of temporal difference learning or Monte Carlo methods."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#temporal-difference-or-td-learning",
    "href": "posts/2020-07-09-learning-methods.html#temporal-difference-or-td-learning",
    "title": "Technical ramblings",
    "section": "Temporal Difference or TD Learning",
    "text": "Temporal Difference or TD Learning\nFrom each step learn something that would enable us to improve the estimated value for the next step. Consider this, three scenarios, in which the third scenario depends on the second and or the first. If that is the case, then knowing the states in the scenario can help us in better predicting the states in scenario three. We can improve the prediction in the third scenario if there is any change in states for either of the other scenarios, rather than waiting for the third one to finish and then realizing our prediction was close or way off.\nConsider, you are going somewhere and you expect to go through 2 cities. You estimate that you would reach the destination in 3 hours, as you know or estimate that you will need 1 hour each to cover the 2 other cities. Now, if it’s your lucky day and traffic is low in the first city, you could pass through it in 30 minutes instead of 60. Thus, you can estimate that you will reach the final destination 30 minutes early. While passing through the second city, you have car trouble and it takes an hour to fix it and start again. Thus, you now predict you would reach 30 minutes past the estimated time at your destination (Provided, you don’t face any further uncertainties). This continuous improvement of estimate is the main principle behind temporal difference or TD learning.\nWe can say, TD learning is an on-line learning (as we don’t need to wait for the entire episode to finish before updating our estimates). It bootstraps on the estimated value of other states to estimate value of the state in concern.\nTypes : TD(0) and TD(lambda)"
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#q-learning",
    "href": "posts/2020-07-09-learning-methods.html#q-learning",
    "title": "Technical ramblings",
    "section": "Q learning",
    "text": "Q learning\nIt is a variation of TD(0) learning, where we incrementally estimate the Q value for a state based on immediate rewards and the Q value for the next state. The variation is that, to estimate the Q value for the next state, we add the immediate reward with the Q value for the next state that maximizes the value (Q value for the state for the action that gives the maximum value). Also, unlike TD(0) learning, Q learning is an off-policy learning algorithm. Thus the estimated Q value at instance k is, the Q value at k for the state and action at time t plus the difference between estimated Q value using the immediate reward and discounted Q value for the next state for the action that gives the maximum value and the Q value of the current state. This delta between estimated Q value for the next state and Q value for the current state is weighted by a factor called the learning rate, which is between [0, 1]. The weighting factor or learning rate alpha can be decreased based on each iteration or as in many scenarios, use a small fixed value. The rate basically determines by how much we update the Q value. Now, if that sounds complex, trust me, it’s not, I am just not that good at explaining I guess. Just search for the algorithm online and you will get it instantly.\nHow is it done?\nWell, basically, the agent at time, t, in state, s, does an action, a, and moves to the next state and thus receives a reward, r. Now at time, t+1, the agent knows that it is in state, s, and knows the reward it obtained from the previous state. It uses this information along with the Q value for the optimum action for this state to get a better estimate of the Q value of the state, s. For this update to work, we need to have some starting point of Q values, in practice this is assumed to be zero or set randomly and at each iteration, k, the Q value is updated a little bit based on the learning rate to be a little closer to reality. In time, after many iterations of learning, the Q values for each states will reflect the real values that can be obtained. It has been proved (don’t ask me how, I haven’t checked out the proof) that if we do this iterative update an infinite time, we will eventually get the right Q values, irrespective on the initial Q values, the actions we took in each state etc."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#sarsa-learning",
    "href": "posts/2020-07-09-learning-methods.html#sarsa-learning",
    "title": "Technical ramblings",
    "section": "SARSA learning",
    "text": "SARSA learning\nFirst of all, what a creative naming, it must have taken them a long time to come up with this name. So, why is it called SARSA?, It’s because, the learning algorithm uses the present state, S, the action taken, A, the reward obtained, R, the next state, S, and the action taken, A, in this next state, while following a policy. SARSA stands for State-Action-Reward-State-Action!. While in traditional Q value function, the objective was to estimate the optimum policy doing exploration of a random policy, in SARSA, we start with a policy and tries to estimate the Q value of starting at a state, doing an action in state and following a policy, that is not changed in the course of learning. Thus it is an on-policy learning algorithm, as we don’t change the policy that has been chosen for a particular iteration. The idea is that if we are able to try all the states and all the possible actions infinitely many times, then this will eventually converge to the optimum policy itself. For a particular iteration, it is computationally less demanding that Q learning, but overall, it may need more time to converge. This learning is used when the state transition probabilities might not be fixed, there can be changes in the probabilities of switching from one state to the other. The only change in the algorithm compared to Q learning is, it doesn’t take the action that maximizes the utility when calculating the next state value, but obeys by the policy and takes the stipulated action. Here, too if you don’t understand it, please check the algorithm once on line and it will be clear."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#actor-critic-algorithms",
    "href": "posts/2020-07-09-learning-methods.html#actor-critic-algorithms",
    "title": "Technical ramblings",
    "section": "Actor Critic Algorithms",
    "text": "Actor Critic Algorithms\nThis class of learning algorithms has two parts, the Actor, which is a policy function and the Critic, which is a value function that is used to obtain the value for a state transition following an action. After an action has been selected, the Critic estimates the value for the state and calculates value for the state (immediate reward plus value for the next state). This is used to evaluate the action taken, which is the difference between the calculated value (using Bellman equation) and the estimated value. This difference or delta is used to improve the probability for the action in that state. The improvement factor delta is weighted by a learning rate, beta. A variation of this algorithm called the A3C algorithm is one of the fastest RL algorithms out there."
  },
  {
    "objectID": "posts/2020-07-09-learning-methods.html#the-end",
    "href": "posts/2020-07-09-learning-methods.html#the-end",
    "title": "Technical ramblings",
    "section": "The end",
    "text": "The end\nAwesome, thus in a two step process, we are able to identify an optimum policy based on nothing but a Q value table for each state action pairs. Kind of cool right!\n\n\n\nFun and study well combined."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "Hi!, Welcome to my blog; here, you can read and comment on my ramblings and learnings on tech and not-so-tech topics. Suppose you are curious as to what the image seen here is. In that case, it is an AI-generated one, specifically using stable diffusion when I used the text “dali designing a cyberpunk version of a dream he had about a futuristic microprocessor”."
  }
]